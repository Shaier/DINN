{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_based_paper_sidr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "name": "python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "JoPs1QTPZtrO",
        "outputId": "a4ebc127-261d-4960-c6f4-ced078401265"
      },
      "source": [
        "#Mount my drive- run the code, go to the link, accept.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#Change working directory to make it easier to access the files\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/dinn\")\n",
        "os.getcwd() "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/dinn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFhy95XbZqOS",
        "outputId": "dd7faa46-9017-4ea9-d782-7ee631cf8db0"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "import torch.nn as nn\n",
        "from numpy import genfromtxt\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sidr_data = genfromtxt('sidr.csv', delimiter=',') #in the form of [t,S,I,D,R]\n",
        "\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f35a0d0c390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD6iFgYfZqOa",
        "outputId": "07fdd372-210a-45fe-9d08-a5b53b3baab9"
      },
      "source": [
        "%%time\n",
        "\n",
        "PATH = 'sidr' \n",
        "\n",
        "class DINN(nn.Module):\n",
        "    def __init__(self, t, S_data, I_data, D_data, R_data): #[t,S,I,D,R]\n",
        "        super(DINN, self).__init__()\n",
        "        self.N = 100 #population size\n",
        "        self.t = torch.tensor(t, requires_grad=True)\n",
        "        self.t_float = self.t.float()\n",
        "        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n",
        "        self.S = torch.tensor(S_data)\n",
        "        self.I = torch.tensor(I_data)\n",
        "        self.D = torch.tensor(D_data)\n",
        "        self.R = torch.tensor(R_data)\n",
        "\n",
        "        self.losses = []\n",
        "        self.save = 3 #which file to save to\n",
        "\n",
        "        self.alpha_tilda = torch.tensor(0.191)#torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
        "        self.beta_tilda = torch.tensor(0.05) #torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
        "        self.gamma_tilda = torch.tensor (0.0294) #torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
        "\n",
        "        #matrices (x4 for S,I,D,R) for the gradients\n",
        "        self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1\n",
        "        self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1\n",
        "        self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1\n",
        "        self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1\n",
        "\n",
        "        #NN\n",
        "        self.net_sidr = self.Net_sidr()\n",
        "        self.params = list(self.net_sidr.parameters())\n",
        "        self.params.extend(list([self.alpha_tilda, self.beta_tilda, self.gamma_tilda]))\n",
        "\n",
        "    #force parameters to be in a range\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        return torch.tanh(self.alpha_tilda) #* 0.1 + 0.2\n",
        "\n",
        "    @property\n",
        "    def beta(self):\n",
        "        return torch.tanh(self.beta_tilda) #* 0.01 + 0.05\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "        return torch.tanh(self.gamma_tilda) #* 0.01 + 0.3\n",
        "\n",
        "\n",
        "    #nets\n",
        "    class Net_sidr(nn.Module): # input = [t]\n",
        "        def __init__(self):\n",
        "            super(DINN.Net_sidr, self).__init__()\n",
        "            self.fc1=nn.Linear(1, 32) #takes 100 t's\n",
        "            self.fc2=nn.Linear(32, 32)\n",
        "            self.fc3=nn.Linear(32, 64)\n",
        "            self.fc4=nn.Linear(64, 128)\n",
        "            self.fc5=nn.Linear(128, 128)\n",
        "            self.fc6=nn.Linear(128, 64)\n",
        "            self.fc7=nn.Linear(64, 32)\n",
        "            self.fc8=nn.Linear(32, 32)\n",
        "            self.out=nn.Linear(32, 4) #outputs S, I, D, R\n",
        "\n",
        "\n",
        "        def forward(self, t_batch):\n",
        "            sidr=F.relu(self.fc1(t_batch))\n",
        "            sidr=F.relu(self.fc2(sidr))\n",
        "            sidr=F.relu(self.fc3(sidr))\n",
        "            sidr=F.relu(self.fc4(sidr))\n",
        "            sidr=F.relu(self.fc5(sidr))\n",
        "            sidr=F.relu(self.fc6(sidr))\n",
        "            sidr=F.relu(self.fc7(sidr))\n",
        "            sidr=F.relu(self.fc8(sidr))\n",
        "            sidr=self.out(sidr)\n",
        "            return sidr\n",
        "            \n",
        "\n",
        "    def net_f(self, t_batch):\n",
        "        sidr = self.net_sidr(t_batch)\n",
        "\n",
        "        S,I,D,R = sidr[:,0], sidr[:,1], sidr[:,2], sidr[:,3]\n",
        "\n",
        "        #S_t\n",
        "        sidr.backward(self.m1, retain_graph=True)\n",
        "        S_t = self.t.grad\n",
        "        self.t.grad.zero_()\n",
        "\n",
        "        #I_t\n",
        "        sidr.backward(self.m2, retain_graph=True)\n",
        "        I_t = self.t.grad\n",
        "        self.t.grad.zero_()\n",
        "\n",
        "        #D_t\n",
        "        sidr.backward(self.m3, retain_graph=True)\n",
        "        D_t = self.t.grad\n",
        "        self.t.grad.zero_()\n",
        "\n",
        "        #R_t\n",
        "        sidr.backward(self.m4, retain_graph=True)\n",
        "        R_t = self.t.grad\n",
        "        self.t.grad.zero_()\n",
        "\n",
        "        f1 = S_t + (self.alpha / self.N) * S * I\n",
        "        f2 = I_t - (self.alpha / self.N) * S * I + self.beta * I + self.gamma * I \n",
        "        f3 = D_t - self.gamma * I\n",
        "        f4 = R_t - self.beta * I \n",
        "\n",
        "        return f1, f2, f3, f4, S, I, D, R\n",
        "    \n",
        "    def load(self):\n",
        "      # Load checkpoint\n",
        "      try:\n",
        "        checkpoint = torch.load(PATH + str(self.save)+'.pt') \n",
        "        print('\\nloading pre-trained model...')\n",
        "        self.load_state_dict(checkpoint['model'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "        self.losses = checkpoint['losses']\n",
        "        print('loaded previous loss: ', loss)\n",
        "\n",
        "      except RuntimeError :\n",
        "          print('changed the architecture, ignore')\n",
        "          pass\n",
        "      except FileNotFoundError:\n",
        "          pass\n",
        "\n",
        "    def train(self, n_epochs):\n",
        "      #try loading\n",
        "      self.load()\n",
        "\n",
        "      #train\n",
        "      print('\\nstarting training...\\n')\n",
        "      \n",
        "      for epoch in range(n_epochs):\n",
        "        #lists to hold the output (maintain only the final epoch)\n",
        "        S_pred_list = []\n",
        "        I_pred_list = []\n",
        "        D_pred_list = []\n",
        "        R_pred_list = []\n",
        "\n",
        "        f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        S_pred_list.append(S_pred)\n",
        "        I_pred_list.append(I_pred)\n",
        "        D_pred_list.append(D_pred)\n",
        "        R_pred_list.append(R_pred)\n",
        "\n",
        "        loss = (torch.mean(torch.square(self.S - S_pred))+ \n",
        "                torch.mean(torch.square(self.I - I_pred))+\n",
        "                torch.mean(torch.square(self.D - D_pred))+\n",
        "                torch.mean(torch.square(self.R - R_pred))+\n",
        "                torch.mean(torch.square(f1))+\n",
        "                torch.mean(torch.square(f2))+\n",
        "                torch.mean(torch.square(f3))+\n",
        "                torch.mean(torch.square(f4))\n",
        "                ) \n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step() #scheduler\n",
        "        #self.scheduler.step(loss)\n",
        "\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        #loss + model parameters update\n",
        "        if epoch % 1000 == 0:\n",
        "          #checkpoint save every 1000 epochs if the loss is lower\n",
        "          print('\\nSaving model... Loss is: ', loss)\n",
        "          torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model': self.state_dict(),\n",
        "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "              'scheduler': self.scheduler.state_dict(),\n",
        "              'loss': loss,\n",
        "              'losses': self.losses,\n",
        "              }, PATH + str(self.save)+'.pt')\n",
        "          if self.save % 2 > 0: #its on 3\n",
        "            self.save = 2 #change to 2\n",
        "          else: #its on 2\n",
        "            self.save = 3 #change to 3\n",
        "\n",
        "          print('epoch: ', epoch)\n",
        "          print('alpha: (goal 0.191 ', self.alpha)\n",
        "          print('beta: (goal 0.05 ', self.beta)\n",
        "          print('gamma: (goal 0.0294 ', self.gamma)\n",
        "\n",
        "          # print('#################################')                \n",
        "\n",
        "        \n",
        "      #plot\n",
        "      plt.plot(self.losses, color = 'teal')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      return S_pred_list, I_pred_list, D_pred_list, R_pred_list"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 52 µs, sys: 0 ns, total: 52 µs\n",
            "Wall time: 56.5 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P1obOwWZqOc",
        "outputId": "934f23e6-c3db-421e-8550-2841079ef812"
      },
      "source": [
        "%%time\n",
        "\n",
        "#this worked best\n",
        "dinn = DINN(sidr_data[0], sidr_data[1], sidr_data[2], sidr_data[3], \n",
        "            sidr_data[4]) #in the form of [t,S,I,D,R]\n",
        "\n",
        "learning_rate = 2e-2\n",
        "optimizer = optim.Adam(dinn.params, lr = learning_rate)\n",
        "dinn.optimizer = optimizer\n",
        "\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dinn.optimizer, factor=0.8, patience = 500, verbose=True)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-6, max_lr=5e-4, step_size_up=3000, mode=\"triangular2\", cycle_momentum=False)\n",
        "\n",
        "dinn.scheduler = scheduler\n",
        "\n",
        "S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(100000) #train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loading pre-trained model...\n",
            "loaded previous loss:  tensor(7.9855e+14, dtype=torch.float64, requires_grad=True)\n",
            "\n",
            "starting training...\n",
            "\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9853e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  0\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9855e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  1000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9854e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  2000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9859e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  3000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9849e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  4000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9850e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  5000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9847e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  6000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9846e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  7000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9920e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  8000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9843e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  9000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9922e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  10000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9842e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  11000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9840e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  12000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9841e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  13000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9839e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  14000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9847e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  15000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9836e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  16000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9836e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  17000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9835e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  18000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9833e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  19000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9833e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  20000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9832e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  21000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n",
            "\n",
            "Saving model... Loss is:  tensor(7.9831e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
            "epoch:  22000\n",
            "alpha: (goal 0.191  tensor(0.1887)\n",
            "beta: (goal 0.05  tensor(0.0500)\n",
            "gamma: (goal 0.0294  tensor(0.0294)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwqBVtEM9FYG"
      },
      "source": [
        "plt.plot(dinn.losses[150000:], color = 'teal')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJrvoRWQZqOd"
      },
      "source": [
        "fig = plt.figure(facecolor='w', figsize=(12,12))\n",
        "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
        "\n",
        "ax.plot(sidr_data[0], sidr_data[1], 'black', alpha=0.5, lw=2, label='Susceptible')\n",
        "ax.plot(sidr_data[0], S_pred_list[0].detach().numpy(), 'red', alpha=0.9, lw=2, label='Susceptible Prediction', linestyle='dashed')\n",
        "\n",
        "ax.plot(sidr_data[0], sidr_data[2], 'violet', alpha=0.5, lw=2, label='Infected')\n",
        "ax.plot(sidr_data[0], I_pred_list[0].detach().numpy(), 'dodgerblue', alpha=0.9, lw=2, label='Infected Prediction', linestyle='dashed')\n",
        "\n",
        "ax.plot(sidr_data[0], sidr_data[3], 'darkgreen', alpha=0.5, lw=2, label='Dead')\n",
        "ax.plot(sidr_data[0], D_pred_list[0].detach().numpy(), 'green', alpha=0.9, lw=2, label='Dead Prediction', linestyle='dashed')\n",
        "\n",
        "ax.plot(sidr_data[0], sidr_data[4], 'blue', alpha=0.5, lw=2, label='Recovered')\n",
        "ax.plot(sidr_data[0], R_pred_list[0].detach().numpy(), 'teal', alpha=0.9, lw=2, label='Recovered Prediction', linestyle='dashed')\n",
        "\n",
        "\n",
        "ax.set_xlabel('Time /days')\n",
        "ax.set_ylabel('Number')\n",
        "#ax.set_ylim([-1,50])\n",
        "ax.yaxis.set_tick_params(length=0)\n",
        "ax.xaxis.set_tick_params(length=0)\n",
        "ax.grid(b=True, which='major', c='w', lw=2, ls='-')\n",
        "legend = ax.legend()\n",
        "legend.get_frame().set_alpha(0.5)\n",
        "for spine in ('top', 'right', 'bottom', 'left'):\n",
        "    ax.spines[spine].set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4iB-Ag_OcUx"
      },
      "source": [
        "print(torch.tensor(sidr_data[1]) - S_pred_list[0])\n",
        "print(torch.tensor(sidr_data[2]) - I_pred_list[0])\n",
        "print(torch.tensor(sidr_data[3]) - D_pred_list[0])\n",
        "print(torch.tensor(sidr_data[4]) - R_pred_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUzZI6VMZqOe"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import odeint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initial conditions\n",
        "N = 100\n",
        "\n",
        "S0 = N - 1\n",
        "I0 = 1\n",
        "D0 = 0\n",
        "R0 = 0\n",
        "# A grid of time points (in days)\n",
        "t = np.linspace(0, 500, 100) \n",
        "\n",
        "#parameters\n",
        "alpha = dinn.alpha\n",
        "beta = dinn.beta\n",
        "gamma = dinn.gamma\n",
        "\n",
        "# The SIR model differential equations.\n",
        "def deriv(y, t, alpha, betta, gamma):\n",
        "    S, I, D, R = y\n",
        "    dSdt = - (alpha / N) * S * I\n",
        "    dIdt = (alpha / N) * S * I - beta * I - gamma * I \n",
        "    dDdt = gamma * I\n",
        "    dRdt = beta * I\n",
        "\n",
        "    return dSdt, dIdt, dDdt, dRdt\n",
        "\n",
        "\n",
        "# Initial conditions vector\n",
        "y0 = S0, I0, D0, R0\n",
        "# Integrate the SIR equations over the time grid, t.\n",
        "ret = odeint(deriv, y0, t, args=(alpha, beta, gamma))\n",
        "S, I, D, R = ret.T\n",
        "\n",
        "# Plot the data on two separate curves for S(t), I(t)\n",
        "fig = plt.figure(facecolor='w', figsize=(12,12))\n",
        "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
        "\n",
        "ax.plot(t, S, 'violet', alpha=0.5, lw=2, label='Learnable Param Susceptible', linestyle='dashed')\n",
        "ax.plot(t, sidr_data[1], 'dodgerblue', alpha=0.5, lw=2, label='Susceptible')\n",
        "\n",
        "ax.plot(t, I, 'darkgreen', alpha=0.5, lw=2, label='Learnable Param Infected', linestyle='dashed')\n",
        "ax.plot(t, sidr_data[2], 'gold', alpha=0.5, lw=2, label='Susceptible')\n",
        "\n",
        "ax.plot(t, D, 'red', alpha=0.5, lw=2, label='Learnable Param Dead', linestyle='dashed')\n",
        "ax.plot(t, sidr_data[3], 'salmon', alpha=0.5, lw=2, label='Dead')\n",
        "\n",
        "ax.plot(t, R, 'blue', alpha=0.5, lw=2, label='Learnable Param Recovered', linestyle='dashed')\n",
        "ax.plot(t, sidr_data[4], 'black', alpha=0.5, lw=2, label='Recovered')\n",
        "\n",
        "ax.set_xlabel('Time /days')\n",
        "ax.yaxis.set_tick_params(length=0)\n",
        "ax.xaxis.set_tick_params(length=0)\n",
        "ax.grid(b=True, which='major', c='w', lw=2, ls='-')\n",
        "legend = ax.legend()\n",
        "legend.get_frame().set_alpha(0.5)\n",
        "for spine in ('top', 'right', 'bottom', 'left'):\n",
        "    ax.spines[spine].set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-zofRIm2RNz"
      },
      "source": [
        "#calculate relative MSE loss\n",
        "import math\n",
        "\n",
        "S_total_loss = 0\n",
        "S_den = 0\n",
        "I_total_loss = 0\n",
        "I_den = 0\n",
        "D_total_loss = 0\n",
        "D_den = 0\n",
        "R_total_loss = 0\n",
        "R_den = 0\n",
        "\n",
        "for timestep in range(len(t)):\n",
        "  S_value = sidr_data[1][timestep] - S[timestep]\n",
        "  S_total_loss += S_value**2\n",
        "  S_den += (sidr_data[1][timestep])**2\n",
        "  I_value = sidr_data[2][timestep] - I[timestep]\n",
        "  I_total_loss += I_value**2\n",
        "  I_den += (sidr_data[2][timestep])**2\n",
        "  D_value = sidr_data[3][timestep] - D[timestep]\n",
        "  D_total_loss += D_value**2\n",
        "  D_den += (sidr_data[3][timestep])**2\n",
        "  R_value = sidr_data[4][timestep] - R[timestep]\n",
        "  R_total_loss += R_value**2\n",
        "  R_den += (sidr_data[4][timestep])**2\n",
        "\n",
        "S_total_loss = math.sqrt(S_total_loss/S_den)\n",
        "I_total_loss = math.sqrt(I_total_loss/I_den)\n",
        "D_total_loss = math.sqrt(D_total_loss/D_den)\n",
        "R_total_loss = math.sqrt(R_total_loss/R_den)\n",
        "\n",
        "print('S_total_loss: ', S_total_loss)\n",
        "print('I_total_loss: ', I_total_loss)\n",
        "print('D_total_loss: ', D_total_loss)\n",
        "print('R_total_loss: ', R_total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT-zCNDnVD2W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}