{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ebola.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3.7.3 64-bit ('base': conda)","name":"python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"id":"JoPs1QTPZtrO","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1622070533708,"user_tz":360,"elapsed":152,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"af0cef91-f3a0-450d-b218-90fbb47288f9"},"source":["#Mount my drive- run the code, go to the link, accept.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#Change working directory to make it easier to access the files\n","import os\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/dinn\")\n","os.getcwd() "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/Colab Notebooks/dinn'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"pFhy95XbZqOS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622070534338,"user_tz":360,"elapsed":527,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"0d3866f5-ecfe-47ca-f175-4219577eccf7"},"source":["import torch\n","from torch.autograd import grad\n","import torch.nn as nn\n","from numpy import genfromtxt\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","ebola_data = genfromtxt('ebola.csv', delimiter=',') #in the form of [t, S, E, I, H, F, R]\n","\n","torch.manual_seed(1234)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f3aaecd8bd0>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"AD6iFgYfZqOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622070535575,"user_tz":360,"elapsed":1238,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"d8259f69-5569-40cd-fb26-d565421186e6"},"source":["%%time\n","\n","PATH = 'ebola' \n","\n","class DINN(nn.Module):\n","    def __init__(self, t, S_data, E_data, I_data, H_data, F_data, R_data): \n","        super(DINN, self).__init__()\n","        self.t = torch.tensor(t, requires_grad=True)\n","        self.t_float = self.t.float()\n","        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n","        self.S = torch.tensor(S_data) \n","        self.E = torch.tensor(E_data) \n","        self.I = torch.tensor(I_data) \n","        self.H = torch.tensor(H_data) \n","        self.F = torch.tensor(F_data) \n","        self.R = torch.tensor(R_data)         \n","\n","        self.N = torch.tensor(470000, requires_grad=False)\n","        self.losses = [] #keep the losses\n","        self.save = 3 #which file to save to\n","  \n","        #learnable parameters\n","        self.beta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.beta_h_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.beta_f_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_h_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.theta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_i_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.delta1_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_d_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.delta2_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_f_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_ih_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        self.gamma_dh_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True))\n","        \n","\n","        #matrices (x6 for T,I,V) for the gradients\n","        self.m1 = torch.zeros((len(self.t), 6)); self.m1[:, 0] = 1\n","        self.m2 = torch.zeros((len(self.t), 6)); self.m2[:, 1] = 1\n","        self.m3 = torch.zeros((len(self.t), 6)); self.m3[:, 2] = 1\n","        self.m4 = torch.zeros((len(self.t), 6)); self.m1[:, 3] = 1\n","        self.m5 = torch.zeros((len(self.t), 6)); self.m2[:, 4] = 1\n","        self.m6 = torch.zeros((len(self.t), 6)); self.m3[:, 5] = 1\n","\n","        #values for norm\n","        self.S_max = max(self.S)\n","        self.E_max = max(self.E)\n","        self.I_max = max(self.I)\n","        self.H_max = max(self.H)\n","        self.F_max = max(self.F)\n","        self.R_max = max(self.R)\n","\n","        self.S_min = min(self.S)\n","        self.E_min = min(self.E)\n","        self.I_min = min(self.I)\n","        self.H_min = min(self.H)\n","        self.F_min = min(self.F)\n","        self.R_min = min(self.R)\n","\n","        #normalize \n","        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n","        self.E_hat = (self.E - self.E_min) / (self.E_max - self.E_min)\n","        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n","        self.H_hat = (self.H - self.H_min) / (self.H_max - self.H_min)\n","        self.F_hat = (self.F - self.F_min) / (self.F_max - self.F_min)\n","        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n","\n","        #NN\n","        self.net_ebola = self.Net_ebola()\n","        self.params = list(self.net_ebola.parameters())\n","        self.params.extend(list([self.beta1_tilda, self.beta_h_tilda, self.beta_f_tilda, self.alpha_tilda, self.gamma_h_tilda, self.theta1_tilda, self.gamma_i_tilda, self.delta1_tilda, self.gamma_d_tilda, self.delta2_tilda, self.gamma_f_tilda, self.gamma_ih_tilda, self.gamma_dh_tilda]))\n","\n","        \n","    #force parameters to be in a range\n","    @property\n","    def beta1(self):\n","        return torch.tanh(self.beta1_tilda) + 4\n","\n","    @property\n","    def beta_h(self):\n","        return torch.tanh(self.beta_h_tilda) * 0.02\n","\n","    @property\n","    def beta_f(self):\n","        return torch.tanh(self.beta_f_tilda) * 0.2 + 0.5\n","\n","    @property\n","    def alpha(self):\n","        return torch.tanh(self.alpha_tilda) * 0.1\n","\n","    @property\n","    def gamma_h(self):\n","        return torch.tanh(self.gamma_h_tilda) \n","\n","    @property\n","    def theta1(self):\n","        return torch.tanh(self.theta1_tilda) \n","\n","    @property\n","    def gamma_i(self):\n","        return torch.tanh(self.gamma_i_tilda)\n","\n","    @property\n","    def delta1(self):\n","        return torch.tanh(self.delta1_tilda) \n","\n","    @property\n","    def gamma_d(self):\n","        return torch.tanh(self.gamma_d_tilda) \n","\n","    @property\n","    def delta2(self):\n","        return torch.tanh(self.delta2_tilda) * 0.5 + 0.6\n","\n","    @property\n","    def gamma_f(self):\n","        return torch.tanh(self.gamma_f_tilda) * 0.5 + 0.6\n","\n","    @property\n","    def gamma_ih(self):\n","\n","        return torch.tanh(self.gamma_ih_tilda) * 0.1 \n","    @property\n","    def gamma_dh(self):\n","        return torch.tanh(self.gamma_dh_tilda) * 0.1\n","    \n","    #nets\n","    class Net_ebola(nn.Module): # input = [t]\n","        def __init__(self):\n","            super(DINN.Net_ebola, self).__init__()\n","            self.fc1=nn.Linear(1, 20) #takes t's\n","            self.fc2=nn.Linear(20, 20)\n","            self.fc3=nn.Linear(20, 20)\n","            self.fc4=nn.Linear(20, 20)\n","            self.fc5=nn.Linear(20, 20)\n","            self.fc6=nn.Linear(20, 20)\n","            self.fc7=nn.Linear(20, 20)\n","            self.fc8=nn.Linear(20, 20)\n","            self.out=nn.Linear(20, 6) #outputs S, E, I, H, F, R\n","\n","        def forward(self, t):\n","            ebola=F.relu(self.fc1(t))\n","            ebola=F.relu(self.fc2(ebola))\n","            ebola=F.relu(self.fc3(ebola))\n","            ebola=F.relu(self.fc4(ebola))\n","            ebola=F.relu(self.fc5(ebola))\n","            ebola=F.relu(self.fc6(ebola))\n","            ebola=F.relu(self.fc7(ebola))\n","            ebola=F.relu(self.fc8(ebola))\n","            ebola=self.out(ebola)\n","            return ebola    \n","\n","    def net_f(self, t_batch):      \n","\n","        ebola_hat = self.net_ebola(t_batch)\n","\n","        S_hat, E_hat, I_hat, H_hat, F_hat, R_hat = ebola_hat[:,0], ebola_hat[:,1], ebola_hat[:,2], ebola_hat[:,3], ebola_hat[:,4], ebola_hat[:,5]\n","\n","        #S_t\n","        ebola_hat.backward(self.m1, retain_graph=True)\n","        S_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #E_t\n","        ebola_hat.backward(self.m2, retain_graph=True)\n","        E_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #I_t\n","        ebola_hat.backward(self.m3, retain_graph=True)\n","        I_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #H_t\n","        ebola_hat.backward(self.m4, retain_graph=True)\n","        H_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #F_t\n","        ebola_hat.backward(self.m5, retain_graph=True)\n","        F_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","        \n","        #R_t\n","        ebola_hat.backward(self.m6, retain_graph=True)\n","        R_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","        \n","        #unnormalize\n","        S = self.S_min + (self.S_max - self.S_min) * S_hat\n","        E = self.E_min + (self.E_max - self.E_min) * E_hat\n","        I = self.I_min + (self.I_max - self.I_min) * I_hat\n","        H = self.H_min + (self.H_max - self.H_min) * H_hat\n","        F = self.F_min + (self.F_max - self.F_min) * F_hat\n","        R = self.R_min + (self.R_max - self.R_min) * R_hat\n","\n","        f1_hat = S_hat_t - (-1/self.N * (self.beta1 * S * I + self.beta_h * S * H + self.beta_f * S * F)) / (self.S_max - self.S_min) \n","        f2_hat = E_hat_t - (1/self.N * (self.beta1 * S * I + self.beta_h * S * H + self.beta_f * S * F) - self.alpha * E) / (self.E_max - self.E_min) \n","        f3_hat = I_hat_t - (self.alpha * E - (self.gamma_h * self.theta1 + self.gamma_i * (1-self.theta1)*(1-self.delta1) + self.gamma_d * (1-self.theta1) * self.delta1) * I) / (self.I_max - self.I_min) \n","        f4_hat = H_hat_t - (self.gamma_h * self.theta1 * I - (self.gamma_dh * self.delta2 + self.gamma_ih * (1-self.delta2)) * H) / (self.H_max - self.H_min)  \n","        f5_hat = F_hat_t - (self.gamma_d * (1-self.theta1) * self.delta1 * I + self.gamma_dh * self.delta2 * H - self.gamma_f * F) / (self.F_max - self.F_min) \n","        f6_hat = R_hat_t - (self.gamma_i * (1-self.theta1) * (1-self.delta1) * I + self.gamma_ih * (1-self.delta2) * H + self.gamma_f * F) / (self.R_max - self.R_min) \n","\n","        return f1_hat, f2_hat, f3_hat, f4_hat, f5_hat, f6_hat, S_hat, E_hat, I_hat, H_hat, F_hat, R_hat\n","    \n","    def load(self):\n","      # Load checkpoint\n","      try:\n","        checkpoint = torch.load(PATH + str(self.save)+'.pt') \n","        print('\\nloading pre-trained model...')\n","        self.load_state_dict(checkpoint['model'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler'])\n","        epoch = checkpoint['epoch']\n","        loss = checkpoint['loss']\n","        self.losses = checkpoint['losses']\n","        print('loaded previous loss: ', loss)\n","      except RuntimeError :\n","          print('changed the architecture, ignore')\n","          pass\n","      except FileNotFoundError:\n","          pass\n","\n","    def train(self, n_epochs):\n","      #try loading\n","      self.load()\n","\n","      #train\n","      print('\\nstarting training...\\n')\n","      \n","      for epoch in range(n_epochs):\n","        #lists to hold the output (maintain only the final epoch)\n","        S_pred_list= []\n","        E_pred_list= []\n","        I_pred_list= []\n","        H_pred_list= []\n","        F_pred_list= []\n","        R_pred_list= []\n","        \n","        f1, f2, f3, f4, f5, f6, S_pred, E_pred, I_pred, H_pred, F_pred, R_pred = self.net_f(self.t_batch)\n","        self.optimizer.zero_grad()\n","\n","        S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred)\n","        E_pred_list.append(self.E_min + (self.E_max - self.E_min) * E_pred)\n","        I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n","        H_pred_list.append(self.H_min + (self.H_max - self.H_min) * H_pred)\n","        F_pred_list.append(self.F_min + (self.F_max - self.F_min) * F_pred)\n","        R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n","\n","        loss = (\n","              torch.mean(torch.square(self.S_hat - S_pred)) + torch.mean(torch.square(self.I_hat - I_pred)) + torch.mean(torch.square(self.E_hat - E_pred)) + \n","              torch.mean(torch.square(self.H_hat - H_pred)) + torch.mean(torch.square(self.F_hat - F_pred)) + torch.mean(torch.square(self.R_hat - R_pred)) + \n","              torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)) + torch.mean(torch.square(f3)) +\n","              torch.mean(torch.square(f4)) + torch.mean(torch.square(f5)) + torch.mean(torch.square(f6)) \n","               )\n","        loss.backward()\n","\n","        self.optimizer.step()\n","        #self.scheduler.step() \n","        self.scheduler.step(loss) \n","\n","        self.losses.append(loss.item())\n","\n","        if epoch % 1000 == 0:          \n","          print('\\nEpoch ', epoch)\n","\n","        #loss + model parameters update\n","        if epoch % 40000 == 0:\n","          #checkpoint save every 1000 epochs if the loss is lower\n","          print('\\nSaving model... Loss is: ', loss)\n","          torch.save({\n","              'epoch': epoch,\n","              'model': self.state_dict(),\n","              'optimizer_state_dict': self.optimizer.state_dict(),\n","              'scheduler': self.scheduler.state_dict(),\n","              'loss': loss,\n","              'losses': self.losses,\n","              }, PATH + str(self.save)+'.pt')\n","          if self.save % 2 > 0: #its on 3\n","            self.save = 2 #change to 2\n","          else: #its on 2\n","            self.save = 3 #change to 3\n","\n","          print('epoch: ', epoch)\n","          print('beta1: (goal 3.532)', self.beta1)\n","          print('\\nbeta_h: (goal 0.012)', self.beta_h)\n","          print('\\nbeta_f: (goal 0.462): ', self.beta_f)\n","          print('\\ntheta1: (goal 0.65): ', self.theta1)\n","          print('\\ndelta2 (goal 0.42): ', self.delta2)\n","          print('\\ngamma_f (goal 0.5): ', self.gamma_f)\n","          print('#################################')\n","        \n","      #plot\n","      plt.plot(self.losses, color = 'teal')\n","      plt.xlabel('Epochs')\n","      plt.ylabel('Loss')\n","      return S_pred_list, E_pred_list, I_pred_list, H_pred_list, F_pred_list, R_pred_list\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CPU times: user 49 µs, sys: 8 µs, total: 57 µs\n","Wall time: 61.5 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_P1obOwWZqOc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5088f2e0-d702-47c0-8af8-07e326a363cc"},"source":["%%time\n","\n","#this worked best\n","dinn = DINN(ebola_data[0], ebola_data[1], ebola_data[2], ebola_data[3], ebola_data[4], ebola_data[5], ebola_data[6]) #t,S, E, I, H, F, R\n","\n","learning_rate = 1e-5\n","optimizer = optim.Adam(dinn.params, lr = learning_rate)\n","dinn.optimizer = optimizer\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dinn.optimizer, factor=0.99, patience = 5000, verbose=True)\n","#scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-7, max_lr=1e-5, step_size_up=20000, mode=\"triangular2\", cycle_momentum=False)\n","\n","dinn.scheduler = scheduler\n","\n","try: \n","  S_pred_list, E_pred_list, I_pred_list, H_pred_list, F_pred_list, R_pred_list = dinn.train(8000000) #train\n","except EOFError:\n","  if dinn.save == 2:\n","    dinn.save = 3\n","    S_pred_list, E_pred_list, I_pred_list, H_pred_list, F_pred_list, R_pred_list = dinn.train(8000000) #train\n","  elif dinn.save == 3:\n","    dinn.save = 2\n","    S_pred_list, E_pred_list, I_pred_list, H_pred_list, F_pred_list, R_pred_list = dinn.train(8000000) #train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch  186000\n","\n","Epoch  187000\n","\n","Epoch  188000\n","\n","Epoch  189000\n","Epoch 189287: reducing learning rate of group 0 to 9.2274e-06.\n","\n","Epoch  190000\n","\n","Epoch  191000\n","\n","Epoch  192000\n","\n","Epoch  193000\n","\n","Epoch  194000\n","Epoch 194288: reducing learning rate of group 0 to 9.1352e-06.\n","\n","Epoch  195000\n","\n","Epoch  196000\n","\n","Epoch  197000\n","\n","Epoch  198000\n","\n","Epoch  199000\n","Epoch 199289: reducing learning rate of group 0 to 9.0438e-06.\n","\n","Epoch  200000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  200000\n","beta1: (goal 3.532) tensor([3.4666], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0180], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6676], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6882], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3886], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.2553], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  201000\n","\n","Epoch  202000\n","\n","Epoch  203000\n","\n","Epoch  204000\n","Epoch 204290: reducing learning rate of group 0 to 8.9534e-06.\n","\n","Epoch  205000\n","\n","Epoch  206000\n","\n","Epoch  207000\n","\n","Epoch  208000\n","\n","Epoch  209000\n","Epoch 209291: reducing learning rate of group 0 to 8.8638e-06.\n","\n","Epoch  210000\n","\n","Epoch  211000\n","\n","Epoch  212000\n","\n","Epoch  213000\n","\n","Epoch  214000\n","Epoch 214292: reducing learning rate of group 0 to 8.7752e-06.\n","\n","Epoch  215000\n","\n","Epoch  216000\n","\n","Epoch  217000\n","\n","Epoch  218000\n","\n","Epoch  219000\n","Epoch 219293: reducing learning rate of group 0 to 8.6875e-06.\n","\n","Epoch  220000\n","\n","Epoch  221000\n","\n","Epoch  222000\n","\n","Epoch  223000\n","\n","Epoch  224000\n","Epoch 224294: reducing learning rate of group 0 to 8.6006e-06.\n","\n","Epoch  225000\n","\n","Epoch  226000\n","\n","Epoch  227000\n","\n","Epoch  228000\n","\n","Epoch  229000\n","Epoch 229295: reducing learning rate of group 0 to 8.5146e-06.\n","\n","Epoch  230000\n","\n","Epoch  231000\n","\n","Epoch  232000\n","\n","Epoch  233000\n","\n","Epoch  234000\n","Epoch 234296: reducing learning rate of group 0 to 8.4294e-06.\n","\n","Epoch  235000\n","\n","Epoch  236000\n","\n","Epoch  237000\n","\n","Epoch  238000\n","\n","Epoch  239000\n","Epoch 239297: reducing learning rate of group 0 to 8.3451e-06.\n","\n","Epoch  240000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  240000\n","beta1: (goal 3.532) tensor([3.4487], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0188], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6805], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6883], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3483], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1875], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  241000\n","\n","Epoch  242000\n","\n","Epoch  243000\n","\n","Epoch  244000\n","Epoch 244298: reducing learning rate of group 0 to 8.2617e-06.\n","\n","Epoch  245000\n","\n","Epoch  246000\n","\n","Epoch  247000\n","\n","Epoch  248000\n","\n","Epoch  249000\n","Epoch 249299: reducing learning rate of group 0 to 8.1791e-06.\n","\n","Epoch  250000\n","\n","Epoch  251000\n","\n","Epoch  252000\n","\n","Epoch  253000\n","\n","Epoch  254000\n","Epoch 254300: reducing learning rate of group 0 to 8.0973e-06.\n","\n","Epoch  255000\n","\n","Epoch  256000\n","\n","Epoch  257000\n","\n","Epoch  258000\n","\n","Epoch  259000\n","Epoch 259301: reducing learning rate of group 0 to 8.0163e-06.\n","\n","Epoch  260000\n","\n","Epoch  261000\n","\n","Epoch  262000\n","\n","Epoch  263000\n","\n","Epoch  264000\n","Epoch 264302: reducing learning rate of group 0 to 7.9361e-06.\n","\n","Epoch  265000\n","\n","Epoch  266000\n","\n","Epoch  267000\n","\n","Epoch  268000\n","\n","Epoch  269000\n","Epoch 269303: reducing learning rate of group 0 to 7.8568e-06.\n","\n","Epoch  270000\n","\n","Epoch  271000\n","\n","Epoch  272000\n","\n","Epoch  273000\n","\n","Epoch  274000\n","Epoch 274304: reducing learning rate of group 0 to 7.7782e-06.\n","\n","Epoch  275000\n","\n","Epoch  276000\n","\n","Epoch  277000\n","\n","Epoch  278000\n","\n","Epoch  279000\n","Epoch 279305: reducing learning rate of group 0 to 7.7004e-06.\n","\n","Epoch  280000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  280000\n","beta1: (goal 3.532) tensor([3.4431], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0192], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6873], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6880], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3254], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1495], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  281000\n","\n","Epoch  282000\n","\n","Epoch  283000\n","\n","Epoch  284000\n","Epoch 284306: reducing learning rate of group 0 to 7.6234e-06.\n","\n","Epoch  285000\n","\n","Epoch  286000\n","\n","Epoch  287000\n","\n","Epoch  288000\n","\n","Epoch  289000\n","Epoch 289307: reducing learning rate of group 0 to 7.5472e-06.\n","\n","Epoch  290000\n","\n","Epoch  291000\n","\n","Epoch  292000\n","\n","Epoch  293000\n","\n","Epoch  294000\n","Epoch 294308: reducing learning rate of group 0 to 7.4717e-06.\n","\n","Epoch  295000\n","\n","Epoch  296000\n","\n","Epoch  297000\n","\n","Epoch  298000\n","\n","Epoch  299000\n","Epoch 299309: reducing learning rate of group 0 to 7.3970e-06.\n","\n","Epoch  300000\n","\n","Epoch  301000\n","\n","Epoch  302000\n","\n","Epoch  303000\n","\n","Epoch  304000\n","Epoch 304310: reducing learning rate of group 0 to 7.3230e-06.\n","\n","Epoch  305000\n","\n","Epoch  306000\n","\n","Epoch  307000\n","\n","Epoch  308000\n","\n","Epoch  309000\n","Epoch 309311: reducing learning rate of group 0 to 7.2498e-06.\n","\n","Epoch  310000\n","\n","Epoch  311000\n","\n","Epoch  312000\n","\n","Epoch  313000\n","\n","Epoch  314000\n","Epoch 314312: reducing learning rate of group 0 to 7.1773e-06.\n","\n","Epoch  315000\n","\n","Epoch  316000\n","\n","Epoch  317000\n","\n","Epoch  318000\n","\n","Epoch  319000\n","Epoch 319313: reducing learning rate of group 0 to 7.1055e-06.\n","\n","Epoch  320000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  320000\n","beta1: (goal 3.532) tensor([3.4388], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0194], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6911], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6880], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3129], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1288], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  321000\n","\n","Epoch  322000\n","\n","Epoch  323000\n","\n","Epoch  324000\n","Epoch 324314: reducing learning rate of group 0 to 7.0345e-06.\n","\n","Epoch  325000\n","\n","Epoch  326000\n","\n","Epoch  327000\n","\n","Epoch  328000\n","\n","Epoch  329000\n","Epoch 329315: reducing learning rate of group 0 to 6.9641e-06.\n","\n","Epoch  330000\n","\n","Epoch  331000\n","\n","Epoch  332000\n","\n","Epoch  333000\n","\n","Epoch  334000\n","Epoch 334316: reducing learning rate of group 0 to 6.8945e-06.\n","\n","Epoch  335000\n","\n","Epoch  336000\n","\n","Epoch  337000\n","\n","Epoch  338000\n","\n","Epoch  339000\n","Epoch 339317: reducing learning rate of group 0 to 6.8255e-06.\n","\n","Epoch  340000\n","\n","Epoch  341000\n","\n","Epoch  342000\n","\n","Epoch  343000\n","\n","Epoch  344000\n","Epoch 344318: reducing learning rate of group 0 to 6.7573e-06.\n","\n","Epoch  345000\n","\n","Epoch  346000\n","\n","Epoch  347000\n","\n","Epoch  348000\n","\n","Epoch  349000\n","Epoch 349319: reducing learning rate of group 0 to 6.6897e-06.\n","\n","Epoch  350000\n","\n","Epoch  351000\n","\n","Epoch  352000\n","\n","Epoch  353000\n","\n","Epoch  354000\n","Epoch 354320: reducing learning rate of group 0 to 6.6228e-06.\n","\n","Epoch  355000\n","\n","Epoch  356000\n","\n","Epoch  357000\n","\n","Epoch  358000\n","\n","Epoch  359000\n","Epoch 359321: reducing learning rate of group 0 to 6.5566e-06.\n","\n","Epoch  360000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  360000\n","beta1: (goal 3.532) tensor([3.4354], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0196], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6933], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6880], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3059], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1174], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  361000\n","\n","Epoch  362000\n","\n","Epoch  363000\n","\n","Epoch  364000\n","Epoch 364322: reducing learning rate of group 0 to 6.4910e-06.\n","\n","Epoch  365000\n","\n","Epoch  366000\n","\n","Epoch  367000\n","\n","Epoch  368000\n","\n","Epoch  369000\n","Epoch 369323: reducing learning rate of group 0 to 6.4261e-06.\n","\n","Epoch  370000\n","\n","Epoch  371000\n","\n","Epoch  372000\n","\n","Epoch  373000\n","\n","Epoch  374000\n","Epoch 374324: reducing learning rate of group 0 to 6.3619e-06.\n","\n","Epoch  375000\n","\n","Epoch  376000\n","\n","Epoch  377000\n","\n","Epoch  378000\n","\n","Epoch  379000\n","Epoch 379325: reducing learning rate of group 0 to 6.2982e-06.\n","\n","Epoch  380000\n","\n","Epoch  381000\n","\n","Epoch  382000\n","\n","Epoch  383000\n","\n","Epoch  384000\n","Epoch 384326: reducing learning rate of group 0 to 6.2353e-06.\n","\n","Epoch  385000\n","\n","Epoch  386000\n","\n","Epoch  387000\n","\n","Epoch  388000\n","\n","Epoch  389000\n","Epoch 389327: reducing learning rate of group 0 to 6.1729e-06.\n","\n","Epoch  390000\n","\n","Epoch  391000\n","\n","Epoch  392000\n","\n","Epoch  393000\n","\n","Epoch  394000\n","Epoch 394328: reducing learning rate of group 0 to 6.1112e-06.\n","\n","Epoch  395000\n","\n","Epoch  396000\n","\n","Epoch  397000\n","\n","Epoch  398000\n","\n","Epoch  399000\n","Epoch 399329: reducing learning rate of group 0 to 6.0501e-06.\n","\n","Epoch  400000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  400000\n","beta1: (goal 3.532) tensor([3.4326], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0196], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6947], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6881], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.3019], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1109], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  401000\n","\n","Epoch  402000\n","\n","Epoch  403000\n","\n","Epoch  404000\n","Epoch 404330: reducing learning rate of group 0 to 5.9896e-06.\n","\n","Epoch  405000\n","\n","Epoch  406000\n","\n","Epoch  407000\n","\n","Epoch  408000\n","\n","Epoch  409000\n","Epoch 409331: reducing learning rate of group 0 to 5.9297e-06.\n","\n","Epoch  410000\n","\n","Epoch  411000\n","\n","Epoch  412000\n","\n","Epoch  413000\n","\n","Epoch  414000\n","Epoch 414332: reducing learning rate of group 0 to 5.8704e-06.\n","\n","Epoch  415000\n","\n","Epoch  416000\n","\n","Epoch  417000\n","\n","Epoch  418000\n","\n","Epoch  419000\n","Epoch 419333: reducing learning rate of group 0 to 5.8117e-06.\n","\n","Epoch  420000\n","\n","Epoch  421000\n","\n","Epoch  422000\n","\n","Epoch  423000\n","\n","Epoch  424000\n","Epoch 424334: reducing learning rate of group 0 to 5.7535e-06.\n","\n","Epoch  425000\n","\n","Epoch  426000\n","\n","Epoch  427000\n","\n","Epoch  428000\n","\n","Epoch  429000\n","Epoch 429335: reducing learning rate of group 0 to 5.6960e-06.\n","\n","Epoch  430000\n","\n","Epoch  431000\n","\n","Epoch  432000\n","\n","Epoch  433000\n","\n","Epoch  434000\n","Epoch 434336: reducing learning rate of group 0 to 5.6391e-06.\n","\n","Epoch  435000\n","\n","Epoch  436000\n","\n","Epoch  437000\n","\n","Epoch  438000\n","\n","Epoch  439000\n","Epoch 439337: reducing learning rate of group 0 to 5.5827e-06.\n","\n","Epoch  440000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  440000\n","beta1: (goal 3.532) tensor([3.4593], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0197], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6956], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6873], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2996], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1072], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  441000\n","\n","Epoch  442000\n","\n","Epoch  443000\n","\n","Epoch  444000\n","Epoch 444338: reducing learning rate of group 0 to 5.5268e-06.\n","\n","Epoch  445000\n","\n","Epoch  446000\n","\n","Epoch  447000\n","\n","Epoch  448000\n","\n","Epoch  449000\n","Epoch 449339: reducing learning rate of group 0 to 5.4716e-06.\n","\n","Epoch  450000\n","\n","Epoch  451000\n","\n","Epoch  452000\n","\n","Epoch  453000\n","\n","Epoch  454000\n","Epoch 454340: reducing learning rate of group 0 to 5.4169e-06.\n","\n","Epoch  455000\n","\n","Epoch  456000\n","\n","Epoch  457000\n","\n","Epoch  458000\n","\n","Epoch  459000\n","Epoch 459341: reducing learning rate of group 0 to 5.3627e-06.\n","\n","Epoch  460000\n","\n","Epoch  461000\n","\n","Epoch  462000\n","\n","Epoch  463000\n","\n","Epoch  464000\n","Epoch 464342: reducing learning rate of group 0 to 5.3091e-06.\n","\n","Epoch  465000\n","\n","Epoch  466000\n","\n","Epoch  467000\n","\n","Epoch  468000\n","\n","Epoch  469000\n","Epoch 469343: reducing learning rate of group 0 to 5.2560e-06.\n","\n","Epoch  470000\n","\n","Epoch  471000\n","\n","Epoch  472000\n","\n","Epoch  473000\n","\n","Epoch  474000\n","Epoch 474344: reducing learning rate of group 0 to 5.2034e-06.\n","\n","Epoch  475000\n","\n","Epoch  476000\n","\n","Epoch  477000\n","\n","Epoch  478000\n","\n","Epoch  479000\n","Epoch 479345: reducing learning rate of group 0 to 5.1514e-06.\n","\n","Epoch  480000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  480000\n","beta1: (goal 3.532) tensor([3.4635], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0197], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6961], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6871], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2982], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1049], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  481000\n","\n","Epoch  482000\n","\n","Epoch  483000\n","\n","Epoch  484000\n","Epoch 484346: reducing learning rate of group 0 to 5.0999e-06.\n","\n","Epoch  485000\n","\n","Epoch  486000\n","\n","Epoch  487000\n","\n","Epoch  488000\n","\n","Epoch  489000\n","Epoch 489347: reducing learning rate of group 0 to 5.0489e-06.\n","\n","Epoch  490000\n","\n","Epoch  491000\n","\n","Epoch  492000\n","\n","Epoch  493000\n","\n","Epoch  494000\n","Epoch 494348: reducing learning rate of group 0 to 4.9984e-06.\n","\n","Epoch  495000\n","\n","Epoch  496000\n","\n","Epoch  497000\n","\n","Epoch  498000\n","\n","Epoch  499000\n","Epoch 499349: reducing learning rate of group 0 to 4.9484e-06.\n","\n","Epoch  500000\n","\n","Epoch  501000\n","\n","Epoch  502000\n","\n","Epoch  503000\n","\n","Epoch  504000\n","Epoch 504350: reducing learning rate of group 0 to 4.8989e-06.\n","\n","Epoch  505000\n","\n","Epoch  506000\n","\n","Epoch  507000\n","\n","Epoch  508000\n","\n","Epoch  509000\n","Epoch 509351: reducing learning rate of group 0 to 4.8499e-06.\n","\n","Epoch  510000\n","\n","Epoch  511000\n","\n","Epoch  512000\n","\n","Epoch  513000\n","\n","Epoch  514000\n","Epoch 514352: reducing learning rate of group 0 to 4.8014e-06.\n","\n","Epoch  515000\n","\n","Epoch  516000\n","\n","Epoch  517000\n","\n","Epoch  518000\n","\n","Epoch  519000\n","Epoch 519353: reducing learning rate of group 0 to 4.7534e-06.\n","\n","Epoch  520000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  520000\n","beta1: (goal 3.532) tensor([3.4608], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6965], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6870], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2973], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1035], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  521000\n","\n","Epoch  522000\n","\n","Epoch  523000\n","\n","Epoch  524000\n","Epoch 524354: reducing learning rate of group 0 to 4.7059e-06.\n","\n","Epoch  525000\n","\n","Epoch  526000\n","\n","Epoch  527000\n","\n","Epoch  528000\n","\n","Epoch  529000\n","Epoch 529355: reducing learning rate of group 0 to 4.6588e-06.\n","\n","Epoch  530000\n","\n","Epoch  531000\n","\n","Epoch  532000\n","\n","Epoch  533000\n","\n","Epoch  534000\n","Epoch 534356: reducing learning rate of group 0 to 4.6122e-06.\n","\n","Epoch  535000\n","\n","Epoch  536000\n","\n","Epoch  537000\n","\n","Epoch  538000\n","\n","Epoch  539000\n","Epoch 539357: reducing learning rate of group 0 to 4.5661e-06.\n","\n","Epoch  540000\n","\n","Epoch  541000\n","\n","Epoch  542000\n","\n","Epoch  543000\n","\n","Epoch  544000\n","Epoch 544358: reducing learning rate of group 0 to 4.5204e-06.\n","\n","Epoch  545000\n","\n","Epoch  546000\n","\n","Epoch  547000\n","\n","Epoch  548000\n","\n","Epoch  549000\n","Epoch 549359: reducing learning rate of group 0 to 4.4752e-06.\n","\n","Epoch  550000\n","\n","Epoch  551000\n","\n","Epoch  552000\n","\n","Epoch  553000\n","\n","Epoch  554000\n","Epoch 554360: reducing learning rate of group 0 to 4.4305e-06.\n","\n","Epoch  555000\n","\n","Epoch  556000\n","\n","Epoch  557000\n","\n","Epoch  558000\n","\n","Epoch  559000\n","Epoch 559361: reducing learning rate of group 0 to 4.3862e-06.\n","\n","Epoch  560000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  560000\n","beta1: (goal 3.532) tensor([3.4559], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6968], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6872], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2968], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1026], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  561000\n","\n","Epoch  562000\n","\n","Epoch  563000\n","\n","Epoch  564000\n","Epoch 564362: reducing learning rate of group 0 to 4.3423e-06.\n","\n","Epoch  565000\n","\n","Epoch  566000\n","\n","Epoch  567000\n","\n","Epoch  568000\n","\n","Epoch  569000\n","Epoch 569363: reducing learning rate of group 0 to 4.2989e-06.\n","\n","Epoch  570000\n","\n","Epoch  571000\n","\n","Epoch  572000\n","\n","Epoch  573000\n","\n","Epoch  574000\n","Epoch 574364: reducing learning rate of group 0 to 4.2559e-06.\n","\n","Epoch  575000\n","\n","Epoch  576000\n","\n","Epoch  577000\n","\n","Epoch  578000\n","\n","Epoch  579000\n","Epoch 579365: reducing learning rate of group 0 to 4.2133e-06.\n","\n","Epoch  580000\n","\n","Epoch  581000\n","\n","Epoch  582000\n","\n","Epoch  583000\n","\n","Epoch  584000\n","Epoch 584366: reducing learning rate of group 0 to 4.1712e-06.\n","\n","Epoch  585000\n","\n","Epoch  586000\n","\n","Epoch  587000\n","\n","Epoch  588000\n","\n","Epoch  589000\n","Epoch 589367: reducing learning rate of group 0 to 4.1295e-06.\n","\n","Epoch  590000\n","\n","Epoch  591000\n","\n","Epoch  592000\n","\n","Epoch  593000\n","\n","Epoch  594000\n","Epoch 594368: reducing learning rate of group 0 to 4.0882e-06.\n","\n","Epoch  595000\n","\n","Epoch  596000\n","\n","Epoch  597000\n","\n","Epoch  598000\n","\n","Epoch  599000\n","Epoch 599369: reducing learning rate of group 0 to 4.0473e-06.\n","\n","Epoch  600000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  600000\n","beta1: (goal 3.532) tensor([3.4534], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6971], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6873], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2964], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1020], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  601000\n","\n","Epoch  602000\n","\n","Epoch  603000\n","\n","Epoch  604000\n","Epoch 604370: reducing learning rate of group 0 to 4.0068e-06.\n","\n","Epoch  605000\n","\n","Epoch  606000\n","\n","Epoch  607000\n","\n","Epoch  608000\n","\n","Epoch  609000\n","Epoch 609371: reducing learning rate of group 0 to 3.9668e-06.\n","\n","Epoch  610000\n","\n","Epoch  611000\n","\n","Epoch  612000\n","\n","Epoch  613000\n","\n","Epoch  614000\n","Epoch 614372: reducing learning rate of group 0 to 3.9271e-06.\n","\n","Epoch  615000\n","\n","Epoch  616000\n","\n","Epoch  617000\n","\n","Epoch  618000\n","\n","Epoch  619000\n","Epoch 619373: reducing learning rate of group 0 to 3.8878e-06.\n","\n","Epoch  620000\n","\n","Epoch  621000\n","\n","Epoch  622000\n","\n","Epoch  623000\n","\n","Epoch  624000\n","Epoch 624374: reducing learning rate of group 0 to 3.8490e-06.\n","\n","Epoch  625000\n","\n","Epoch  626000\n","\n","Epoch  627000\n","\n","Epoch  628000\n","\n","Epoch  629000\n","Epoch 629375: reducing learning rate of group 0 to 3.8105e-06.\n","\n","Epoch  630000\n","\n","Epoch  631000\n","\n","Epoch  632000\n","\n","Epoch  633000\n","\n","Epoch  634000\n","Epoch 634376: reducing learning rate of group 0 to 3.7724e-06.\n","\n","Epoch  635000\n","\n","Epoch  636000\n","\n","Epoch  637000\n","\n","Epoch  638000\n","\n","Epoch  639000\n","Epoch 639377: reducing learning rate of group 0 to 3.7346e-06.\n","\n","Epoch  640000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  640000\n","beta1: (goal 3.532) tensor([3.4515], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6973], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6875], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2962], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1016], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  641000\n","\n","Epoch  642000\n","\n","Epoch  643000\n","\n","Epoch  644000\n","Epoch 644378: reducing learning rate of group 0 to 3.6973e-06.\n","\n","Epoch  645000\n","\n","Epoch  646000\n","\n","Epoch  647000\n","\n","Epoch  648000\n","\n","Epoch  649000\n","Epoch 649379: reducing learning rate of group 0 to 3.6603e-06.\n","\n","Epoch  650000\n","\n","Epoch  651000\n","\n","Epoch  652000\n","\n","Epoch  653000\n","\n","Epoch  654000\n","Epoch 654380: reducing learning rate of group 0 to 3.6237e-06.\n","\n","Epoch  655000\n","\n","Epoch  656000\n","\n","Epoch  657000\n","\n","Epoch  658000\n","\n","Epoch  659000\n","Epoch 659381: reducing learning rate of group 0 to 3.5875e-06.\n","\n","Epoch  660000\n","\n","Epoch  661000\n","\n","Epoch  662000\n","\n","Epoch  663000\n","\n","Epoch  664000\n","Epoch 664382: reducing learning rate of group 0 to 3.5516e-06.\n","\n","Epoch  665000\n","\n","Epoch  666000\n","\n","Epoch  667000\n","\n","Epoch  668000\n","\n","Epoch  669000\n","Epoch 669383: reducing learning rate of group 0 to 3.5161e-06.\n","\n","Epoch  670000\n","\n","Epoch  671000\n","\n","Epoch  672000\n","\n","Epoch  673000\n","\n","Epoch  674000\n","Epoch 674384: reducing learning rate of group 0 to 3.4809e-06.\n","\n","Epoch  675000\n","\n","Epoch  676000\n","\n","Epoch  677000\n","\n","Epoch  678000\n","\n","Epoch  679000\n","Epoch 679385: reducing learning rate of group 0 to 3.4461e-06.\n","\n","Epoch  680000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  680000\n","beta1: (goal 3.532) tensor([3.4506], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6974], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6876], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2960], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1013], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  681000\n","\n","Epoch  682000\n","\n","Epoch  683000\n","\n","Epoch  684000\n","Epoch 684386: reducing learning rate of group 0 to 3.4117e-06.\n","\n","Epoch  685000\n","\n","Epoch  686000\n","\n","Epoch  687000\n","\n","Epoch  688000\n","\n","Epoch  689000\n","Epoch 689387: reducing learning rate of group 0 to 3.3775e-06.\n","\n","Epoch  690000\n","\n","Epoch  691000\n","\n","Epoch  692000\n","\n","Epoch  693000\n","\n","Epoch  694000\n","Epoch 694388: reducing learning rate of group 0 to 3.3438e-06.\n","\n","Epoch  695000\n","\n","Epoch  696000\n","\n","Epoch  697000\n","\n","Epoch  698000\n","\n","Epoch  699000\n","Epoch 699389: reducing learning rate of group 0 to 3.3103e-06.\n","\n","Epoch  700000\n","\n","Epoch  701000\n","\n","Epoch  702000\n","\n","Epoch  703000\n","\n","Epoch  704000\n","Epoch 704390: reducing learning rate of group 0 to 3.2772e-06.\n","\n","Epoch  705000\n","\n","Epoch  706000\n","\n","Epoch  707000\n","\n","Epoch  708000\n","\n","Epoch  709000\n","Epoch 709391: reducing learning rate of group 0 to 3.2445e-06.\n","\n","Epoch  710000\n","\n","Epoch  711000\n","\n","Epoch  712000\n","\n","Epoch  713000\n","\n","Epoch  714000\n","Epoch 714392: reducing learning rate of group 0 to 3.2120e-06.\n","\n","Epoch  715000\n","\n","Epoch  716000\n","\n","Epoch  717000\n","\n","Epoch  718000\n","\n","Epoch  719000\n","Epoch 719393: reducing learning rate of group 0 to 3.1799e-06.\n","\n","Epoch  720000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  720000\n","beta1: (goal 3.532) tensor([3.4491], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6976], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6876], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2959], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1011], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  721000\n","\n","Epoch  722000\n","\n","Epoch  723000\n","\n","Epoch  724000\n","Epoch 724394: reducing learning rate of group 0 to 3.1481e-06.\n","\n","Epoch  725000\n","\n","Epoch  726000\n","\n","Epoch  727000\n","\n","Epoch  728000\n","\n","Epoch  729000\n","Epoch 729395: reducing learning rate of group 0 to 3.1166e-06.\n","\n","Epoch  730000\n","\n","Epoch  731000\n","\n","Epoch  732000\n","\n","Epoch  733000\n","\n","Epoch  734000\n","Epoch 734396: reducing learning rate of group 0 to 3.0854e-06.\n","\n","Epoch  735000\n","\n","Epoch  736000\n","\n","Epoch  737000\n","\n","Epoch  738000\n","\n","Epoch  739000\n","Epoch 739397: reducing learning rate of group 0 to 3.0546e-06.\n","\n","Epoch  740000\n","\n","Epoch  741000\n","\n","Epoch  742000\n","\n","Epoch  743000\n","\n","Epoch  744000\n","Epoch 744398: reducing learning rate of group 0 to 3.0240e-06.\n","\n","Epoch  745000\n","\n","Epoch  746000\n","\n","Epoch  747000\n","\n","Epoch  748000\n","\n","Epoch  749000\n","Epoch 749399: reducing learning rate of group 0 to 2.9938e-06.\n","\n","Epoch  750000\n","\n","Epoch  751000\n","\n","Epoch  752000\n","\n","Epoch  753000\n","\n","Epoch  754000\n","Epoch 754400: reducing learning rate of group 0 to 2.9639e-06.\n","\n","Epoch  755000\n","\n","Epoch  756000\n","\n","Epoch  757000\n","\n","Epoch  758000\n","\n","Epoch  759000\n","Epoch 759401: reducing learning rate of group 0 to 2.9342e-06.\n","\n","Epoch  760000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  760000\n","beta1: (goal 3.532) tensor([3.4465], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6977], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6877], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2958], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1009], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  761000\n","\n","Epoch  762000\n","\n","Epoch  763000\n","\n","Epoch  764000\n","Epoch 764402: reducing learning rate of group 0 to 2.9049e-06.\n","\n","Epoch  765000\n","\n","Epoch  766000\n","\n","Epoch  767000\n","\n","Epoch  768000\n","\n","Epoch  769000\n","Epoch 769403: reducing learning rate of group 0 to 2.8758e-06.\n","\n","Epoch  770000\n","\n","Epoch  771000\n","\n","Epoch  772000\n","\n","Epoch  773000\n","\n","Epoch  774000\n","Epoch 774404: reducing learning rate of group 0 to 2.8471e-06.\n","\n","Epoch  775000\n","\n","Epoch  776000\n","\n","Epoch  777000\n","\n","Epoch  778000\n","\n","Epoch  779000\n","Epoch 779405: reducing learning rate of group 0 to 2.8186e-06.\n","\n","Epoch  780000\n","\n","Epoch  781000\n","\n","Epoch  782000\n","\n","Epoch  783000\n","\n","Epoch  784000\n","Epoch 784406: reducing learning rate of group 0 to 2.7904e-06.\n","\n","Epoch  785000\n","\n","Epoch  786000\n","\n","Epoch  787000\n","\n","Epoch  788000\n","\n","Epoch  789000\n","Epoch 789407: reducing learning rate of group 0 to 2.7625e-06.\n","\n","Epoch  790000\n","\n","Epoch  791000\n","\n","Epoch  792000\n","\n","Epoch  793000\n","\n","Epoch  794000\n","Epoch 794408: reducing learning rate of group 0 to 2.7349e-06.\n","\n","Epoch  795000\n","\n","Epoch  796000\n","\n","Epoch  797000\n","\n","Epoch  798000\n","\n","Epoch  799000\n","Epoch 799409: reducing learning rate of group 0 to 2.7075e-06.\n","\n","Epoch  800000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  800000\n","beta1: (goal 3.532) tensor([3.4469], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6978], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6878], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2957], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1008], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  801000\n","\n","Epoch  802000\n","\n","Epoch  803000\n","\n","Epoch  804000\n","Epoch 804410: reducing learning rate of group 0 to 2.6805e-06.\n","\n","Epoch  805000\n","\n","Epoch  806000\n","\n","Epoch  807000\n","\n","Epoch  808000\n","\n","Epoch  809000\n","Epoch 809411: reducing learning rate of group 0 to 2.6537e-06.\n","\n","Epoch  810000\n","\n","Epoch  811000\n","\n","Epoch  812000\n","\n","Epoch  813000\n","\n","Epoch  814000\n","Epoch 814412: reducing learning rate of group 0 to 2.6271e-06.\n","\n","Epoch  815000\n","\n","Epoch  816000\n","\n","Epoch  817000\n","\n","Epoch  818000\n","\n","Epoch  819000\n","Epoch 819413: reducing learning rate of group 0 to 2.6009e-06.\n","\n","Epoch  820000\n","\n","Epoch  821000\n","\n","Epoch  822000\n","\n","Epoch  823000\n","\n","Epoch  824000\n","Epoch 824414: reducing learning rate of group 0 to 2.5748e-06.\n","\n","Epoch  825000\n","\n","Epoch  826000\n","\n","Epoch  827000\n","\n","Epoch  828000\n","\n","Epoch  829000\n","Epoch 829415: reducing learning rate of group 0 to 2.5491e-06.\n","\n","Epoch  830000\n","\n","Epoch  831000\n","\n","Epoch  832000\n","\n","Epoch  833000\n","\n","Epoch  834000\n","Epoch 834416: reducing learning rate of group 0 to 2.5236e-06.\n","\n","Epoch  835000\n","\n","Epoch  836000\n","\n","Epoch  837000\n","\n","Epoch  838000\n","\n","Epoch  839000\n","Epoch 839417: reducing learning rate of group 0 to 2.4984e-06.\n","\n","Epoch  840000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  840000\n","beta1: (goal 3.532) tensor([3.4455], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0198], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6978], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2956], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1007], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  841000\n","\n","Epoch  842000\n","\n","Epoch  843000\n","\n","Epoch  844000\n","Epoch 844418: reducing learning rate of group 0 to 2.4734e-06.\n","\n","Epoch  845000\n","\n","Epoch  846000\n","\n","Epoch  847000\n","\n","Epoch  848000\n","\n","Epoch  849000\n","Epoch 849419: reducing learning rate of group 0 to 2.4487e-06.\n","\n","Epoch  850000\n","\n","Epoch  851000\n","\n","Epoch  852000\n","\n","Epoch  853000\n","\n","Epoch  854000\n","Epoch 854420: reducing learning rate of group 0 to 2.4242e-06.\n","\n","Epoch  855000\n","\n","Epoch  856000\n","\n","Epoch  857000\n","\n","Epoch  858000\n","\n","Epoch  859000\n","Epoch 859421: reducing learning rate of group 0 to 2.3999e-06.\n","\n","Epoch  860000\n","\n","Epoch  861000\n","\n","Epoch  862000\n","\n","Epoch  863000\n","\n","Epoch  864000\n","Epoch 864422: reducing learning rate of group 0 to 2.3759e-06.\n","\n","Epoch  865000\n","\n","Epoch  866000\n","\n","Epoch  867000\n","\n","Epoch  868000\n","\n","Epoch  869000\n","Epoch 869423: reducing learning rate of group 0 to 2.3522e-06.\n","\n","Epoch  870000\n","\n","Epoch  871000\n","\n","Epoch  872000\n","\n","Epoch  873000\n","\n","Epoch  874000\n","Epoch 874424: reducing learning rate of group 0 to 2.3286e-06.\n","\n","Epoch  875000\n","\n","Epoch  876000\n","\n","Epoch  877000\n","\n","Epoch  878000\n","\n","Epoch  879000\n","Epoch 879425: reducing learning rate of group 0 to 2.3054e-06.\n","\n","Epoch  880000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  880000\n","beta1: (goal 3.532) tensor([3.4453], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6979], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2956], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1006], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  881000\n","\n","Epoch  882000\n","\n","Epoch  883000\n","\n","Epoch  884000\n","Epoch 884426: reducing learning rate of group 0 to 2.2823e-06.\n","\n","Epoch  885000\n","\n","Epoch  886000\n","\n","Epoch  887000\n","\n","Epoch  888000\n","\n","Epoch  889000\n","Epoch 889427: reducing learning rate of group 0 to 2.2595e-06.\n","\n","Epoch  890000\n","\n","Epoch  891000\n","\n","Epoch  892000\n","\n","Epoch  893000\n","\n","Epoch  894000\n","Epoch 894428: reducing learning rate of group 0 to 2.2369e-06.\n","\n","Epoch  895000\n","\n","Epoch  896000\n","\n","Epoch  897000\n","\n","Epoch  898000\n","\n","Epoch  899000\n","Epoch 899429: reducing learning rate of group 0 to 2.2145e-06.\n","\n","Epoch  900000\n","\n","Epoch  901000\n","\n","Epoch  902000\n","\n","Epoch  903000\n","\n","Epoch  904000\n","Epoch 904430: reducing learning rate of group 0 to 2.1924e-06.\n","\n","Epoch  905000\n","\n","Epoch  906000\n","\n","Epoch  907000\n","\n","Epoch  908000\n","\n","Epoch  909000\n","Epoch 909431: reducing learning rate of group 0 to 2.1704e-06.\n","\n","Epoch  910000\n","\n","Epoch  911000\n","\n","Epoch  912000\n","\n","Epoch  913000\n","\n","Epoch  914000\n","Epoch 914432: reducing learning rate of group 0 to 2.1487e-06.\n","\n","Epoch  915000\n","\n","Epoch  916000\n","\n","Epoch  917000\n","\n","Epoch  918000\n","\n","Epoch  919000\n","Epoch 919433: reducing learning rate of group 0 to 2.1273e-06.\n","\n","Epoch  920000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  920000\n","beta1: (goal 3.532) tensor([3.4484], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6980], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6878], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2956], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1006], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  921000\n","\n","Epoch  922000\n","\n","Epoch  923000\n","\n","Epoch  924000\n","Epoch 924434: reducing learning rate of group 0 to 2.1060e-06.\n","\n","Epoch  925000\n","\n","Epoch  926000\n","\n","Epoch  927000\n","\n","Epoch  928000\n","\n","Epoch  929000\n","Epoch 929435: reducing learning rate of group 0 to 2.0849e-06.\n","\n","Epoch  930000\n","\n","Epoch  931000\n","\n","Epoch  932000\n","\n","Epoch  933000\n","\n","Epoch  934000\n","Epoch 934436: reducing learning rate of group 0 to 2.0641e-06.\n","\n","Epoch  935000\n","\n","Epoch  936000\n","\n","Epoch  937000\n","\n","Epoch  938000\n","\n","Epoch  939000\n","Epoch 939437: reducing learning rate of group 0 to 2.0434e-06.\n","\n","Epoch  940000\n","\n","Epoch  941000\n","\n","Epoch  942000\n","\n","Epoch  943000\n","\n","Epoch  944000\n","Epoch 944438: reducing learning rate of group 0 to 2.0230e-06.\n","\n","Epoch  945000\n","\n","Epoch  946000\n","\n","Epoch  947000\n","\n","Epoch  948000\n","\n","Epoch  949000\n","Epoch 949439: reducing learning rate of group 0 to 2.0028e-06.\n","\n","Epoch  950000\n","\n","Epoch  951000\n","\n","Epoch  952000\n","\n","Epoch  953000\n","\n","Epoch  954000\n","Epoch 954440: reducing learning rate of group 0 to 1.9827e-06.\n","\n","Epoch  955000\n","\n","Epoch  956000\n","\n","Epoch  957000\n","\n","Epoch  958000\n","\n","Epoch  959000\n","Epoch 959441: reducing learning rate of group 0 to 1.9629e-06.\n","\n","Epoch  960000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  960000\n","beta1: (goal 3.532) tensor([3.4490], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6980], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6878], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2955], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1005], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  961000\n","\n","Epoch  962000\n","\n","Epoch  963000\n","\n","Epoch  964000\n","Epoch 964442: reducing learning rate of group 0 to 1.9433e-06.\n","\n","Epoch  965000\n","\n","Epoch  966000\n","\n","Epoch  967000\n","\n","Epoch  968000\n","\n","Epoch  969000\n","Epoch 969443: reducing learning rate of group 0 to 1.9239e-06.\n","\n","Epoch  970000\n","\n","Epoch  971000\n","\n","Epoch  972000\n","\n","Epoch  973000\n","\n","Epoch  974000\n","Epoch 974444: reducing learning rate of group 0 to 1.9046e-06.\n","\n","Epoch  975000\n","\n","Epoch  976000\n","\n","Epoch  977000\n","\n","Epoch  978000\n","\n","Epoch  979000\n","Epoch 979445: reducing learning rate of group 0 to 1.8856e-06.\n","\n","Epoch  980000\n","\n","Epoch  981000\n","\n","Epoch  982000\n","\n","Epoch  983000\n","\n","Epoch  984000\n","Epoch 984446: reducing learning rate of group 0 to 1.8667e-06.\n","\n","Epoch  985000\n","\n","Epoch  986000\n","\n","Epoch  987000\n","\n","Epoch  988000\n","\n","Epoch  989000\n","Epoch 989447: reducing learning rate of group 0 to 1.8480e-06.\n","\n","Epoch  990000\n","\n","Epoch  991000\n","\n","Epoch  992000\n","\n","Epoch  993000\n","\n","Epoch  994000\n","Epoch 994448: reducing learning rate of group 0 to 1.8296e-06.\n","\n","Epoch  995000\n","\n","Epoch  996000\n","\n","Epoch  997000\n","\n","Epoch  998000\n","\n","Epoch  999000\n","Epoch 999449: reducing learning rate of group 0 to 1.8113e-06.\n","\n","Epoch  1000000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1000000\n","beta1: (goal 3.532) tensor([3.4489], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6981], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2955], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1005], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1001000\n","\n","Epoch  1002000\n","\n","Epoch  1003000\n","\n","Epoch  1004000\n","Epoch 1004450: reducing learning rate of group 0 to 1.7932e-06.\n","\n","Epoch  1005000\n","\n","Epoch  1006000\n","\n","Epoch  1007000\n","\n","Epoch  1008000\n","\n","Epoch  1009000\n","Epoch 1009451: reducing learning rate of group 0 to 1.7752e-06.\n","\n","Epoch  1010000\n","\n","Epoch  1011000\n","\n","Epoch  1012000\n","\n","Epoch  1013000\n","\n","Epoch  1014000\n","Epoch 1014452: reducing learning rate of group 0 to 1.7575e-06.\n","\n","Epoch  1015000\n","\n","Epoch  1016000\n","\n","Epoch  1017000\n","\n","Epoch  1018000\n","\n","Epoch  1019000\n","Epoch 1019453: reducing learning rate of group 0 to 1.7399e-06.\n","\n","Epoch  1020000\n","\n","Epoch  1021000\n","\n","Epoch  1022000\n","\n","Epoch  1023000\n","\n","Epoch  1024000\n","Epoch 1024454: reducing learning rate of group 0 to 1.7225e-06.\n","\n","Epoch  1025000\n","\n","Epoch  1026000\n","\n","Epoch  1027000\n","\n","Epoch  1028000\n","\n","Epoch  1029000\n","Epoch 1029455: reducing learning rate of group 0 to 1.7053e-06.\n","\n","Epoch  1030000\n","\n","Epoch  1031000\n","\n","Epoch  1032000\n","\n","Epoch  1033000\n","\n","Epoch  1034000\n","Epoch 1034456: reducing learning rate of group 0 to 1.6882e-06.\n","\n","Epoch  1035000\n","\n","Epoch  1036000\n","\n","Epoch  1037000\n","\n","Epoch  1038000\n","\n","Epoch  1039000\n","Epoch 1039457: reducing learning rate of group 0 to 1.6713e-06.\n","\n","Epoch  1040000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1040000\n","beta1: (goal 3.532) tensor([3.4490], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6981], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2955], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1005], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1041000\n","\n","Epoch  1042000\n","\n","Epoch  1043000\n","\n","Epoch  1044000\n","Epoch 1044458: reducing learning rate of group 0 to 1.6546e-06.\n","\n","Epoch  1045000\n","\n","Epoch  1046000\n","\n","Epoch  1047000\n","\n","Epoch  1048000\n","\n","Epoch  1049000\n","Epoch 1049459: reducing learning rate of group 0 to 1.6381e-06.\n","\n","Epoch  1050000\n","\n","Epoch  1051000\n","\n","Epoch  1052000\n","\n","Epoch  1053000\n","\n","Epoch  1054000\n","Epoch 1054460: reducing learning rate of group 0 to 1.6217e-06.\n","\n","Epoch  1055000\n","\n","Epoch  1056000\n","\n","Epoch  1057000\n","\n","Epoch  1058000\n","\n","Epoch  1059000\n","Epoch 1059461: reducing learning rate of group 0 to 1.6055e-06.\n","\n","Epoch  1060000\n","\n","Epoch  1061000\n","\n","Epoch  1062000\n","\n","Epoch  1063000\n","\n","Epoch  1064000\n","Epoch 1064462: reducing learning rate of group 0 to 1.5894e-06.\n","\n","Epoch  1065000\n","\n","Epoch  1066000\n","\n","Epoch  1067000\n","\n","Epoch  1068000\n","\n","Epoch  1069000\n","Epoch 1069463: reducing learning rate of group 0 to 1.5735e-06.\n","\n","Epoch  1070000\n","\n","Epoch  1071000\n","\n","Epoch  1072000\n","\n","Epoch  1073000\n","\n","Epoch  1074000\n","Epoch 1074464: reducing learning rate of group 0 to 1.5578e-06.\n","\n","Epoch  1075000\n","\n","Epoch  1076000\n","\n","Epoch  1077000\n","\n","Epoch  1078000\n","\n","Epoch  1079000\n","Epoch 1079465: reducing learning rate of group 0 to 1.5422e-06.\n","\n","Epoch  1080000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1080000\n","beta1: (goal 3.532) tensor([3.4487], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6982], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2955], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1004], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1081000\n","\n","Epoch  1082000\n","\n","Epoch  1083000\n","\n","Epoch  1084000\n","Epoch 1084466: reducing learning rate of group 0 to 1.5268e-06.\n","\n","Epoch  1085000\n","\n","Epoch  1086000\n","\n","Epoch  1087000\n","\n","Epoch  1088000\n","\n","Epoch  1089000\n","Epoch 1089467: reducing learning rate of group 0 to 1.5115e-06.\n","\n","Epoch  1090000\n","\n","Epoch  1091000\n","\n","Epoch  1092000\n","\n","Epoch  1093000\n","\n","Epoch  1094000\n","Epoch 1094468: reducing learning rate of group 0 to 1.4964e-06.\n","\n","Epoch  1095000\n","\n","Epoch  1096000\n","\n","Epoch  1097000\n","\n","Epoch  1098000\n","\n","Epoch  1099000\n","Epoch 1099469: reducing learning rate of group 0 to 1.4814e-06.\n","\n","Epoch  1100000\n","\n","Epoch  1101000\n","\n","Epoch  1102000\n","\n","Epoch  1103000\n","\n","Epoch  1104000\n","Epoch 1104470: reducing learning rate of group 0 to 1.4666e-06.\n","\n","Epoch  1105000\n","\n","Epoch  1106000\n","\n","Epoch  1107000\n","\n","Epoch  1108000\n","\n","Epoch  1109000\n","Epoch 1109471: reducing learning rate of group 0 to 1.4520e-06.\n","\n","Epoch  1110000\n","\n","Epoch  1111000\n","\n","Epoch  1112000\n","\n","Epoch  1113000\n","\n","Epoch  1114000\n","Epoch 1114472: reducing learning rate of group 0 to 1.4374e-06.\n","\n","Epoch  1115000\n","\n","Epoch  1116000\n","\n","Epoch  1117000\n","\n","Epoch  1118000\n","\n","Epoch  1119000\n","Epoch 1119473: reducing learning rate of group 0 to 1.4231e-06.\n","\n","Epoch  1120000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1120000\n","beta1: (goal 3.532) tensor([3.4489], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6982], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2955], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1004], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1121000\n","\n","Epoch  1122000\n","\n","Epoch  1123000\n","\n","Epoch  1124000\n","Epoch 1124474: reducing learning rate of group 0 to 1.4088e-06.\n","\n","Epoch  1125000\n","\n","Epoch  1126000\n","\n","Epoch  1127000\n","\n","Epoch  1128000\n","\n","Epoch  1129000\n","Epoch 1129475: reducing learning rate of group 0 to 1.3948e-06.\n","\n","Epoch  1130000\n","\n","Epoch  1131000\n","\n","Epoch  1132000\n","\n","Epoch  1133000\n","\n","Epoch  1134000\n","Epoch 1134476: reducing learning rate of group 0 to 1.3808e-06.\n","\n","Epoch  1135000\n","\n","Epoch  1136000\n","\n","Epoch  1137000\n","\n","Epoch  1138000\n","\n","Epoch  1139000\n","Epoch 1139477: reducing learning rate of group 0 to 1.3670e-06.\n","\n","Epoch  1140000\n","\n","Epoch  1141000\n","\n","Epoch  1142000\n","\n","Epoch  1143000\n","\n","Epoch  1144000\n","Epoch 1144478: reducing learning rate of group 0 to 1.3533e-06.\n","\n","Epoch  1145000\n","\n","Epoch  1146000\n","\n","Epoch  1147000\n","\n","Epoch  1148000\n","\n","Epoch  1149000\n","Epoch 1149479: reducing learning rate of group 0 to 1.3398e-06.\n","\n","Epoch  1150000\n","\n","Epoch  1151000\n","\n","Epoch  1152000\n","\n","Epoch  1153000\n","\n","Epoch  1154000\n","Epoch 1154480: reducing learning rate of group 0 to 1.3264e-06.\n","\n","Epoch  1155000\n","\n","Epoch  1156000\n","\n","Epoch  1157000\n","\n","Epoch  1158000\n","\n","Epoch  1159000\n","Epoch 1159481: reducing learning rate of group 0 to 1.3131e-06.\n","\n","Epoch  1160000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1160000\n","beta1: (goal 3.532) tensor([3.4492], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6982], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1004], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1161000\n","\n","Epoch  1162000\n","\n","Epoch  1163000\n","\n","Epoch  1164000\n","Epoch 1164482: reducing learning rate of group 0 to 1.3000e-06.\n","\n","Epoch  1165000\n","\n","Epoch  1166000\n","\n","Epoch  1167000\n","\n","Epoch  1168000\n","\n","Epoch  1169000\n","Epoch 1169483: reducing learning rate of group 0 to 1.2870e-06.\n","\n","Epoch  1170000\n","\n","Epoch  1171000\n","\n","Epoch  1172000\n","\n","Epoch  1173000\n","\n","Epoch  1174000\n","Epoch 1174484: reducing learning rate of group 0 to 1.2741e-06.\n","\n","Epoch  1175000\n","\n","Epoch  1176000\n","\n","Epoch  1177000\n","\n","Epoch  1178000\n","\n","Epoch  1179000\n","Epoch 1179485: reducing learning rate of group 0 to 1.2614e-06.\n","\n","Epoch  1180000\n","\n","Epoch  1181000\n","\n","Epoch  1182000\n","\n","Epoch  1183000\n","\n","Epoch  1184000\n","Epoch 1184486: reducing learning rate of group 0 to 1.2488e-06.\n","\n","Epoch  1185000\n","\n","Epoch  1186000\n","\n","Epoch  1187000\n","\n","Epoch  1188000\n","\n","Epoch  1189000\n","Epoch 1189487: reducing learning rate of group 0 to 1.2363e-06.\n","\n","Epoch  1190000\n","\n","Epoch  1191000\n","\n","Epoch  1192000\n","\n","Epoch  1193000\n","\n","Epoch  1194000\n","Epoch 1194488: reducing learning rate of group 0 to 1.2239e-06.\n","\n","Epoch  1195000\n","\n","Epoch  1196000\n","\n","Epoch  1197000\n","\n","Epoch  1198000\n","\n","Epoch  1199000\n","Epoch 1199489: reducing learning rate of group 0 to 1.2117e-06.\n","\n","Epoch  1200000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1200000\n","beta1: (goal 3.532) tensor([3.4496], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6983], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6879], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1004], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1201000\n","\n","Epoch  1202000\n","\n","Epoch  1203000\n","\n","Epoch  1204000\n","Epoch 1204490: reducing learning rate of group 0 to 1.1996e-06.\n","\n","Epoch  1205000\n","\n","Epoch  1206000\n","\n","Epoch  1207000\n","\n","Epoch  1208000\n","\n","Epoch  1209000\n","Epoch 1209491: reducing learning rate of group 0 to 1.1876e-06.\n","\n","Epoch  1210000\n","\n","Epoch  1211000\n","\n","Epoch  1212000\n","\n","Epoch  1213000\n","\n","Epoch  1214000\n","Epoch 1214492: reducing learning rate of group 0 to 1.1757e-06.\n","\n","Epoch  1215000\n","\n","Epoch  1216000\n","\n","Epoch  1217000\n","\n","Epoch  1218000\n","\n","Epoch  1219000\n","Epoch 1219493: reducing learning rate of group 0 to 1.1639e-06.\n","\n","Epoch  1220000\n","\n","Epoch  1221000\n","\n","Epoch  1222000\n","\n","Epoch  1223000\n","\n","Epoch  1224000\n","Epoch 1224494: reducing learning rate of group 0 to 1.1523e-06.\n","\n","Epoch  1225000\n","\n","Epoch  1226000\n","\n","Epoch  1227000\n","\n","Epoch  1228000\n","\n","Epoch  1229000\n","Epoch 1229495: reducing learning rate of group 0 to 1.1408e-06.\n","\n","Epoch  1230000\n","\n","Epoch  1231000\n","\n","Epoch  1232000\n","\n","Epoch  1233000\n","\n","Epoch  1234000\n","Epoch 1234496: reducing learning rate of group 0 to 1.1294e-06.\n","\n","Epoch  1235000\n","\n","Epoch  1236000\n","\n","Epoch  1237000\n","\n","Epoch  1238000\n","\n","Epoch  1239000\n","Epoch 1239497: reducing learning rate of group 0 to 1.1181e-06.\n","\n","Epoch  1240000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1240000\n","beta1: (goal 3.532) tensor([3.4498], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6983], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6880], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1004], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1241000\n","\n","Epoch  1242000\n","\n","Epoch  1243000\n","\n","Epoch  1244000\n","Epoch 1244498: reducing learning rate of group 0 to 1.1069e-06.\n","\n","Epoch  1245000\n","\n","Epoch  1246000\n","\n","Epoch  1247000\n","\n","Epoch  1248000\n","\n","Epoch  1249000\n","Epoch 1249499: reducing learning rate of group 0 to 1.0958e-06.\n","\n","Epoch  1250000\n","\n","Epoch  1251000\n","\n","Epoch  1252000\n","\n","Epoch  1253000\n","\n","Epoch  1254000\n","Epoch 1254500: reducing learning rate of group 0 to 1.0849e-06.\n","\n","Epoch  1255000\n","\n","Epoch  1256000\n","\n","Epoch  1257000\n","\n","Epoch  1258000\n","\n","Epoch  1259000\n","Epoch 1259501: reducing learning rate of group 0 to 1.0740e-06.\n","\n","Epoch  1260000\n","\n","Epoch  1261000\n","\n","Epoch  1262000\n","\n","Epoch  1263000\n","\n","Epoch  1264000\n","Epoch 1264502: reducing learning rate of group 0 to 1.0633e-06.\n","\n","Epoch  1265000\n","\n","Epoch  1266000\n","\n","Epoch  1267000\n","\n","Epoch  1268000\n","\n","Epoch  1269000\n","Epoch 1269503: reducing learning rate of group 0 to 1.0526e-06.\n","\n","Epoch  1270000\n","\n","Epoch  1271000\n","\n","Epoch  1272000\n","\n","Epoch  1273000\n","\n","Epoch  1274000\n","Epoch 1274504: reducing learning rate of group 0 to 1.0421e-06.\n","\n","Epoch  1275000\n","\n","Epoch  1276000\n","\n","Epoch  1277000\n","\n","Epoch  1278000\n","\n","Epoch  1279000\n","Epoch 1279505: reducing learning rate of group 0 to 1.0317e-06.\n","\n","Epoch  1280000\n","\n","Saving model... Loss is:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1280000\n","beta1: (goal 3.532) tensor([3.4495], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6983], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6880], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1281000\n","\n","Epoch  1282000\n","\n","Epoch  1283000\n","\n","Epoch  1284000\n","Epoch 1284506: reducing learning rate of group 0 to 1.0214e-06.\n","\n","Epoch  1285000\n","\n","Epoch  1286000\n","\n","Epoch  1287000\n","\n","Epoch  1288000\n","\n","Epoch  1289000\n","Epoch 1289507: reducing learning rate of group 0 to 1.0112e-06.\n","\n","Epoch  1290000\n","\n","Epoch  1291000\n","\n","Epoch  1292000\n","\n","Epoch  1293000\n","\n","Epoch  1294000\n","Epoch 1294508: reducing learning rate of group 0 to 1.0011e-06.\n","\n","Epoch  1295000\n","\n","Epoch  1296000\n","\n","Epoch  1297000\n","\n","Epoch  1298000\n","\n","Epoch  1299000\n","Epoch 1299509: reducing learning rate of group 0 to 9.9105e-07.\n","\n","Epoch  1300000\n","\n","Epoch  1301000\n","\n","Epoch  1302000\n","\n","Epoch  1303000\n","\n","Epoch  1304000\n","\n","Epoch  1305000\n","\n","Epoch  1306000\n","\n","Epoch  1307000\n","\n","Epoch  1308000\n","\n","Epoch  1309000\n","\n","Epoch  1310000\n","\n","Epoch  1311000\n","\n","Epoch  1312000\n","\n","Epoch  1313000\n","\n","Epoch  1314000\n","\n","Epoch  1315000\n","\n","Epoch  1316000\n","\n","Epoch  1317000\n","\n","Epoch  1318000\n","\n","Epoch  1319000\n","\n","Epoch  1320000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1320000\n","beta1: (goal 3.532) tensor([3.4484], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6983], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6881], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1321000\n","\n","Epoch  1322000\n","\n","Epoch  1323000\n","\n","Epoch  1324000\n","\n","Epoch  1325000\n","\n","Epoch  1326000\n","\n","Epoch  1327000\n","\n","Epoch  1328000\n","\n","Epoch  1329000\n","\n","Epoch  1330000\n","\n","Epoch  1331000\n","\n","Epoch  1332000\n","\n","Epoch  1333000\n","\n","Epoch  1334000\n","\n","Epoch  1335000\n","\n","Epoch  1336000\n","\n","Epoch  1337000\n","\n","Epoch  1338000\n","\n","Epoch  1339000\n","\n","Epoch  1340000\n","\n","Epoch  1341000\n","\n","Epoch  1342000\n","\n","Epoch  1343000\n","\n","Epoch  1344000\n","\n","Epoch  1345000\n","\n","Epoch  1346000\n","\n","Epoch  1347000\n","\n","Epoch  1348000\n","\n","Epoch  1349000\n","\n","Epoch  1350000\n","\n","Epoch  1351000\n","\n","Epoch  1352000\n","\n","Epoch  1353000\n","\n","Epoch  1354000\n","\n","Epoch  1355000\n","\n","Epoch  1356000\n","\n","Epoch  1357000\n","\n","Epoch  1358000\n","\n","Epoch  1359000\n","\n","Epoch  1360000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1360000\n","beta1: (goal 3.532) tensor([3.4493], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6984], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6882], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1361000\n","\n","Epoch  1362000\n","\n","Epoch  1363000\n","\n","Epoch  1364000\n","\n","Epoch  1365000\n","\n","Epoch  1366000\n","\n","Epoch  1367000\n","\n","Epoch  1368000\n","\n","Epoch  1369000\n","\n","Epoch  1370000\n","\n","Epoch  1371000\n","\n","Epoch  1372000\n","\n","Epoch  1373000\n","\n","Epoch  1374000\n","\n","Epoch  1375000\n","\n","Epoch  1376000\n","\n","Epoch  1377000\n","\n","Epoch  1378000\n","\n","Epoch  1379000\n","\n","Epoch  1380000\n","\n","Epoch  1381000\n","\n","Epoch  1382000\n","\n","Epoch  1383000\n","\n","Epoch  1384000\n","\n","Epoch  1385000\n","\n","Epoch  1386000\n","\n","Epoch  1387000\n","\n","Epoch  1388000\n","\n","Epoch  1389000\n","\n","Epoch  1390000\n","\n","Epoch  1391000\n","\n","Epoch  1392000\n","\n","Epoch  1393000\n","\n","Epoch  1394000\n","\n","Epoch  1395000\n","\n","Epoch  1396000\n","\n","Epoch  1397000\n","\n","Epoch  1398000\n","\n","Epoch  1399000\n","\n","Epoch  1400000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1400000\n","beta1: (goal 3.532) tensor([3.4455], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6984], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6883], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1401000\n","\n","Epoch  1402000\n","\n","Epoch  1403000\n","\n","Epoch  1404000\n","\n","Epoch  1405000\n","\n","Epoch  1406000\n","\n","Epoch  1407000\n","\n","Epoch  1408000\n","\n","Epoch  1409000\n","\n","Epoch  1410000\n","\n","Epoch  1411000\n","\n","Epoch  1412000\n","\n","Epoch  1413000\n","\n","Epoch  1414000\n","\n","Epoch  1415000\n","\n","Epoch  1416000\n","\n","Epoch  1417000\n","\n","Epoch  1418000\n","\n","Epoch  1419000\n","\n","Epoch  1420000\n","\n","Epoch  1421000\n","\n","Epoch  1422000\n","\n","Epoch  1423000\n","\n","Epoch  1424000\n","\n","Epoch  1425000\n","\n","Epoch  1426000\n","\n","Epoch  1427000\n","\n","Epoch  1428000\n","\n","Epoch  1429000\n","\n","Epoch  1430000\n","\n","Epoch  1431000\n","\n","Epoch  1432000\n","\n","Epoch  1433000\n","\n","Epoch  1434000\n","\n","Epoch  1435000\n","\n","Epoch  1436000\n","\n","Epoch  1437000\n","\n","Epoch  1438000\n","\n","Epoch  1439000\n","\n","Epoch  1440000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1440000\n","beta1: (goal 3.532) tensor([3.4417], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6984], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6885], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1441000\n","\n","Epoch  1442000\n","\n","Epoch  1443000\n","\n","Epoch  1444000\n","\n","Epoch  1445000\n","\n","Epoch  1446000\n","\n","Epoch  1447000\n","\n","Epoch  1448000\n","\n","Epoch  1449000\n","\n","Epoch  1450000\n","\n","Epoch  1451000\n","\n","Epoch  1452000\n","\n","Epoch  1453000\n","\n","Epoch  1454000\n","\n","Epoch  1455000\n","\n","Epoch  1456000\n","\n","Epoch  1457000\n","\n","Epoch  1458000\n","\n","Epoch  1459000\n","\n","Epoch  1460000\n","\n","Epoch  1461000\n","\n","Epoch  1462000\n","\n","Epoch  1463000\n","\n","Epoch  1464000\n","\n","Epoch  1465000\n","\n","Epoch  1466000\n","\n","Epoch  1467000\n","\n","Epoch  1468000\n","\n","Epoch  1469000\n","\n","Epoch  1470000\n","\n","Epoch  1471000\n","\n","Epoch  1472000\n","\n","Epoch  1473000\n","\n","Epoch  1474000\n","\n","Epoch  1475000\n","\n","Epoch  1476000\n","\n","Epoch  1477000\n","\n","Epoch  1478000\n","\n","Epoch  1479000\n","\n","Epoch  1480000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1480000\n","beta1: (goal 3.532) tensor([3.4373], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6985], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6884], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1481000\n","\n","Epoch  1482000\n","\n","Epoch  1483000\n","\n","Epoch  1484000\n","\n","Epoch  1485000\n","\n","Epoch  1486000\n","\n","Epoch  1487000\n","\n","Epoch  1488000\n","\n","Epoch  1489000\n","\n","Epoch  1490000\n","\n","Epoch  1491000\n","\n","Epoch  1492000\n","\n","Epoch  1493000\n","\n","Epoch  1494000\n","\n","Epoch  1495000\n","\n","Epoch  1496000\n","\n","Epoch  1497000\n","\n","Epoch  1498000\n","\n","Epoch  1499000\n","\n","Epoch  1500000\n","\n","Epoch  1501000\n","\n","Epoch  1502000\n","\n","Epoch  1503000\n","\n","Epoch  1504000\n","\n","Epoch  1505000\n","\n","Epoch  1506000\n","\n","Epoch  1507000\n","\n","Epoch  1508000\n","\n","Epoch  1509000\n","\n","Epoch  1510000\n","\n","Epoch  1511000\n","\n","Epoch  1512000\n","\n","Epoch  1513000\n","\n","Epoch  1514000\n","\n","Epoch  1515000\n","\n","Epoch  1516000\n","\n","Epoch  1517000\n","\n","Epoch  1518000\n","\n","Epoch  1519000\n","\n","Epoch  1520000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1520000\n","beta1: (goal 3.532) tensor([3.4347], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6985], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6885], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1521000\n","\n","Epoch  1522000\n","\n","Epoch  1523000\n","\n","Epoch  1524000\n","\n","Epoch  1525000\n","\n","Epoch  1526000\n","\n","Epoch  1527000\n","\n","Epoch  1528000\n","\n","Epoch  1529000\n","\n","Epoch  1530000\n","\n","Epoch  1531000\n","\n","Epoch  1532000\n","\n","Epoch  1533000\n","\n","Epoch  1534000\n","\n","Epoch  1535000\n","\n","Epoch  1536000\n","\n","Epoch  1537000\n","\n","Epoch  1538000\n","\n","Epoch  1539000\n","\n","Epoch  1540000\n","\n","Epoch  1541000\n","\n","Epoch  1542000\n","\n","Epoch  1543000\n","\n","Epoch  1544000\n","\n","Epoch  1545000\n","\n","Epoch  1546000\n","\n","Epoch  1547000\n","\n","Epoch  1548000\n","\n","Epoch  1549000\n","\n","Epoch  1550000\n","\n","Epoch  1551000\n","\n","Epoch  1552000\n","\n","Epoch  1553000\n","\n","Epoch  1554000\n","\n","Epoch  1555000\n","\n","Epoch  1556000\n","\n","Epoch  1557000\n","\n","Epoch  1558000\n","\n","Epoch  1559000\n","\n","Epoch  1560000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1560000\n","beta1: (goal 3.532) tensor([3.4333], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6985], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6885], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1561000\n","\n","Epoch  1562000\n","\n","Epoch  1563000\n","\n","Epoch  1564000\n","\n","Epoch  1565000\n","\n","Epoch  1566000\n","\n","Epoch  1567000\n","\n","Epoch  1568000\n","\n","Epoch  1569000\n","\n","Epoch  1570000\n","\n","Epoch  1571000\n","\n","Epoch  1572000\n","\n","Epoch  1573000\n","\n","Epoch  1574000\n","\n","Epoch  1575000\n","\n","Epoch  1576000\n","\n","Epoch  1577000\n","\n","Epoch  1578000\n","\n","Epoch  1579000\n","\n","Epoch  1580000\n","\n","Epoch  1581000\n","\n","Epoch  1582000\n","\n","Epoch  1583000\n","\n","Epoch  1584000\n","\n","Epoch  1585000\n","\n","Epoch  1586000\n","\n","Epoch  1587000\n","\n","Epoch  1588000\n","\n","Epoch  1589000\n","\n","Epoch  1590000\n","\n","Epoch  1591000\n","\n","Epoch  1592000\n","\n","Epoch  1593000\n","\n","Epoch  1594000\n","\n","Epoch  1595000\n","\n","Epoch  1596000\n","\n","Epoch  1597000\n","\n","Epoch  1598000\n","\n","Epoch  1599000\n","\n","Epoch  1600000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1600000\n","beta1: (goal 3.532) tensor([3.4320], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6986], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6886], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1003], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1601000\n","\n","Epoch  1602000\n","\n","Epoch  1603000\n","\n","Epoch  1604000\n","\n","Epoch  1605000\n","\n","Epoch  1606000\n","\n","Epoch  1607000\n","\n","Epoch  1608000\n","\n","Epoch  1609000\n","\n","Epoch  1610000\n","\n","Epoch  1611000\n","\n","Epoch  1612000\n","\n","Epoch  1613000\n","\n","Epoch  1614000\n","\n","Epoch  1615000\n","\n","Epoch  1616000\n","\n","Epoch  1617000\n","\n","Epoch  1618000\n","\n","Epoch  1619000\n","\n","Epoch  1620000\n","\n","Epoch  1621000\n","\n","Epoch  1622000\n","\n","Epoch  1623000\n","\n","Epoch  1624000\n","\n","Epoch  1625000\n","\n","Epoch  1626000\n","\n","Epoch  1627000\n","\n","Epoch  1628000\n","\n","Epoch  1629000\n","\n","Epoch  1630000\n","\n","Epoch  1631000\n","\n","Epoch  1632000\n","\n","Epoch  1633000\n","\n","Epoch  1634000\n","\n","Epoch  1635000\n","\n","Epoch  1636000\n","\n","Epoch  1637000\n","\n","Epoch  1638000\n","\n","Epoch  1639000\n","\n","Epoch  1640000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1640000\n","beta1: (goal 3.532) tensor([3.4307], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6986], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6886], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1641000\n","\n","Epoch  1642000\n","\n","Epoch  1643000\n","\n","Epoch  1644000\n","\n","Epoch  1645000\n","\n","Epoch  1646000\n","\n","Epoch  1647000\n","\n","Epoch  1648000\n","\n","Epoch  1649000\n","\n","Epoch  1650000\n","\n","Epoch  1651000\n","\n","Epoch  1652000\n","\n","Epoch  1653000\n","\n","Epoch  1654000\n","\n","Epoch  1655000\n","\n","Epoch  1656000\n","\n","Epoch  1657000\n","\n","Epoch  1658000\n","\n","Epoch  1659000\n","\n","Epoch  1660000\n","\n","Epoch  1661000\n","\n","Epoch  1662000\n","\n","Epoch  1663000\n","\n","Epoch  1664000\n","\n","Epoch  1665000\n","\n","Epoch  1666000\n","\n","Epoch  1667000\n","\n","Epoch  1668000\n","\n","Epoch  1669000\n","\n","Epoch  1670000\n","\n","Epoch  1671000\n","\n","Epoch  1672000\n","\n","Epoch  1673000\n","\n","Epoch  1674000\n","\n","Epoch  1675000\n","\n","Epoch  1676000\n","\n","Epoch  1677000\n","\n","Epoch  1678000\n","\n","Epoch  1679000\n","\n","Epoch  1680000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1680000\n","beta1: (goal 3.532) tensor([3.4295], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6986], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6886], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2954], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1681000\n","\n","Epoch  1682000\n","\n","Epoch  1683000\n","\n","Epoch  1684000\n","\n","Epoch  1685000\n","\n","Epoch  1686000\n","\n","Epoch  1687000\n","\n","Epoch  1688000\n","\n","Epoch  1689000\n","\n","Epoch  1690000\n","\n","Epoch  1691000\n","\n","Epoch  1692000\n","\n","Epoch  1693000\n","\n","Epoch  1694000\n","\n","Epoch  1695000\n","\n","Epoch  1696000\n","\n","Epoch  1697000\n","\n","Epoch  1698000\n","\n","Epoch  1699000\n","\n","Epoch  1700000\n","\n","Epoch  1701000\n","\n","Epoch  1702000\n","\n","Epoch  1703000\n","\n","Epoch  1704000\n","\n","Epoch  1705000\n","\n","Epoch  1706000\n","\n","Epoch  1707000\n","\n","Epoch  1708000\n","\n","Epoch  1709000\n","\n","Epoch  1710000\n","\n","Epoch  1711000\n","\n","Epoch  1712000\n","\n","Epoch  1713000\n","\n","Epoch  1714000\n","\n","Epoch  1715000\n","\n","Epoch  1716000\n","\n","Epoch  1717000\n","\n","Epoch  1718000\n","\n","Epoch  1719000\n","\n","Epoch  1720000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1720000\n","beta1: (goal 3.532) tensor([3.4283], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6986], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1721000\n","\n","Epoch  1722000\n","\n","Epoch  1723000\n","\n","Epoch  1724000\n","\n","Epoch  1725000\n","\n","Epoch  1726000\n","\n","Epoch  1727000\n","\n","Epoch  1728000\n","\n","Epoch  1729000\n","\n","Epoch  1730000\n","\n","Epoch  1731000\n","\n","Epoch  1732000\n","\n","Epoch  1733000\n","\n","Epoch  1734000\n","\n","Epoch  1735000\n","\n","Epoch  1736000\n","\n","Epoch  1737000\n","\n","Epoch  1738000\n","\n","Epoch  1739000\n","\n","Epoch  1740000\n","\n","Epoch  1741000\n","\n","Epoch  1742000\n","\n","Epoch  1743000\n","\n","Epoch  1744000\n","\n","Epoch  1745000\n","\n","Epoch  1746000\n","\n","Epoch  1747000\n","\n","Epoch  1748000\n","\n","Epoch  1749000\n","\n","Epoch  1750000\n","\n","Epoch  1751000\n","\n","Epoch  1752000\n","\n","Epoch  1753000\n","\n","Epoch  1754000\n","\n","Epoch  1755000\n","\n","Epoch  1756000\n","\n","Epoch  1757000\n","\n","Epoch  1758000\n","\n","Epoch  1759000\n","\n","Epoch  1760000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1760000\n","beta1: (goal 3.532) tensor([3.4267], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6987], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1761000\n","\n","Epoch  1762000\n","\n","Epoch  1763000\n","\n","Epoch  1764000\n","\n","Epoch  1765000\n","\n","Epoch  1766000\n","\n","Epoch  1767000\n","\n","Epoch  1768000\n","\n","Epoch  1769000\n","\n","Epoch  1770000\n","\n","Epoch  1771000\n","\n","Epoch  1772000\n","\n","Epoch  1773000\n","\n","Epoch  1774000\n","\n","Epoch  1775000\n","\n","Epoch  1776000\n","\n","Epoch  1777000\n","\n","Epoch  1778000\n","\n","Epoch  1779000\n","\n","Epoch  1780000\n","\n","Epoch  1781000\n","\n","Epoch  1782000\n","\n","Epoch  1783000\n","\n","Epoch  1784000\n","\n","Epoch  1785000\n","\n","Epoch  1786000\n","\n","Epoch  1787000\n","\n","Epoch  1788000\n","\n","Epoch  1789000\n","\n","Epoch  1790000\n","\n","Epoch  1791000\n","\n","Epoch  1792000\n","\n","Epoch  1793000\n","\n","Epoch  1794000\n","\n","Epoch  1795000\n","\n","Epoch  1796000\n","\n","Epoch  1797000\n","\n","Epoch  1798000\n","\n","Epoch  1799000\n","\n","Epoch  1800000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1800000\n","beta1: (goal 3.532) tensor([3.4250], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6987], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1801000\n","\n","Epoch  1802000\n","\n","Epoch  1803000\n","\n","Epoch  1804000\n","\n","Epoch  1805000\n","\n","Epoch  1806000\n","\n","Epoch  1807000\n","\n","Epoch  1808000\n","\n","Epoch  1809000\n","\n","Epoch  1810000\n","\n","Epoch  1811000\n","\n","Epoch  1812000\n","\n","Epoch  1813000\n","\n","Epoch  1814000\n","\n","Epoch  1815000\n","\n","Epoch  1816000\n","\n","Epoch  1817000\n","\n","Epoch  1818000\n","\n","Epoch  1819000\n","\n","Epoch  1820000\n","\n","Epoch  1821000\n","\n","Epoch  1822000\n","\n","Epoch  1823000\n","\n","Epoch  1824000\n","\n","Epoch  1825000\n","\n","Epoch  1826000\n","\n","Epoch  1827000\n","\n","Epoch  1828000\n","\n","Epoch  1829000\n","\n","Epoch  1830000\n","\n","Epoch  1831000\n","\n","Epoch  1832000\n","\n","Epoch  1833000\n","\n","Epoch  1834000\n","\n","Epoch  1835000\n","\n","Epoch  1836000\n","\n","Epoch  1837000\n","\n","Epoch  1838000\n","\n","Epoch  1839000\n","\n","Epoch  1840000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1840000\n","beta1: (goal 3.532) tensor([3.4268], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6987], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1841000\n","\n","Epoch  1842000\n","\n","Epoch  1843000\n","\n","Epoch  1844000\n","\n","Epoch  1845000\n","\n","Epoch  1846000\n","\n","Epoch  1847000\n","\n","Epoch  1848000\n","\n","Epoch  1849000\n","\n","Epoch  1850000\n","\n","Epoch  1851000\n","\n","Epoch  1852000\n","\n","Epoch  1853000\n","\n","Epoch  1854000\n","\n","Epoch  1855000\n","\n","Epoch  1856000\n","\n","Epoch  1857000\n","\n","Epoch  1858000\n","\n","Epoch  1859000\n","\n","Epoch  1860000\n","\n","Epoch  1861000\n","\n","Epoch  1862000\n","\n","Epoch  1863000\n","\n","Epoch  1864000\n","\n","Epoch  1865000\n","\n","Epoch  1866000\n","\n","Epoch  1867000\n","\n","Epoch  1868000\n","\n","Epoch  1869000\n","\n","Epoch  1870000\n","\n","Epoch  1871000\n","\n","Epoch  1872000\n","\n","Epoch  1873000\n","\n","Epoch  1874000\n","\n","Epoch  1875000\n","\n","Epoch  1876000\n","\n","Epoch  1877000\n","\n","Epoch  1878000\n","\n","Epoch  1879000\n","\n","Epoch  1880000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1880000\n","beta1: (goal 3.532) tensor([3.4270], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6987], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1881000\n","\n","Epoch  1882000\n","\n","Epoch  1883000\n","\n","Epoch  1884000\n","\n","Epoch  1885000\n","\n","Epoch  1886000\n","\n","Epoch  1887000\n","\n","Epoch  1888000\n","\n","Epoch  1889000\n","\n","Epoch  1890000\n","\n","Epoch  1891000\n","\n","Epoch  1892000\n","\n","Epoch  1893000\n","\n","Epoch  1894000\n","\n","Epoch  1895000\n","\n","Epoch  1896000\n","\n","Epoch  1897000\n","\n","Epoch  1898000\n","\n","Epoch  1899000\n","\n","Epoch  1900000\n","\n","Epoch  1901000\n","\n","Epoch  1902000\n","\n","Epoch  1903000\n","\n","Epoch  1904000\n","\n","Epoch  1905000\n","\n","Epoch  1906000\n","\n","Epoch  1907000\n","\n","Epoch  1908000\n","\n","Epoch  1909000\n","\n","Epoch  1910000\n","\n","Epoch  1911000\n","\n","Epoch  1912000\n","\n","Epoch  1913000\n","\n","Epoch  1914000\n","\n","Epoch  1915000\n","\n","Epoch  1916000\n","\n","Epoch  1917000\n","\n","Epoch  1918000\n","\n","Epoch  1919000\n","\n","Epoch  1920000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1920000\n","beta1: (goal 3.532) tensor([3.4264], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6987], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1921000\n","\n","Epoch  1922000\n","\n","Epoch  1923000\n","\n","Epoch  1924000\n","\n","Epoch  1925000\n","\n","Epoch  1926000\n","\n","Epoch  1927000\n","\n","Epoch  1928000\n","\n","Epoch  1929000\n","\n","Epoch  1930000\n","\n","Epoch  1931000\n","\n","Epoch  1932000\n","\n","Epoch  1933000\n","\n","Epoch  1934000\n","\n","Epoch  1935000\n","\n","Epoch  1936000\n","\n","Epoch  1937000\n","\n","Epoch  1938000\n","\n","Epoch  1939000\n","\n","Epoch  1940000\n","\n","Epoch  1941000\n","\n","Epoch  1942000\n","\n","Epoch  1943000\n","\n","Epoch  1944000\n","\n","Epoch  1945000\n","\n","Epoch  1946000\n","\n","Epoch  1947000\n","\n","Epoch  1948000\n","\n","Epoch  1949000\n","\n","Epoch  1950000\n","\n","Epoch  1951000\n","\n","Epoch  1952000\n","\n","Epoch  1953000\n","\n","Epoch  1954000\n","\n","Epoch  1955000\n","\n","Epoch  1956000\n","\n","Epoch  1957000\n","\n","Epoch  1958000\n","\n","Epoch  1959000\n","\n","Epoch  1960000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  1960000\n","beta1: (goal 3.532) tensor([3.4253], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6887], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  1961000\n","\n","Epoch  1962000\n","\n","Epoch  1963000\n","\n","Epoch  1964000\n","\n","Epoch  1965000\n","\n","Epoch  1966000\n","\n","Epoch  1967000\n","\n","Epoch  1968000\n","\n","Epoch  1969000\n","\n","Epoch  1970000\n","\n","Epoch  1971000\n","\n","Epoch  1972000\n","\n","Epoch  1973000\n","\n","Epoch  1974000\n","\n","Epoch  1975000\n","\n","Epoch  1976000\n","\n","Epoch  1977000\n","\n","Epoch  1978000\n","\n","Epoch  1979000\n","\n","Epoch  1980000\n","\n","Epoch  1981000\n","\n","Epoch  1982000\n","\n","Epoch  1983000\n","\n","Epoch  1984000\n","\n","Epoch  1985000\n","\n","Epoch  1986000\n","\n","Epoch  1987000\n","\n","Epoch  1988000\n","\n","Epoch  1989000\n","\n","Epoch  1990000\n","\n","Epoch  1991000\n","\n","Epoch  1992000\n","\n","Epoch  1993000\n","\n","Epoch  1994000\n","\n","Epoch  1995000\n","\n","Epoch  1996000\n","\n","Epoch  1997000\n","\n","Epoch  1998000\n","\n","Epoch  1999000\n","\n","Epoch  2000000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  2000000\n","beta1: (goal 3.532) tensor([3.4239], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6888], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  2001000\n","\n","Epoch  2002000\n","\n","Epoch  2003000\n","\n","Epoch  2004000\n","\n","Epoch  2005000\n","\n","Epoch  2006000\n","\n","Epoch  2007000\n","\n","Epoch  2008000\n","\n","Epoch  2009000\n","\n","Epoch  2010000\n","\n","Epoch  2011000\n","\n","Epoch  2012000\n","\n","Epoch  2013000\n","\n","Epoch  2014000\n","\n","Epoch  2015000\n","\n","Epoch  2016000\n","\n","Epoch  2017000\n","\n","Epoch  2018000\n","\n","Epoch  2019000\n","\n","Epoch  2020000\n","\n","Epoch  2021000\n","\n","Epoch  2022000\n","\n","Epoch  2023000\n","\n","Epoch  2024000\n","\n","Epoch  2025000\n","\n","Epoch  2026000\n","\n","Epoch  2027000\n","\n","Epoch  2028000\n","\n","Epoch  2029000\n","\n","Epoch  2030000\n","\n","Epoch  2031000\n","\n","Epoch  2032000\n","\n","Epoch  2033000\n","\n","Epoch  2034000\n","\n","Epoch  2035000\n","\n","Epoch  2036000\n","\n","Epoch  2037000\n","\n","Epoch  2038000\n","\n","Epoch  2039000\n","\n","Epoch  2040000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  2040000\n","beta1: (goal 3.532) tensor([3.4224], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6888], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  2041000\n","\n","Epoch  2042000\n","\n","Epoch  2043000\n","\n","Epoch  2044000\n","\n","Epoch  2045000\n","\n","Epoch  2046000\n","\n","Epoch  2047000\n","\n","Epoch  2048000\n","\n","Epoch  2049000\n","\n","Epoch  2050000\n","\n","Epoch  2051000\n","\n","Epoch  2052000\n","\n","Epoch  2053000\n","\n","Epoch  2054000\n","\n","Epoch  2055000\n","\n","Epoch  2056000\n","\n","Epoch  2057000\n","\n","Epoch  2058000\n","\n","Epoch  2059000\n","\n","Epoch  2060000\n","\n","Epoch  2061000\n","\n","Epoch  2062000\n","\n","Epoch  2063000\n","\n","Epoch  2064000\n","\n","Epoch  2065000\n","\n","Epoch  2066000\n","\n","Epoch  2067000\n","\n","Epoch  2068000\n","\n","Epoch  2069000\n","\n","Epoch  2070000\n","\n","Epoch  2071000\n","\n","Epoch  2072000\n","\n","Epoch  2073000\n","\n","Epoch  2074000\n","\n","Epoch  2075000\n","\n","Epoch  2076000\n","\n","Epoch  2077000\n","\n","Epoch  2078000\n","\n","Epoch  2079000\n","\n","Epoch  2080000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  2080000\n","beta1: (goal 3.532) tensor([3.4200], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6888], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  2081000\n","\n","Epoch  2082000\n","\n","Epoch  2083000\n","\n","Epoch  2084000\n","\n","Epoch  2085000\n","\n","Epoch  2086000\n","\n","Epoch  2087000\n","\n","Epoch  2088000\n","\n","Epoch  2089000\n","\n","Epoch  2090000\n","\n","Epoch  2091000\n","\n","Epoch  2092000\n","\n","Epoch  2093000\n","\n","Epoch  2094000\n","\n","Epoch  2095000\n","\n","Epoch  2096000\n","\n","Epoch  2097000\n","\n","Epoch  2098000\n","\n","Epoch  2099000\n","\n","Epoch  2100000\n","\n","Epoch  2101000\n","\n","Epoch  2102000\n","\n","Epoch  2103000\n","\n","Epoch  2104000\n","\n","Epoch  2105000\n","\n","Epoch  2106000\n","\n","Epoch  2107000\n","\n","Epoch  2108000\n","\n","Epoch  2109000\n","\n","Epoch  2110000\n","\n","Epoch  2111000\n","\n","Epoch  2112000\n","\n","Epoch  2113000\n","\n","Epoch  2114000\n","\n","Epoch  2115000\n","\n","Epoch  2116000\n","\n","Epoch  2117000\n","\n","Epoch  2118000\n","\n","Epoch  2119000\n","\n","Epoch  2120000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  2120000\n","beta1: (goal 3.532) tensor([3.4175], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6891], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  2121000\n","\n","Epoch  2122000\n","\n","Epoch  2123000\n","\n","Epoch  2124000\n","\n","Epoch  2125000\n","\n","Epoch  2126000\n","\n","Epoch  2127000\n","\n","Epoch  2128000\n","\n","Epoch  2129000\n","\n","Epoch  2130000\n","\n","Epoch  2131000\n","\n","Epoch  2132000\n","\n","Epoch  2133000\n","\n","Epoch  2134000\n","\n","Epoch  2135000\n","\n","Epoch  2136000\n","\n","Epoch  2137000\n","\n","Epoch  2138000\n","\n","Epoch  2139000\n","\n","Epoch  2140000\n","\n","Epoch  2141000\n","\n","Epoch  2142000\n","\n","Epoch  2143000\n","\n","Epoch  2144000\n","\n","Epoch  2145000\n","\n","Epoch  2146000\n","\n","Epoch  2147000\n","\n","Epoch  2148000\n","\n","Epoch  2149000\n","\n","Epoch  2150000\n","\n","Epoch  2151000\n","\n","Epoch  2152000\n","\n","Epoch  2153000\n","\n","Epoch  2154000\n","\n","Epoch  2155000\n","\n","Epoch  2156000\n","\n","Epoch  2157000\n","\n","Epoch  2158000\n","\n","Epoch  2159000\n","\n","Epoch  2160000\n","\n","Saving model... Loss is:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  2160000\n","beta1: (goal 3.532) tensor([3.4150], grad_fn=<AddBackward0>)\n","\n","beta_h: (goal 0.012) tensor([0.0199], grad_fn=<MulBackward0>)\n","\n","beta_f: (goal 0.462):  tensor([0.6988], grad_fn=<AddBackward0>)\n","\n","theta1: (goal 0.65):  tensor([0.6892], grad_fn=<TanhBackward>)\n","\n","delta2 (goal 0.42):  tensor([0.2953], grad_fn=<AddBackward0>)\n","\n","gamma_f (goal 0.5):  tensor([0.1002], grad_fn=<AddBackward0>)\n","#################################\n","\n","Epoch  2161000\n","\n","Epoch  2162000\n","\n","Epoch  2163000\n","\n","Epoch  2164000\n","\n","Epoch  2165000\n","\n","Epoch  2166000\n","\n","Epoch  2167000\n","\n","Epoch  2168000\n","\n","Epoch  2169000\n","\n","Epoch  2170000\n","\n","Epoch  2171000\n","\n","Epoch  2172000\n","\n","Epoch  2173000\n","\n","Epoch  2174000\n","\n","Epoch  2175000\n","\n","Epoch  2176000\n","\n","Epoch  2177000\n","\n","Epoch  2178000\n","\n","Epoch  2179000\n","\n","Epoch  2180000\n","\n","Epoch  2181000\n","\n","Epoch  2182000\n","\n","Epoch  2183000\n","\n","Epoch  2184000\n","\n","Epoch  2185000\n","\n","Epoch  2186000\n","\n","Epoch  2187000\n","\n","Epoch  2188000\n","\n","Epoch  2189000\n","\n","Epoch  2190000\n","\n","Epoch  2191000\n","\n","Epoch  2192000\n","\n","Epoch  2193000\n","\n","Epoch  2194000\n","\n","Epoch  2195000\n","\n","Epoch  2196000\n","\n","Epoch  2197000\n","\n","Epoch  2198000\n","\n","Epoch  2199000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RY71fo4_Ic_N"},"source":["plt.plot(dinn.losses[5000000:], color = 'teal')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJrvoRWQZqOd"},"source":["fig = plt.figure(facecolor='w')\n","ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n","\n","ax.plot(ebola_data[0], ebola_data[1], 'pink', alpha=0.5, lw=2, label='S')\n","ax.plot(ebola_data[0], S_pred_list[0].detach().numpy(), 'navy', alpha=0.9, lw=2, label='S Prediction', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[2], 'violet', alpha=0.5, lw=2, label='E')\n","ax.plot(ebola_data[0], E_pred_list[0].detach().numpy(), 'dodgerblue', alpha=0.9, lw=2, label='E Prediction', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[3], 'darkgreen', alpha=0.5, lw=2, label='I')\n","ax.plot(ebola_data[0], I_pred_list[0].detach().numpy(), 'gold', alpha=0.9, lw=2, label='I Prediction', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[4], 'red', alpha=0.5, lw=2, label='H')\n","ax.plot(ebola_data[0], H_pred_list[0].detach().numpy(), 'salmon', alpha=0.9, lw=2, label='H Prediction', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[5], 'blue', alpha=0.5, lw=2, label='F')\n","ax.plot(ebola_data[0], F_pred_list[0].detach().numpy(), 'wheat', alpha=0.9, lw=2, label='F Prediction', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[6], 'purple', alpha=0.5, lw=2, label='R')\n","ax.plot(ebola_data[0], R_pred_list[0].detach().numpy(), 'teal', alpha=0.9, lw=2, label='R Prediction', linestyle='dashed')\n","\n","\n","ax.set_xlabel('Time /days')\n","ax.set_ylabel('Number')\n","ax.yaxis.set_tick_params(length=0)\n","ax.xaxis.set_tick_params(length=0)\n","ax.grid(b=True, which='major', c='w', lw=2, ls='-')\n","legend = ax.legend()\n","legend.get_frame().set_alpha(0.5)\n","for spine in ('top', 'right', 'bottom', 'left'):\n","    ax.spines[spine].set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgcowlQFZqOe"},"source":["# print('alpha1: (goal 1)', round(dinn.alpha1.item(),2))\n","# print('\\nalpha2: (goal 0)', round(dinn.alpha2.item(),2))\n","# print('\\nbeta: (goal 0.0075): ', round(dinn.beta.item(),4))\n","# print('\\nmu (goal 5): ', round(dinn.mu.item(),2))\n","# print('\\nu: (goal 0.515151515): ', round(dinn.u.item(),2))\n","# print('\\ntao (goal 0.58): ', round(dinn.tao.item(),2))\n","\n","\n","# print('\\nerror:')\n","# print('alpha1: ', round((1-round(dinn.alpha1.item(),2))/1,2)*100,'%')\n","# print('alpha2: ', round((0-round(dinn.alpha2.item(),2))/1e-20,2)*100,'%')\n","# print('beta: ', round((0.0075-round(dinn.beta.item(),4))/0.0075,2)*100,'%')\n","# print('mu: ', round((5-round(dinn.mu.item(),2))/5,2)*100,'%')\n","# print('u: ', round((0.515151515-round(dinn.u.item(),2))/0.515151515,2)*100,'%')\n","# print('tao: ', round((0.58-round(dinn.tao.item(),2))/0.58,2)*100,'%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUzZI6VMZqOe"},"source":["import numpy as np\n","from scipy.integrate import odeint\n","import matplotlib.pyplot as plt\n","\n","# Initial conditions\n","E0 = 0\n","I0 = 425\n","H0 = 0\n","F0 = 0\n","R0 = 0\n","N = 470000\n","S0 = N - I0\n","\n","# A grid of time points (in days)\n","t = np.linspace(0, 100, 100) \n","\n","#parameters\n","beta1 = dinn.beta1\n","beta_h = dinn.beta_h\n","beta_f = dinn.beta_f\n","alpha = dinn.alpha\n","gamma_h = dinn.gamma_h\n","theta1 = dinn.theta1\n","gamma_i = dinn.gamma_i\n","delta1 = dinn.delta1\n","gamma_d = dinn.gamma_d\n","delta2 = dinn.delta2\n","gamma_f = dinn.gamma_f\n","gamma_ih  = dinn.gamma_ih\n","gamma_dh = dinn.gamma_dh\n","\n","# The SIR model differential equations.\n","def deriv(y, t, N, beta1, beta_h, beta_f, alpha, gamma_h, theta1, gamma_i, delta1, gamma_d, delta2, gamma_f, gamma_ih , gamma_dh):\n","    S, E, I, H, F, R  = y\n","    dSdt = -1/N * (beta1 * S * I + beta_h * S * H + beta_f * S * F)\n","    dEdt = 1/N * (beta1 * S * I + beta_h * S * H + beta_f * S * F) - alpha * E\n","    dIdt = alpha * E - (gamma_h * theta1 + gamma_i * (1-theta1)*(1-delta1) + gamma_d * (1-theta1) * delta1) * I\n","    dHdt = gamma_h * theta1 * I - (gamma_dh * delta2 + gamma_ih * (1-delta2)) * H\n","    dFdt = gamma_d * (1-theta1) * delta1 * I + gamma_dh * delta2 * H - gamma_f * F\n","    dRdt = gamma_i * (1-theta1) * (1-delta1) * I + gamma_ih * (1-delta2) * H + gamma_f * F\n","\n","    return dSdt, dEdt, dIdt, dHdt, dFdt, dRdt\n","\n","\n","# Initial conditions vector\n","y0 = S0, E0, I0, H0, F0, R0\n","# Integrate the SIR equations over the time grid, t.\n","ret = odeint(deriv, y0, t, args=(N, beta1, beta_h, beta_f, alpha, gamma_h, theta1, gamma_i, delta1, gamma_d, delta2, gamma_f, gamma_ih , gamma_dh))\n","S, E, I, H, F, R = ret.T\n","\n","# Plot the data on two separate curves for S(t), I(t)\n","fig = plt.figure(facecolor='w')\n","ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n","\n","ax.plot(ebola_data[0], ebola_data[1], 'violet', alpha=0.5, lw=2, label='S')\n","ax.plot(t, S, 'dodgerblue', alpha=0.5, lw=2, label='S', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[3], 'darkgreen', alpha=0.5, lw=2, label='Diagnosed')\n","ax.plot(t, D, 'gold', alpha=0.5, lw=2, label='Diagnosed', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[4], 'red', alpha=0.5, lw=2, label='Ailling')\n","ax.plot(t, A, 'salmon', alpha=0.5, lw=2, label='Ailling', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[5], 'blue', alpha=0.5, lw=2, label='Recognized')\n","ax.plot(t, R, 'wheat', alpha=0.5, lw=2, label='Recognized', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[6], 'purple', alpha=0.5, lw=2, label='Threatened')\n","ax.plot(t, T, 'teal', alpha=0.5, lw=2, label='Threatened', linestyle='dashed')\n","\n","ax.plot(ebola_data[0], ebola_data[7], 'yellow', alpha=0.5, lw=2, label='Healed')\n","ax.plot(t, H, 'slategrey', alpha=0.5, lw=2, label='Healed', linestyle='dashed')\n","\n","ax.set_xlabel('Time /days')\n","ax.yaxis.set_tick_params(length=0)\n","ax.xaxis.set_tick_params(length=0)\n","ax.grid(b=True, which='major', c='w', lw=2, ls='-')\n","legend = ax.legend()\n","legend.get_frame().set_alpha(0.5)\n","for spine in ('top', 'right', 'bottom', 'left'):\n","    ax.spines[spine].set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICWNogFjn27j"},"source":["#calculate relative MSE loss\n","import math\n","\n","T_total_loss = 0\n","T_den = 0\n","I_total_loss = 0\n","I_den = 0\n","V_total_loss = 0\n","V_den = 0\n","\n","for timestep in range(len(t)):\n","  T_value = hiv_data[1][timestep] - T[timestep]\n","  T_total_loss += T_value**2\n","  T_den += (hiv_data[1][timestep])**2\n","  I_value = hiv_data[2][timestep] - I[timestep]\n","  I_total_loss += I_value**2\n","  I_den += (hiv_data[2][timestep])**2\n","  V_value = hiv_data[3][timestep] - V[timestep]\n","  V_total_loss += V_value**2\n","  V_den += (hiv_data[3][timestep])**2\n","\n","T_total_loss = math.sqrt(T_total_loss/T_den)\n","I_total_loss = math.sqrt(I_total_loss/I_den)\n","V_total_loss = math.sqrt(V_total_loss/V_den)\n","\n","print('T_total_loss: ', T_total_loss)\n","print('I_total_loss: ', I_total_loss)\n","print('V_total_loss: ', V_total_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfH8jZ8u93OF"},"source":[""],"execution_count":null,"outputs":[]}]}