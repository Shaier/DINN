{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSI_data = genfromtxt('tSI_data.csv', delimiter=',') #in the form of [t, S, I]\n",
    "tao_data = genfromtxt('tao_data.csv', delimiter=',') #in the form of [t, tao_star, T_star, us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nstarting training...\n\nself.alpha1[0] before training tensor(0.7381, grad_fn=<SelectBackward>)\nepoch 0\nloss tensor(926467.7956, dtype=torch.float64, grad_fn=<AddBackward0>)\n3\n\nepoch:  0\nloss:  tensor(926467.7956, dtype=torch.float64, grad_fn=<AddBackward0>)\n\nself.alpha1 after the training!!!:  tensor(0.7381, grad_fn=<SelectBackward>)\n\nepoch 1\nloss tensor(926468.4366, dtype=torch.float64, grad_fn=<AddBackward0>)\n3\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a3e486639376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mdinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#t, S_data, I_data, tao_data, T_star_data, u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mdinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-a3e486639376>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "class DINN:\n",
    "    def __init__(self, t, S_data, I_data, tao_data, T_star_data, u_data): #, t, S_data, I_data, tao_data, T_star_data, u\n",
    "        self.t = torch.tensor(t, requires_grad = True).float()\n",
    "        self.S = torch.tensor(S_data, requires_grad = True)\n",
    "        self.I = torch.tensor(I_data, requires_grad = True)\n",
    "        self.taos = torch.tensor(tao_data, requires_grad = True)\n",
    "        self.T_stars = torch.tensor(T_star_data, requires_grad = True)\n",
    "        self.u = torch.tensor(u_data, requires_grad = True).float()\n",
    "        self.t_u = torch.stack((self.t, self.u), dim=1) #stack for the NN input \n",
    "\n",
    "        #learnable parameters\n",
    "        self.alpha1=torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.alpha2=torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.mu=torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.beta=torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "\n",
    "        #NN + params init\n",
    "        self.nx = self.net_x()\n",
    "        self.params = list(self.nx.parameters())\n",
    "        self.params.extend(list([self.alpha1, self.alpha2, self.mu, self.beta]))\n",
    "        \n",
    "        #predictions\n",
    "        self.net_output = self.nx(self.t_u) #size = (100,4)\n",
    "        self.S_pred = self.net_output[:,0]\n",
    "        self.I_pred = self.net_output[:,1]\n",
    "        self.tao_pred = self.net_output[:,2]\n",
    "        self.T_star_pred = self.net_output[:,3]\n",
    "        \n",
    "    #networks\n",
    "    class net_x(nn.Module): # input = [t, u]\n",
    "        def __init__(self):\n",
    "            super(DINN.net_x, self).__init__()\n",
    "            self.fc1=nn.Linear(2, 20) #takes t, u\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 4) #outputs S, I, tao, T*\n",
    "\n",
    "        def forward(self, x):\n",
    "            x=self.fc1(x)\n",
    "            x=self.fc2(x)\n",
    "            x=self.out(x)\n",
    "            return x    \n",
    "    \n",
    "    \n",
    "    def net_f(self, t, u):\n",
    "        #print('net_f')\n",
    "        S_t_list = []\n",
    "        I_t_list = [] \n",
    "        for input_tensor in self.t_u:\n",
    "            output_tensor = self.nx(input_tensor)\n",
    "            S_t_value = grad(output_tensor[0], input_tensor, retain_graph=True)[0][0] #derivative of S wrt t\n",
    "            I_t_value = grad(output_tensor[1], input_tensor, retain_graph=True)[0][0] #derivative of I wrt t\n",
    "            S_t_list.append(S_t_value)\n",
    "            I_t_list.append(I_t_value)\n",
    "\n",
    "        #convert to tensors\n",
    "        S_t = torch.tensor(S_t_list, requires_grad = True)\n",
    "        I_t = torch.tensor(I_t_list, requires_grad = True)\n",
    "\n",
    "        f1 = S_t + self.beta * self.S_pred * self.I_pred + self.u * (t > self.tao_pred) * self.alpha1\n",
    "        f2 = I_t - self.beta * self.S_pred * self.I_pred + self.mu * self.I_pred + self.u * (t > self.tao_pred) * self.alpha2\n",
    "        return f1, f2\n",
    "    \n",
    "    #train    \n",
    "    def train(self, n_epochs):\n",
    "        print('\\nstarting training...\\n')\n",
    "        losses = []\n",
    "        print('self.alpha1[0] before training', self.alpha1[0])\n",
    "        learning_rate = 0.01\n",
    "        momentum = 0.5        \n",
    "        optimizer = optim.SGD(self.params, lr = learning_rate, momentum = momentum)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            print('epoch', epoch)\n",
    "            optimizer.zero_grad()\n",
    "            #print('1')\n",
    "            f1, f2 = self.net_f(self.t, self.u)\n",
    "            #print('2')\n",
    "            #print('f1, f2: ', f1, f2)\n",
    "            #S_pred, I_pred, tao_pred, T_star_pred = self.nx(self.t_u)\n",
    "            #print('S_pred, I_pred, tao_pred, T_star_pred: ', self.S_pred, self.I_pred, self.tao_pred, self.T_star_pred)    \n",
    "\n",
    "\n",
    "            \n",
    "            #print('self.S', type(self.S))\n",
    "            #print('self.S_pred', type(self.S_pred))\n",
    "            \n",
    "            loss = (torch.mean(torch.square(self.S - self.S_pred))+torch.mean(torch.square(self.I - self.I_pred)) + #S,I \n",
    "                torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)) \\\n",
    "                + #f1, f2\n",
    "                torch.mean(torch.square(self.taos - self.tao_pred)) + torch.mean(torch.square(self.T_stars - self.T_star_pred))) #tao, T_star\n",
    "            print('loss', loss)\n",
    "            print('3')\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.nx.parameters(), 100) #gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss)\n",
    "            if epoch % 1 == 0:\n",
    "                print('\\nepoch: ', epoch)\n",
    "                print('loss: ', losses[-1])\n",
    "                print('\\nself.alpha1 after the training!!!: ', self.alpha1[0])\n",
    "                print('')\n",
    "        plt.plot(losses, color = 'teal')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "dinn = DINN(tSI_data[0], tSI_data[1], tSI_data[2], tao_data[1], tao_data[2], tao_data[3]) #t, S_data, I_data, tao_data, T_star_data, u\n",
    "dinn.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class net_x(nn.Module): \n",
    "        def __init__(self):\n",
    "            super(net_x, self).__init__()\n",
    "            self.fc1=nn.Linear(2, 20) \n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 4) \n",
    "\n",
    "        def forward(self, x):\n",
    "            x=self.fc1(x)\n",
    "            x=F.relu(self.fc2(x))\n",
    "            #x=self.fc2(x)\n",
    "            x=self.out(x)\n",
    "            return x\n",
    "\n",
    "nx = net_x()\n",
    "#r = torch.tensor([1.0,2.0], requires_grad=True)\n",
    "#net_output = nx(r) #size 4\n",
    "#net_output\n",
    "#print(grad(net_output[0], r, retain_graph=True))#[0][0]) #[0] for S, [1] for I, wrt input r (i.e. t, u). So to get wrt \"t\" need this output[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((8,2), requires_grad=True)\n",
    "'''s = []\n",
    "s_t = []\n",
    "for input_tensor in a:\n",
    "    output_tensor = nx(input_tensor)\n",
    "    #print(output_tensor)\n",
    "    #print()\n",
    "    #print(output_tensor[1])\n",
    "    s.append(output_tensor[0])\n",
    "    s_t_value = grad(output_tensor[1], input_tensor)[0][0]\n",
    "    s_t.append(s_t_value)\n",
    "'''\n",
    "S_list = []\n",
    "I_list = []\n",
    "S_t_list = []\n",
    "I_t_list = [] \n",
    "for input_tensor in a:\n",
    "    output_tensor = nx(input_tensor)\n",
    "    S_list.append(output_tensor[0])\n",
    "    I_list.append(output_tensor[1])\n",
    "    S_t_value = grad(output_tensor[0], input_tensor, retain_graph=True)[0][0] #derivative of S \n",
    "    I_t_value = grad(output_tensor[1], input_tensor)[0][0] #derivative of I wrt t\n",
    "    S_t_list.append(S_t_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0263, -0.1806, -0.2035, -0.1546, -0.1431, -0.3125, -0.0680, -0.3217],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "S_t = torch.tensor(S_t_list, requires_grad = True)\n",
    "I_t = torch.tensor(I_t_list, requires_grad = True)\n",
    "S_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradients of output[0] wrt input[0]\n(tensor([[-0.0460,  0.1006]]),)\n\nGradients of output[1] wrt input[1]\n(tensor([[ 0.0281, -0.0312]]),)\n\nGradients of output[2] wrt input[2]\n(tensor([[0.0102, 0.0087]]),)\n\nGradients of output[3] wrt input[3]\n(tensor([[-0.0767,  0.0920]]),)\n\nGradients of output[4] wrt input[4]\n(tensor([[0.0862, 0.0088]]),)\n\nGradients of output[5] wrt input[5]\n(tensor([[-0.0602,  0.0580]]),)\n\nGradients of output[6] wrt input[6]\n(tensor([[0.0097, 0.0479]]),)\n\nGradients of output[7] wrt input[7]\n(tensor([[-0.0736,  0.0645]]),)\n\n"
     ]
    }
   ],
   "source": [
    "inputs = [torch.randn(1, 2, requires_grad=True) for i in range(8)]\n",
    "r = torch.cat(inputs) # shape : (8, 2)\n",
    "a = torch.rand((8,2), requires_grad=True)\n",
    "\n",
    "y = nx(r) # shape : (8, 4)\n",
    "for i in range(len(y)):\n",
    "    print(f\"Gradients of output[{i}] wrt input[{i}]\") \n",
    "    for output_vector in y[i]: \n",
    "        # prints a tensor of size (2)\n",
    "        print(grad(output_vector, inputs[i], retain_graph=True))\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}