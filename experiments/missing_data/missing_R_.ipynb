{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"missing_R_.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3.7.3 64-bit ('base': conda)","name":"python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"id":"JoPs1QTPZtrO","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1627620086824,"user_tz":360,"elapsed":15775,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"053546ca-d9d5-4b83-b603-04238b380ad5"},"source":["#Mount my drive- run the code, go to the link, accept.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#Change working directory to make it easier to access the files\n","import os\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/dinn/experiments/different_architectures\")\n","os.getcwd() "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/gdrive/My Drive/Colab Notebooks/dinn/experiments/different_architectures'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"pFhy95XbZqOS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627620091423,"user_tz":360,"elapsed":4602,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"094fe8b2-f058-4892-ee28-f0682e998eb6"},"source":["import torch\n","from torch.autograd import grad\n","import torch.nn as nn\n","from numpy import genfromtxt\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","\n","sidr_data = genfromtxt('covid_100_pts.csv', delimiter=',') #in the form of [t,S,I,D,R]\n","\n","torch.manual_seed(1234)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ff9b75cae90>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"AD6iFgYfZqOa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627620091997,"user_tz":360,"elapsed":437,"user":{"displayName":"Sagi Shaier","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXQKL7UiRoL28-GMShElFe0PuFh4NWnMP9hbDD=s64","userId":"12455150063240177220"}},"outputId":"b2edabd5-e10c-4310-fe6f-35c1a820afb2"},"source":["%%time\n","\n","PATH = 'missing_R_' \n","\n","class DINN(nn.Module):\n","    def __init__(self, t, S_data, I_data, D_data, R_data): #[t,S,I,D,R]\n","        super(DINN, self).__init__()\n","        self.N = 59e6 #population size\n","        self.t = torch.tensor(t, requires_grad=True)\n","        self.t_float = self.t.float()\n","        self.t_batch = torch.reshape(self.t_float, (len(self.t),1)) #reshape for batch \n","        self.S = torch.tensor(S_data)\n","        self.I = torch.tensor(I_data)\n","        self.D = torch.tensor(D_data)\n","        self.R = torch.tensor(R_data)\n","\n","        self.losses = []\n","        self.save = 3 #which file to save to\n","\n","        # self.alpha_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True)) #0.191\n","        # self.beta_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True)) #0.05\n","        # self.gamma_tilda = torch.nn.Parameter(torch.rand(1, requires_grad=True)) #0.0294\n","        self.alpha_tilda = torch.tensor(0.191)\n","        self.beta_tilda = torch.tensor(0.05)\n","        self.gamma_tilda = torch.tensor(0.0294)\n","        \n","        #find values for normalization\n","        self.S_max = max(self.S)\n","        self.I_max = max(self.I)\n","        self.D_max = max(self.D)\n","        self.R_max = max(self.R)\n","        self.S_min = min(self.S)\n","        self.I_min = min(self.I)\n","        self.D_min = min(self.D)\n","        self.R_min = min(self.R)\n","\n","        #unnormalize\n","        self.S_hat = (self.S - self.S_min) / (self.S_max - self.S_min)\n","        self.I_hat = (self.I - self.I_min) / (self.I_max - self.I_min)\n","        self.D_hat = (self.D - self.D_min) / (self.D_max - self.D_min)\n","        self.R_hat = (self.R - self.R_min) / (self.R_max - self.R_min)\n","\n","        #matrices (x4 for S,I,D,R) for the gradients\n","        self.m1 = torch.zeros((len(self.t), 4)); self.m1[:, 0] = 1\n","        self.m2 = torch.zeros((len(self.t), 4)); self.m2[:, 1] = 1\n","        self.m3 = torch.zeros((len(self.t), 4)); self.m3[:, 2] = 1\n","        self.m4 = torch.zeros((len(self.t), 4)); self.m4[:, 3] = 1\n","\n","        #NN\n","        self.net_sidr = self.Net_sidr()\n","        self.params = list(self.net_sidr.parameters())\n","        self.params.extend(list([self.alpha_tilda, self.beta_tilda, self.gamma_tilda]))\n","\n","    #force parameters to be in a range\n","    @property\n","    def alpha(self):\n","        return self.alpha_tilda #torch.tanh(self.alpha_tilda)*0.191*20\n","\n","    @property\n","    def beta(self):\n","        return self.beta_tilda #torch.tanh(self.beta_tilda)*0.05*20\n","    \n","    @property\n","    def gamma(self):\n","        return self.gamma_tilda #torch.tanh(self.gamma_tilda)*0.0294*20\n","\n","\n","    #nets\n","    class Net_sidr(nn.Module): # input = [t]\n","        def __init__(self):\n","            super(DINN.Net_sidr, self).__init__()\n","            self.fc1=nn.Linear(1, 20) #takes 100 t's\n","            self.fc2=nn.Linear(20, 20)\n","            self.fc3=nn.Linear(20, 20)\n","            self.fc4=nn.Linear(20, 20)\n","            # self.fc5=nn.Linear(20, 20)\n","            # self.fc6=nn.Linear(20, 20)\n","            # self.fc7=nn.Linear(20, 20)\n","            # self.fc8=nn.Linear(20, 20)\n","            self.out=nn.Linear(20, 4) #outputs S, I, D, R\n","\n","        def forward(self, t_batch):\n","            sidr=F.relu(self.fc1(t_batch))\n","            sidr=F.relu(self.fc2(sidr))\n","            sidr=F.relu(self.fc3(sidr))\n","            sidr=F.relu(self.fc4(sidr))\n","            # sidr=F.relu(self.fc5(sidr))\n","            # sidr=F.relu(self.fc6(sidr))\n","            # sidr=F.relu(self.fc7(sidr))\n","            # sidr=F.relu(self.fc8(sidr))\n","            sidr=self.out(sidr)\n","            return sidr\n","            \n","    def net_f(self, t_batch):\n","        sidr_hat = self.net_sidr(t_batch)\n","\n","        S_hat, I_hat, D_hat, R_hat = sidr_hat[:,0], sidr_hat[:,1], sidr_hat[:,2], sidr_hat[:,3]\n","\n","        #S_t\n","        sidr_hat.backward(self.m1, retain_graph=True)\n","        #S_hat_t = self.t.grad.detach().clone()\n","        S_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #I_t\n","        sidr_hat.backward(self.m2, retain_graph=True)\n","        # I_hat_t = self.t.grad.detach().clone()\n","        I_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #D_t\n","        sidr_hat.backward(self.m3, retain_graph=True)\n","        # D_hat_t = self.t.grad.detach().clone()\n","        D_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #R_t\n","        sidr_hat.backward(self.m4, retain_graph=True)\n","        # R_hat_t = self.t.grad.detach().clone()\n","        R_hat_t = self.t.grad.clone()\n","        self.t.grad.zero_()\n","\n","        #unnormalize\n","        S = self.S_min + (self.S_max - self.S_min) * S_hat\n","        I = self.I_min + (self.I_max - self.I_min) * I_hat\n","        D = self.D_min + (self.D_max - self.D_min) * D_hat      \n","        R = self.R_min + (self.R_max - self.R_min) * R_hat\n","\n","        f1_hat = S_hat_t - (-(self.alpha / self.N) * S * I)  / (self.S_max - self.S_min)\n","        f2_hat = I_hat_t - ((self.alpha / self.N) * S * I - self.beta * I - self.gamma * I ) / (self.I_max - self.I_min)\n","        f3_hat = D_hat_t - (self.gamma * I) / (self.D_max - self.D_min)\n","        f4_hat = R_hat_t - (self.beta * I ) / (self.R_max - self.R_min)\n","\n","\n","        return f1_hat, f2_hat, f3_hat, f4_hat, S_hat, I_hat, D_hat, R_hat\n","    \n","    def load(self):\n","      # Load checkpoint\n","      try:\n","        checkpoint = torch.load(PATH + str(self.save)+'.pt') \n","        print('\\nloading pre-trained model...')\n","        self.load_state_dict(checkpoint['model'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler'])\n","        epoch = checkpoint['epoch']\n","        #loss = checkpoint['loss']\n","        self.losses = checkpoint['losses']\n","        #print('loaded previous loss: ', loss)\n","        # self.optimizer.param_groups[0]['lr'] = 1e-9\n","\n","      except RuntimeError :\n","          print('changed the architecture, ignore')\n","          pass\n","      except FileNotFoundError:\n","          pass\n","\n","    def train(self, n_epochs):\n","      #try loading\n","      self.load()\n","\n","      #train\n","      print('\\nstarting training...\\n')\n","      \n","      for epoch in range(n_epochs):\n","        #lists to hold the output (maintain only the final epoch)\n","        S_pred_list = []\n","        I_pred_list = []\n","        D_pred_list = []\n","        R_pred_list = []\n","\n","        #f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_hat)\n","        f1, f2, f3, f4, S_pred, I_pred, D_pred, R_pred = self.net_f(self.t_batch)\n","        self.optimizer.zero_grad()\n","\n","        S_pred_list.append(self.S_min + (self.S_max - self.S_min) * S_pred) \n","        I_pred_list.append(self.I_min + (self.I_max - self.I_min) * I_pred)\n","        D_pred_list.append(self.D_min + (self.D_max - self.D_min) * D_pred)\n","        R_pred_list.append(self.R_min + (self.R_max - self.R_min) * R_pred)\n","\n","        #100 data points on S, and only 1 data point (initial conditions) for I, D, R\n","        loss = (torch.mean(torch.square(self.S_hat - S_pred))+ \n","                torch.mean(torch.square(self.I_hat - I_pred))+\n","                torch.mean(torch.square(self.D_hat - D_pred))+\n","                torch.mean(torch.square(self.R_hat[0] - R_pred[0]))+\n","                torch.mean(torch.square(f1))+\n","                torch.mean(torch.square(f2))+\n","                torch.mean(torch.square(f3))+\n","                torch.mean(torch.square(f4))\n","                ) \n","\n","        #loss.backward(retain_graph=True)\n","        loss.backward()\n","        self.optimizer.step()\n","        self.scheduler.step() \n","        #self.scheduler.step(loss) \n","        # self.optimizer.param_groups[0]['lr'] = 1e-9\n","\n","        self.losses.append(loss.item())\n","\n","        if epoch % 1000 == 0:          \n","          print('\\nEpoch ', epoch)\n","\n","        #loss + model parameters update\n","        if epoch % 4000 == 0:\n","          #checkpoint save every 100 epochs if the loss is lower\n","          print('\\nSaving model... Loss is: ', loss)\n","          torch.save({\n","              'epoch': epoch,\n","              'model': self.state_dict(),\n","              'optimizer_state_dict': self.optimizer.state_dict(),\n","              'scheduler': self.scheduler.state_dict(),\n","              #'loss': loss,\n","              'losses': self.losses,\n","              }, PATH + str(self.save)+'.pt')\n","          if self.save % 2 > 0: #its on 3\n","            self.save = 2 #change to 2\n","          else: #its on 2\n","            self.save = 3 #change to 3\n","\n","          print('epoch: ', epoch)\n","          print('alpha: (goal 0.191 ', self.alpha)\n","          print('beta: (goal 0.05 ', self.beta)\n","          print('gamma: (goal 0.0294 ', self.gamma)\n","\n","          # print('#################################')                \n","\n","        \n","      #plot\n","      #plt.plot(self.losses, color = 'teal')\n","      #plt.xlabel('Epochs')\n","      #plt.ylabel('Loss')\n","      return S_pred_list, I_pred_list, D_pred_list, R_pred_list"],"execution_count":3,"outputs":[{"output_type":"stream","text":["CPU times: user 54 µs, sys: 0 ns, total: 54 µs\n","Wall time: 57.7 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_P1obOwWZqOc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efe60bdd-f548-485c-f5a4-452137208f39"},"source":["%%time\n","\n","#this worked best\n","dinn = DINN(sidr_data[0], sidr_data[1], sidr_data[2], sidr_data[3], \n","            sidr_data[4]) #in the form of [t,S,I,D,R]\n","\n","learning_rate = 1e-5\n","optimizer = optim.Adam(dinn.params, lr = learning_rate)\n","dinn.optimizer = optimizer\n","\n","#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(dinn.optimizer, factor=0.9, patience = 1000, verbose=True)\n","#scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-9, max_lr=1e-5, step_size_up=20000, mode=\"triangular2\", cycle_momentum=False)\n","scheduler = torch.optim.lr_scheduler.CyclicLR(dinn.optimizer, base_lr=1e-6, max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n","\n","dinn.scheduler = scheduler\n","\n","try: \n","  S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(700000) #train\n","except EOFError:\n","  if dinn.save == 2:\n","    dinn.save = 3\n","    S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(700000) #train\n","  elif dinn.save == 3:\n","    dinn.save = 2\n","    S_pred_list, I_pred_list, D_pred_list, R_pred_list = dinn.train(700000) #train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","starting training...\n","\n","\n","Epoch  0\n","\n","Saving model... Loss is:  tensor(16.6192, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  0\n","alpha: (goal 0.191  tensor(0.1910)\n","beta: (goal 0.05  tensor(0.0500)\n","gamma: (goal 0.0294  tensor(0.0294)\n","\n","Epoch  1000\n","\n","Epoch  2000\n","\n","Epoch  3000\n","\n","Epoch  4000\n","\n","Saving model... Loss is:  tensor(5.7273, dtype=torch.float64, grad_fn=<AddBackward0>)\n","epoch:  4000\n","alpha: (goal 0.191  tensor(0.1910)\n","beta: (goal 0.05  tensor(0.0500)\n","gamma: (goal 0.0294  tensor(0.0294)\n","\n","Epoch  5000\n","\n","Epoch  6000\n","\n","Epoch  7000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WwqBVtEM9FYG"},"source":["plt.plot(dinn.losses[300000:], color = 'teal')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss'),"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJrvoRWQZqOd"},"source":["fig = plt.figure(figsize=(12,12))\n","\n","ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n","ax.set_facecolor('xkcd:white')\n","\n","\n","ax.plot(sidr_data[0], sidr_data[1], 'pink', alpha=0.5, lw=2, label='Susceptible')\n","ax.plot(sidr_data[0], S_pred_list[0].detach().numpy(), 'red', alpha=0.9, lw=2, label='Susceptible Prediction', linestyle='dashed')\n","\n","ax.plot(sidr_data[0], sidr_data[2], 'violet', alpha=0.5, lw=2, label='Infected')\n","ax.plot(sidr_data[0], I_pred_list[0].detach().numpy(), 'dodgerblue', alpha=0.9, lw=2, label='Infected Prediction', linestyle='dashed')\n","\n","ax.plot(sidr_data[0], sidr_data[3], 'darkgreen', alpha=0.5, lw=2, label='Dead')\n","ax.plot(sidr_data[0], D_pred_list[0].detach().numpy(), 'green', alpha=0.9, lw=2, label='Dead Prediction', linestyle='dashed')\n","\n","ax.plot(sidr_data[0], sidr_data[4], 'blue', alpha=0.5, lw=2, label='Recovered')\n","ax.plot(sidr_data[0], R_pred_list[0].detach().numpy(), 'teal', alpha=0.9, lw=2, label='Recovered Prediction', linestyle='dashed')\n","\n","\n","ax.set_xlabel('Time /days')\n","ax.set_ylabel('Number')\n","#ax.set_ylim([-1,50])\n","ax.yaxis.set_tick_params(length=0)\n","ax.xaxis.set_tick_params(length=0)\n","ax.grid(b=True, which='major', c='black', lw=0.2, ls='-')\n","legend = ax.legend()\n","legend.get_frame().set_alpha(0.5)\n","for spine in ('top', 'right', 'bottom', 'left'):\n","    ax.spines[spine].set_visible(False)\n","plt.savefig('itswhite.pdf')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBbyd2AgPrwe"},"source":["#calculate relative MSE loss\n","import math\n","import numpy as np\n","\n","S_total_loss = 0\n","S_den = 0\n","I_total_loss = 0\n","I_den = 0\n","D_total_loss = 0\n","D_den = 0\n","R_total_loss = 0\n","R_den = 0\n","t = np.linspace(0, 500, 100) \n","\n","for timestep in range(len(t)):\n","  S_value = sidr_data[1][timestep] - S_pred_list[0].detach().numpy()[timestep]\n","  S_total_loss += S_value**2\n","  S_den += (sidr_data[1][timestep])**2\n","  I_value = sidr_data[2][timestep] - I_pred_list[0].detach().numpy()[timestep]\n","  I_total_loss += I_value**2\n","  I_den += (sidr_data[2][timestep])**2\n","  D_value = sidr_data[3][timestep] - D_pred_list[0].detach().numpy()[timestep]\n","  D_total_loss += D_value**2\n","  D_den += (sidr_data[3][timestep])**2\n","  R_value = sidr_data[4][timestep] - R_pred_list[0].detach().numpy()[timestep]\n","  R_total_loss += R_value**2\n","  R_den += (sidr_data[4][timestep])**2\n","\n","S_total_loss = math.sqrt(S_total_loss/S_den)\n","I_total_loss = math.sqrt(I_total_loss/I_den)\n","D_total_loss = math.sqrt(D_total_loss/D_den)\n","R_total_loss = math.sqrt(R_total_loss/R_den)\n","\n","print('S_total_loss: ', S_total_loss)\n","print('I_total_loss: ', I_total_loss)\n","print('D_total_loss: ', D_total_loss)\n","print('R_total_loss: ', R_total_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUzZI6VMZqOe"},"source":["import numpy as np\n","from scipy.integrate import odeint\n","import matplotlib.pyplot as plt\n","\n","# Initial conditions\n","N = 59e6\n","\n","S0 = N - 1\n","I0 = 1\n","D0 = 0\n","R0 = 0\n","# A grid of time points (in days)\n","t = np.linspace(0, 500, 100) \n","\n","#parameters\n","alpha = dinn.alpha\n","beta = dinn.beta\n","gamma = dinn.gamma\n","\n","# The SIR model differential equations.\n","def deriv(y, t, alpha, betta, gamma):\n","    S, I, D, R = y\n","    dSdt = - (alpha / N) * S * I\n","    dIdt = (alpha / N) * S * I - beta * I - gamma * I \n","    dDdt = gamma * I\n","    dRdt = beta * I\n","\n","    return dSdt, dIdt, dDdt, dRdt\n","\n","\n","# Initial conditions vector\n","y0 = S0, I0, D0, R0\n","# Integrate the SIR equations over the time grid, t.\n","ret = odeint(deriv, y0, t, args=(alpha, beta, gamma))\n","S, I, D, R = ret.T\n","\n","# Plot the data on two separate curves for S(t), I(t)\n","fig = plt.figure(facecolor='w', figsize=(12,12))\n","ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n","\n","ax.plot(t, S, 'violet', alpha=0.5, lw=2, label='Learnable Param Susceptible', linestyle='dashed')\n","ax.plot(t, sidr_data[1], 'dodgerblue', alpha=0.5, lw=2, label='Susceptible')\n","\n","ax.plot(t, I, 'darkgreen', alpha=0.5, lw=2, label='Learnable Param Infected', linestyle='dashed')\n","ax.plot(t, sidr_data[2], 'gold', alpha=0.5, lw=2, label='Susceptible')\n","\n","ax.plot(t, D, 'red', alpha=0.5, lw=2, label='Learnable Param Dead', linestyle='dashed')\n","ax.plot(t, sidr_data[3], 'salmon', alpha=0.5, lw=2, label='Dead')\n","\n","ax.plot(t, R, 'blue', alpha=0.5, lw=2, label='Learnable Param Recovered', linestyle='dashed')\n","ax.plot(t, sidr_data[4], 'wheat', alpha=0.5, lw=2, label='Recovered')\n","\n","ax.set_xlabel('Time /days')\n","ax.yaxis.set_tick_params(length=0)\n","ax.xaxis.set_tick_params(length=0)\n","ax.grid(b=True, which='major', c='w', lw=2, ls='-')\n","legend = ax.legend()\n","legend.get_frame().set_alpha(0.5)\n","for spine in ('top', 'right', 'bottom', 'left'):\n","    ax.spines[spine].set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-zofRIm2RNz"},"source":["#calculate relative MSE loss\n","import math\n","\n","S_total_loss = 0\n","S_den = 0\n","I_total_loss = 0\n","I_den = 0\n","D_total_loss = 0\n","D_den = 0\n","R_total_loss = 0\n","R_den = 0\n","\n","for timestep in range(len(t)):\n","  S_value = sidr_data[1][timestep] - S[timestep]\n","  S_total_loss += S_value**2\n","  S_den += (sidr_data[1][timestep])**2\n","  I_value = sidr_data[2][timestep] - I[timestep]\n","  I_total_loss += I_value**2\n","  I_den += (sidr_data[2][timestep])**2\n","  D_value = sidr_data[3][timestep] - D[timestep]\n","  D_total_loss += D_value**2\n","  D_den += (sidr_data[3][timestep])**2\n","  R_value = sidr_data[4][timestep] - R[timestep]\n","  R_total_loss += R_value**2\n","  R_den += (sidr_data[4][timestep])**2\n","\n","S_total_loss = math.sqrt(S_total_loss/S_den)\n","I_total_loss = math.sqrt(I_total_loss/I_den)\n","D_total_loss = math.sqrt(D_total_loss/D_den)\n","R_total_loss = math.sqrt(R_total_loss/R_den)\n","\n","print('S_total_loss: ', S_total_loss)\n","print('I_total_loss: ', I_total_loss)\n","print('D_total_loss: ', D_total_loss)\n","print('R_total_loss: ', R_total_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lFJLEj4LFVw"},"source":[""],"execution_count":null,"outputs":[]}]}