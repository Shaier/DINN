{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tSI_data = genfromtxt('tSI_data.csv', delimiter=',') #in the form of [t, S, I]\n",
    "tao_data = genfromtxt('tao_data.csv', delimiter=',') #in the form of [t, tao_star, T_star, u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nstarting training...\n\nself.alpha1[0] before training tensor(0.9947, grad_fn=<SelectBackward>)\nepoch:  0\nbefore backward\nafter backward\nloss:  tensor(926070.9222, dtype=torch.float64, grad_fn=<AddBackward0>)\n\nself.alpha1 after the training:  tensor(0.9947, grad_fn=<SelectBackward>)\nepoch:  1\nbefore backward\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-325c058ee6b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mdinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#t, S_data, I_data, tao_data, T_star_data, u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mdinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-325c058ee6b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "class DINN:\n",
    "    def __init__(self, t, S_data, I_data, tao_data, T_star_data, u_data): #t, S_data, I_data, tao_data, T_star_data, u_data\n",
    "        \n",
    "        #data\n",
    "        self.t = torch.tensor(t, requires_grad = True).float()\n",
    "        self.S = torch.tensor(S_data, requires_grad = True)\n",
    "        self.I = torch.tensor(I_data, requires_grad = True)\n",
    "        self.taos = torch.tensor(tao_data, requires_grad = True)\n",
    "        self.T_stars = torch.tensor(T_star_data, requires_grad = True)\n",
    "        self.u = torch.tensor(u_data, requires_grad = True).float() \n",
    "        self.t_u = torch.stack((self.t, self.u), dim=1) #stack for the NN input \n",
    "\n",
    "        #NN init\n",
    "        self.neural_network = self.neural_network()\n",
    "        \n",
    "        #learnable parameters\n",
    "        self.alpha1 = torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.alpha2 = torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.mu = torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "        self.beta = torch.nn.Parameter(torch.rand(len(t), requires_grad=True))\n",
    "\n",
    "        #combine parameters for optimizer\n",
    "        self.params = list(self.neural_network.parameters())\n",
    "        self.params.extend(list([self.alpha1, self.alpha2, self.mu, self.beta]))\n",
    "\n",
    "\n",
    "        #predictions\n",
    "        #self.S_pred, self.I_pred, self.tao_pred, self.T_star_pred = self.net_x(self.t_u) \n",
    "        #self.f1, self.f2 = self.net_f(self.t, self.u)\n",
    "        \n",
    "        #loss\n",
    "        #self.loss = (torch.mean(torch.square(self.S - self.S_pred))+torch.mean(torch.square(self.I - self.I_pred)) +\n",
    "        #        torch.mean(torch.square(self.f1)) + torch.mean(torch.square(self.f2)) \\\n",
    "        #        + torch.mean(torch.square(self.taos - self.tao_pred)) + torch.mean(torch.square(self.T_stars - self.T_star_pred)))\n",
    "\n",
    "        #optimizer\n",
    "        self.learning_rate = 0.01\n",
    "        self.optimizer = optim.Adam(self.params, lr = self.learning_rate)\n",
    "\n",
    "\n",
    "    # NN\n",
    "    class neural_network(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(DINN.neural_network, self).__init__()\n",
    "            self.fc1=nn.Linear(2, 20) #takes [t, u]\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 4) #outputs S, I, tao, T*\n",
    "\n",
    "        def forward(self, x):\n",
    "            x=F.relu(self.fc1(x))\n",
    "            x=F.relu(self.fc2(x))\n",
    "            x=self.out(x)\n",
    "            return x    \n",
    "    # Net x\n",
    "    def net_x(self, t_u):\n",
    "        net_output = self.neural_network(t_u)\n",
    "        S_pred = net_output[:,0]\n",
    "        I_pred = net_output[:,1]\n",
    "        tao_pred = net_output[:,2]\n",
    "        T_star_pred = net_output[:,3]\n",
    "        return S_pred, I_pred, tao_pred, T_star_pred\n",
    "\n",
    "    # Net f\n",
    "    def net_f(self, t, u):\n",
    "        alpha1 = self.alpha1\n",
    "        alpha2 = self.alpha2\n",
    "        mu = self.mu\n",
    "        beta = self.beta\n",
    "\n",
    "        S_pred, I_pred, tao_pred, T_star_pred = self.net_x(torch.stack((t, u), dim=1))\n",
    "\n",
    "        #need to fix this loop to be more efficient (this just calculates S_t, I_t)\n",
    "        S_t_list = []\n",
    "        I_t_list = [] \n",
    "        for input_tensor in self.t_u:\n",
    "            output_tensor = self.neural_network(input_tensor) #size 4 (S_pred, I_pred, tao_pred, T_star_pred)\n",
    "            S_t_value = grad(output_tensor[0], input_tensor, retain_graph=True)[0][0] #derivative of S wrt t            \n",
    "            I_t_value = grad(output_tensor[1], input_tensor, retain_graph=True)[0][0] #derivative of I wrt t\n",
    "            S_t_list.append(S_t_value)\n",
    "            I_t_list.append(I_t_value)\n",
    "        #convert to tensors\n",
    "        S_t = torch.tensor(S_t_list, requires_grad = True)\n",
    "        I_t = torch.tensor(I_t_list, requires_grad = True)        \n",
    "\n",
    "        f1 = S_t + self.beta * S_pred * I_pred + u * (t > tao_pred) * alpha1\n",
    "        f2 = I_t - self.beta * S_pred * I_pred + mu * I_pred + u * (t > tao_pred) * alpha2\n",
    "        return f1, f2\n",
    "    \n",
    "    #train    \n",
    "    def train(self, n_epochs):\n",
    "        print('\\nstarting training...\\n')\n",
    "        print('self.alpha1[0] before training', self.alpha1[0])\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            f1, f2 = self.net_f(self.t, self.u)\n",
    "            S_pred, I_pred, tao_pred, T_star_pred = self.net_x(self.t_u)\n",
    "\n",
    "            loss = (torch.mean(torch.square(self.S-S_pred))+torch.mean(torch.square(self.I-I_pred)) \\\n",
    "                    +torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)) \\\n",
    "                    +torch.mean(torch.square(self.taos-tao_pred)) + torch.mean(torch.square(self.T_stars-T_star_pred))) #tao, T_star\n",
    "\n",
    "            losses.append(loss)\n",
    "            print('before backward')\n",
    "            loss.backward()\n",
    "            print('after backward')\n",
    "            torch.nn.utils.clip_grad_norm_(self.neural_network.parameters(), 100) #gradient clipping\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #print\n",
    "            if epoch % 1 == 0:\n",
    "                #print('\\nepoch: ', epoch)\n",
    "                print('loss: ', losses[-1])\n",
    "                print('\\nself.alpha1 after the training: ', self.alpha1[0])\n",
    "\n",
    "        plt.plot(losses, color = 'teal')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "dinn = DINN(tSI_data[0], tSI_data[1], tSI_data[2], tao_data[1], tao_data[2], tao_data[3]) #t, S_data, I_data, tao_data, T_star_data, u\n",
    "dinn.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}