{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSI_data = genfromtxt('tSI_data.csv', delimiter=',') #in the form of [t, S, I]\n",
    "tao_data = genfromtxt('tao_data.csv', delimiter=',') #in the form of [t, tao_star, T_star, us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINN:\n",
    "    def __init__(self, t, S_data, I_data, tao_data, T_star_data, u_data): #, t, S_data, I_data, tao_data, T_star_data, u\n",
    "        self.t = t\n",
    "        self.S = S_data\n",
    "        self.I = I_data\n",
    "        self.taos = tao_data\n",
    "        self.T_stars = T_star_data\n",
    "        self.u_data = u_data\n",
    "\n",
    "        self.nx = self.net_x()\n",
    "\n",
    "        #learnable parameters\n",
    "        self.alpha1=torch.rand(1, requires_grad=True)\n",
    "        self.alpha2=torch.rand(1, requires_grad=True)\n",
    "        self.mu=torch.rand(1, requires_grad=True)\n",
    "        self.beta=torch.rand(1, requires_grad=True)\n",
    "\n",
    "    #nets\n",
    "    class net_x(nn.Module): #, t, u\n",
    "        def __init__(self):\n",
    "            super(DINN.net_x, self).__init__()\n",
    "            self.fc1=nn.Linear(2, 20) #takes t, u\n",
    "            self.fc2=nn.Linear(20, 20)\n",
    "            self.out=nn.Linear(20, 4) #outputs S, I, tao, T*\n",
    "\n",
    "        def forward(self, x):\n",
    "            x=self.fc1(x)\n",
    "            x=self.fc2(x)\n",
    "            x=self.out(x)\n",
    "            return x    \n",
    "    \n",
    "    \n",
    "    def net_f(self, t, u):\n",
    "        #print('net_f')\n",
    "        t_u = torch.tensor([t.float(),u.float()], requires_grad = True)\n",
    "\n",
    "        #nx = self.net_x()\n",
    "\n",
    "        #S, I, tao, T_star = nx(t_u) #input = tensor of size 2  \n",
    "        S, I, tao, T_star = self.nx(t_u) #input = tensor of size 2  \n",
    "        S_t = grad(S, t_u, retain_graph=True)[0][0] #derivative of S_pred wrt t              \n",
    "        I_t = grad(I, t_u, retain_graph=True)[0][0] #derivative of I_pred wrt t              \n",
    "        \n",
    "        f1 = S_t + self.beta * S * I + u * (t > tao) * self.alpha1\n",
    "        f2 = I_t - self.beta * S * I + self.mu * I + u * (t > tao) * self.alpha2\n",
    "\n",
    "        return f1, f2\n",
    "    \n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        print('\\nstarting training...\\n')\n",
    "        losses = []\n",
    "        learning_rate = 0.01\n",
    "        momentum = 0.5        \n",
    "        #nx = self.net_x()\n",
    "        #optimizer = optim.SGD(nx.parameters(), lr = learning_rate, momentum = momentum)\n",
    "        #optimizer = optim.SGD(self.net_x().parameters(), lr = learning_rate, momentum = momentum)\n",
    "        \n",
    "        optimizer = optim.SGD(self.nx.parameters(), lr = learning_rate, momentum = momentum)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print('epoch: ', epoch)\n",
    "            print('self.alpha1', self.alpha1)\n",
    "            #print('self.alpha2', self.alpha2)\n",
    "            for time_step in range(len(self.t)):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                t_value, u_value = self.t[time_step], self.u_data[time_step]\n",
    "                t_value = torch.tensor(t_value)\n",
    "                u_value = torch.tensor(u_value)\n",
    "\n",
    "                f1, f2 = self.net_f(t_value, u_value)\n",
    "                #print('f1, f2: ', f1, f2)\n",
    "                t_u = torch.tensor([t_value.float(), u_value.float()], requires_grad = True)\n",
    "                #print('t_u: ', t_u)\n",
    "                S_pred, I_pred, tao_pred, T_star_pred = self.nx(t_u)\n",
    "                #S_pred, I_pred, tao_pred, T_star_pred = self.net_x(t_u)\n",
    "                #print('S_pred, I_pred, tao_pred, T_star_pred: ', S_pred, I_pred, tao_pred, T_star_pred)\n",
    "                #print('4')\n",
    "\n",
    "                #print(torch.mean(torch.square(self.S[time_step]-S_pred))+torch.mean(torch.square(self.I[time_step]-I_pred)))\n",
    "                loss = (torch.mean(torch.square(self.S[time_step]-S_pred))+torch.mean(torch.square(self.I[time_step]-I_pred)) \\\n",
    "                    + #S,I \n",
    "                    torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)) \\\n",
    "                    + #f1, f2\n",
    "                    torch.mean(torch.square(self.taos[time_step]-tao_pred)) + torch.mean(torch.square(self.T_stars[time_step]-T_star_pred))) #tao, T_star\n",
    "                \n",
    "                    \n",
    "                #print(loss)\n",
    "                \n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.nx.parameters(), 100) #gradient clipping\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(loss)    \n",
    "            print('loss: ', losses[-1])\n",
    "            print('')\n",
    "        plt.plot(losses, color = 'teal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "starting training...\n",
      "\n",
      "epoch:  0\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(7842.0049, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  1\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(16848.1250, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  2\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(9198.0225, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  3\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(10685.0381, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  4\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(17761.0156, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  5\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(8023.0830, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  6\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6080.3086, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  7\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5215.1768, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  8\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(4953.6553, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  9\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(30529.3672, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  10\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(49175.2656, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  11\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5257.1133, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  12\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(9245.9287, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  13\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(7532.3857, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  14\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(41659.8281, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  15\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(12783.6533, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  16\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(10180.5684, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  17\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(45683.5039, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  18\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(7013.2358, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  19\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5529.6343, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  20\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(2133.8252, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  21\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5810.4307, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  22\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3075.3513, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  23\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(18640.3906, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  24\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(4890.6289, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  25\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(15699.7568, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  26\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(29010.8633, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  27\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(11740.2041, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  28\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(22819.3535, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  29\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(149997.2031, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  30\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(17494.1738, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  31\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(14267.7129, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  32\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5521.9810, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  33\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(13702.8574, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  34\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(12013.9688, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  35\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(15830.2266, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  36\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3795.1099, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  37\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(2852.7441, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  38\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5749.4507, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  39\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(24084.0586, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  40\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(2838.1094, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  41\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6805.9355, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  42\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(8938.6045, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  43\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5036.4077, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  44\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(13469.9619, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  45\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5359.9243, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  46\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(146107.2188, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  47\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(7290.5737, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  48\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(426709.1562, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  49\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(20352.6953, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  50\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(8263316., grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  51\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3955.8132, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  52\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(66812.4609, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  53\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(29065.4180, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  54\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6437294., grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  55\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(9599.8340, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  56\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(9739.0654, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  57\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(1803468.6250, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  58\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(274154.0625, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  59\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(25253.6855, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  60\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(30521.4375, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  61\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5031.7109, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  62\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(4084.2900, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  63\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(303596.3125, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  64\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6261.6831, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  65\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5843.6646, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  66\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3319.3887, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  67\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(712.4662, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  68\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(8749.5498, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  69\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(2372.5757, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  70\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(755.9780, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  71\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(8725.1846, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  72\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5400.2930, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  73\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3780.4775, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  74\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(27043.8965, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  75\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(4743.7646, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  76\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3301.9812, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  77\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3830.9504, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  78\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6361.9893, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  79\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(62586.9727, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  80\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(31807.7754, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  81\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5110.5405, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  82\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(139170.9688, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  83\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(13127.9902, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  84\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(876.2108, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  85\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3205.2087, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  86\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(6359.4512, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  87\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5885.8364, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  88\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(2345791., grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  89\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(12565.4277, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  90\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(5056.9985, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  91\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(1709.0809, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  92\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(57691.3867, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  93\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(3005.9570, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  94\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(52191.9727, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  95\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(4737.9258, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  96\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n",
      "loss:  tensor(11036.2119, grad_fn=<AddBackward0>)\n",
      "\n",
      "epoch:  97\n",
      "self.alpha1 tensor([0.6489], requires_grad=True)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e67ce279d893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdinn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtSI_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtao_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#t, S_data, I_data, tao_data, T_star_data, u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-b33f04558690>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dinn = DINN(tSI_data[0], tSI_data[1], tSI_data[2], tao_data[1], tao_data[2], tao_data[3]) #t, S_data, I_data, tao_data, T_star_data, u\n",
    "dinn.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}