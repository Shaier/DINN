{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd0f0396a0f98e081442f6005f4438dae70905c4dba32e635697d7a979ca5a56ea2",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "from numpy import genfromtxt\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tSI_data = genfromtxt('tSI_data.csv', delimiter=',') #in the form of [t, S, I]\n",
    "tao_data = genfromtxt('tao_data.csv', delimiter=',') #in the form of [t, tao_star, T_star, us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " containing:\n",
      "tensor([3.4083], requires_grad=True)\n",
      "loss:  tensor(4.5203, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4091], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4091], requires_grad=True)\n",
      "loss:  tensor(4.4603, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4099], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4099], requires_grad=True)\n",
      "loss:  tensor(4.5391, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4107], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4107], requires_grad=True)\n",
      "loss:  tensor(4.6411, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4116], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4116], requires_grad=True)\n",
      "loss:  tensor(4.7591, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4124], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4124], requires_grad=True)\n",
      "loss:  tensor(4.7349, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4132], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4132], requires_grad=True)\n",
      "loss:  tensor(4.5853, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4140], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4140], requires_grad=True)\n",
      "loss:  tensor(4.4061, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4148], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4148], requires_grad=True)\n",
      "loss:  tensor(4.3139, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4156], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4156], requires_grad=True)\n",
      "loss:  tensor(4.3291, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4164], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4164], requires_grad=True)\n",
      "loss:  tensor(4.3458, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4172], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4172], requires_grad=True)\n",
      "loss:  tensor(4.3525, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4180], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4180], requires_grad=True)\n",
      "loss:  tensor(4.2757, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4188], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4188], requires_grad=True)\n",
      "loss:  tensor(4.2049, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4197], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4197], requires_grad=True)\n",
      "loss:  tensor(4.1723, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4205], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4205], requires_grad=True)\n",
      "loss:  tensor(4.1982, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4213], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4213], requires_grad=True)\n",
      "loss:  tensor(4.2567, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4221], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4221], requires_grad=True)\n",
      "loss:  tensor(4.2537, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4229], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4229], requires_grad=True)\n",
      "loss:  tensor(4.1909, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4237], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4237], requires_grad=True)\n",
      "loss:  tensor(4.1284, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4245], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4245], requires_grad=True)\n",
      "loss:  tensor(4.1081, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4253], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4253], requires_grad=True)\n",
      "loss:  tensor(4.1581, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4262], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4262], requires_grad=True)\n",
      "loss:  tensor(4.2800, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4270], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4270], requires_grad=True)\n",
      "loss:  tensor(4.3954, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4278], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4278], requires_grad=True)\n",
      "loss:  tensor(4.3574, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4286], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4286], requires_grad=True)\n",
      "loss:  tensor(4.2041, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4294], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4294], requires_grad=True)\n",
      "loss:  tensor(4.0571, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4302], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4302], requires_grad=True)\n",
      "loss:  tensor(4.0696, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4310], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4310], requires_grad=True)\n",
      "loss:  tensor(4.2809, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4318], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4318], requires_grad=True)\n",
      "loss:  tensor(4.5045, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4326], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4326], requires_grad=True)\n",
      "loss:  tensor(4.5894, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4334], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4334], requires_grad=True)\n",
      "loss:  tensor(4.4367, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4342], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4342], requires_grad=True)\n",
      "loss:  tensor(4.1976, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4350], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4350], requires_grad=True)\n",
      "loss:  tensor(4.0084, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4358], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4358], requires_grad=True)\n",
      "loss:  tensor(3.9546, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4366], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4366], requires_grad=True)\n",
      "loss:  tensor(4.0842, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4375], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4375], requires_grad=True)\n",
      "loss:  tensor(4.2769, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4383], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4383], requires_grad=True)\n",
      "loss:  tensor(4.3857, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4391], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4391], requires_grad=True)\n",
      "loss:  tensor(4.3123, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4399], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4399], requires_grad=True)\n",
      "loss:  tensor(4.2923, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4407], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4407], requires_grad=True)\n",
      "loss:  tensor(4.3267, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4415], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4415], requires_grad=True)\n",
      "loss:  tensor(4.4076, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4423], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4423], requires_grad=True)\n",
      "loss:  tensor(4.4714, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4431], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4431], requires_grad=True)\n",
      "loss:  tensor(4.4165, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4439], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4439], requires_grad=True)\n",
      "loss:  tensor(4.2688, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4447], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4447], requires_grad=True)\n",
      "loss:  tensor(4.1133, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4455], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4455], requires_grad=True)\n",
      "loss:  tensor(3.9730, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4463], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4463], requires_grad=True)\n",
      "loss:  tensor(3.8625, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4471], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4471], requires_grad=True)\n",
      "loss:  tensor(3.7967, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4479], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4479], requires_grad=True)\n",
      "loss:  tensor(3.8171, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4487], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4487], requires_grad=True)\n",
      "loss:  tensor(3.8725, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4495], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4495], requires_grad=True)\n",
      "loss:  tensor(3.9096, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4503], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4503], requires_grad=True)\n",
      "loss:  tensor(3.8959, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4511], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4511], requires_grad=True)\n",
      "loss:  tensor(3.8571, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4519], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4519], requires_grad=True)\n",
      "loss:  tensor(3.8490, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4527], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4527], requires_grad=True)\n",
      "loss:  tensor(3.8874, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4535], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4535], requires_grad=True)\n",
      "loss:  tensor(3.9488, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4543], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4543], requires_grad=True)\n",
      "loss:  tensor(3.9519, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4551], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4551], requires_grad=True)\n",
      "loss:  tensor(3.8956, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4560], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4560], requires_grad=True)\n",
      "loss:  tensor(3.8045, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4568], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4568], requires_grad=True)\n",
      "loss:  tensor(3.7656, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4576], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4576], requires_grad=True)\n",
      "loss:  tensor(3.7191, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4584], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4584], requires_grad=True)\n",
      "loss:  tensor(3.7156, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4592], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4592], requires_grad=True)\n",
      "loss:  tensor(3.7552, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4600], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4600], requires_grad=True)\n",
      "loss:  tensor(3.8219, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4608], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4608], requires_grad=True)\n",
      "loss:  tensor(3.8407, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4616], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4616], requires_grad=True)\n",
      "loss:  tensor(3.8374, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4624], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4624], requires_grad=True)\n",
      "loss:  tensor(3.8617, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4632], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4632], requires_grad=True)\n",
      "loss:  tensor(4.0374, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4640], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4640], requires_grad=True)\n",
      "loss:  tensor(4.2445, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4648], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4648], requires_grad=True)\n",
      "loss:  tensor(4.2962, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4656], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4656], requires_grad=True)\n",
      "loss:  tensor(4.1604, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4664], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4664], requires_grad=True)\n",
      "loss:  tensor(4.0191, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4672], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4672], requires_grad=True)\n",
      "loss:  tensor(3.8483, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4680], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4680], requires_grad=True)\n",
      "loss:  tensor(3.7402, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4688], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4688], requires_grad=True)\n",
      "loss:  tensor(3.7262, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4697], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4697], requires_grad=True)\n",
      "loss:  tensor(3.8030, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4705], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4705], requires_grad=True)\n",
      "loss:  tensor(3.9374, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4713], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4713], requires_grad=True)\n",
      "loss:  tensor(4.0476, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4721], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4721], requires_grad=True)\n",
      "loss:  tensor(4.0860, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4729], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4729], requires_grad=True)\n",
      "loss:  tensor(4.0525, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4737], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4737], requires_grad=True)\n",
      "loss:  tensor(3.9916, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4745], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4745], requires_grad=True)\n",
      "loss:  tensor(3.9472, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4753], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4753], requires_grad=True)\n",
      "loss:  tensor(3.9784, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4761], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4761], requires_grad=True)\n",
      "loss:  tensor(4.0910, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4769], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4769], requires_grad=True)\n",
      "loss:  tensor(4.1901, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4777], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4777], requires_grad=True)\n",
      "loss:  tensor(4.2142, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4785], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4785], requires_grad=True)\n",
      "loss:  tensor(4.1796, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4793], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4793], requires_grad=True)\n",
      "loss:  tensor(3.9982, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4801], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4801], requires_grad=True)\n",
      "loss:  tensor(3.8641, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4809], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4809], requires_grad=True)\n",
      "loss:  tensor(3.8359, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4817], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4817], requires_grad=True)\n",
      "loss:  tensor(3.9082, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4825], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4825], requires_grad=True)\n",
      "loss:  tensor(3.9139, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4833], requires_grad=True)\n",
      "\n",
      "self.alpha1 Parameter containing:\n",
      "tensor([3.4833], requires_grad=True)\n",
      "loss:  tensor(3.8527, grad_fn=<DivBackward0>)\n",
      "self.alpha1 after the training!!!:  Parameter containing:\n",
      "tensor([3.4841], requires_grad=True)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 388.965625 262.19625\" width=\"388.965625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 262.19625 \nL 388.965625 262.19625 \nL 388.965625 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 46.965625 224.64 \nL 381.765625 224.64 \nL 381.765625 7.2 \nL 46.965625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5dd2af4112\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.183807\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(59.002557 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.927994\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(103.384244 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.672182\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1000 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(150.947182 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"214.416369\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 1500 -->\n      <g transform=\"translate(201.691369 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"265.160557\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2000 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(252.435557 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.904744\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2500 -->\n      <g transform=\"translate(303.179744 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.648932\" xlink:href=\"#m5dd2af4112\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 3000 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(353.923932 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epochs -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(196.45 252.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m61a0507f59\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m61a0507f59\" y=\"214.828889\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(33.603125 218.628108)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m61a0507f59\" y=\"170.265292\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 200 -->\n      <g transform=\"translate(20.878125 174.06451)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m61a0507f59\" y=\"125.701694\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(20.878125 129.500913)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m61a0507f59\" y=\"81.138096\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 84.937315)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m61a0507f59\" y=\"36.574498\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 800 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(20.878125 40.373717)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Loss -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <g transform=\"translate(14.798438 126.973906)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"55.697266\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"116.878906\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"168.978516\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p86f1e96d78)\" d=\"M 62.183807 183.178518 \nL 63.097202 185.268341 \nL 63.503156 185.928839 \nL 63.706132 185.966786 \nL 63.909109 185.580537 \nL 64.416551 183.86424 \nL 64.518039 183.860517 \nL 64.721016 184.555195 \nL 65.025481 187.397938 \nL 65.634412 193.931519 \nL 65.938877 194.39119 \nL 66.141853 195.980067 \nL 66.446319 198.419786 \nL 66.547807 198.609478 \nL 66.750784 200.263188 \nL 66.95376 201.493095 \nL 67.055249 201.108723 \nL 67.156737 201.568689 \nL 67.461202 205.328044 \nL 67.664179 206.478019 \nL 67.765667 205.543623 \nL 67.867156 205.882379 \nL 68.070133 209.399591 \nL 68.171621 209.921481 \nL 68.374598 208.322024 \nL 68.577574 212.132692 \nL 68.679063 211.442479 \nL 68.780551 209.558016 \nL 68.983528 213.596367 \nL 69.085016 210.961994 \nL 69.287993 214.321623 \nL 69.389481 211.2419 \nL 69.592458 214.287421 \nL 69.693947 209.120818 \nL 69.795435 214.184829 \nL 69.896923 210.93962 \nL 70.0999 212.990896 \nL 70.201388 204.826504 \nL 70.302877 210.969838 \nL 70.404365 205.381183 \nL 70.607342 208.778465 \nL 70.70883 199.311238 \nL 70.810319 207.085461 \nL 70.911807 205.994657 \nL 71.013295 195.213957 \nL 71.114784 202.315621 \nL 71.317761 196.184029 \nL 71.419249 196.373863 \nL 71.520737 186.575248 \nL 71.622226 194.291064 \nL 71.723714 192.468911 \nL 72.028179 175.126097 \nL 72.129668 182.93611 \nL 72.231156 181.104906 \nL 72.535621 162.362148 \nL 72.637109 169.558714 \nL 72.738598 167.295699 \nL 72.941575 152.322466 \nL 73.043063 148.344198 \nL 73.144551 154.650666 \nL 73.24604 151.516893 \nL 73.449016 134.642721 \nL 73.550505 132.522251 \nL 73.651993 137.689041 \nL 73.753482 134.979104 \nL 74.057947 114.584948 \nL 74.159435 118.90958 \nL 74.260923 118.418922 \nL 74.565389 96.514101 \nL 74.768365 101.156118 \nL 74.869854 97.058961 \nL 75.174319 78.295494 \nL 75.377296 82.261801 \nL 75.681761 62.810487 \nL 75.783249 63.293073 \nL 75.986226 65.14333 \nL 76.392179 47.191822 \nL 76.493668 50.018376 \nL 76.595156 49.805445 \nL 76.798133 44.873027 \nL 77.00111 38.322241 \nL 77.102598 35.40639 \nL 77.204086 38.063152 \nL 77.407063 35.179016 \nL 77.508551 33.552652 \nL 77.61004 28.789659 \nL 77.711528 29.458494 \nL 77.813017 26.46148 \nL 77.914505 28.716914 \nL 78.523435 20.789305 \nL 78.624924 22.699455 \nL 78.726412 21.161992 \nL 78.8279 21.705246 \nL 78.929389 21.325148 \nL 79.030877 19.743716 \nL 79.132365 20.161133 \nL 79.233854 17.982511 \nL 79.335342 19.352439 \nL 79.436831 17.812786 \nL 79.538319 18.949424 \nL 79.741296 18.123458 \nL 79.842784 18.51853 \nL 79.944272 17.284572 \nL 80.045761 18.319036 \nL 80.147249 17.083636 \nL 80.248738 18.313664 \nL 80.350226 17.685873 \nL 80.553203 18.422788 \nL 80.654691 18.069345 \nL 80.756179 19.013533 \nL 80.857668 18.241137 \nL 80.959156 19.596309 \nL 81.060645 19.122454 \nL 81.263621 19.936438 \nL 81.36511 20.119178 \nL 81.466598 20.832484 \nL 81.568086 20.550072 \nL 81.669575 21.665318 \nL 81.771063 21.411003 \nL 81.872552 22.501524 \nL 81.97404 22.17577 \nL 82.379993 24.428552 \nL 82.481482 24.37641 \nL 82.785947 26.364677 \nL 83.1919 28.370529 \nL 83.699342 31.974961 \nL 84.612738 38.551396 \nL 84.917203 41.992767 \nL 85.018691 41.327768 \nL 85.12018 44.306381 \nL 85.221668 41.842177 \nL 85.323156 46.334495 \nL 85.424645 40.055581 \nL 85.526133 46.600043 \nL 85.627621 34.846666 \nL 85.72911 47.920604 \nL 85.830598 43.660801 \nL 86.033575 54.702205 \nL 86.135063 47.250926 \nL 86.236552 56.808055 \nL 86.33804 55.577946 \nL 86.439528 56.490039 \nL 86.541017 60.657174 \nL 86.642505 57.271058 \nL 86.845482 64.506836 \nL 86.94697 61.541809 \nL 87.149947 66.873883 \nL 87.251435 66.108803 \nL 87.352924 70.337816 \nL 87.454412 68.645574 \nL 87.657389 73.698023 \nL 87.758877 71.824484 \nL 87.961854 76.644582 \nL 88.063342 76.741766 \nL 88.164831 80.108977 \nL 88.266319 79.827463 \nL 88.469296 83.519749 \nL 88.570784 83.253317 \nL 88.672273 86.235438 \nL 88.773761 85.573744 \nL 88.976738 89.071853 \nL 89.078226 89.105662 \nL 89.179715 91.908593 \nL 89.281203 91.52145 \nL 89.48418 94.409121 \nL 89.890133 100.507268 \nL 89.991622 100.88634 \nL 90.09311 103.493157 \nL 90.194598 102.885568 \nL 90.296087 106.520715 \nL 90.397575 103.301441 \nL 90.499063 108.209115 \nL 90.600552 98.526426 \nL 90.70204 109.05155 \nL 90.803529 106.987254 \nL 91.006505 116.435422 \nL 91.107994 110.380972 \nL 91.31097 121.783108 \nL 91.412459 117.470056 \nL 91.615436 126.507974 \nL 91.716924 123.897574 \nL 91.919901 131.042077 \nL 92.021389 129.52551 \nL 92.224366 135.341142 \nL 92.325854 134.048624 \nL 92.528831 139.381071 \nL 92.630319 138.538181 \nL 92.833296 143.282039 \nL 92.934784 143.181163 \nL 93.137761 146.941277 \nL 93.23925 147.594656 \nL 93.442226 150.431689 \nL 93.949668 157.103867 \nL 94.051157 157.908416 \nL 94.355622 162.047634 \nL 94.761575 166.108884 \nL 95.06604 170.237905 \nL 95.167529 170.373464 \nL 95.269017 172.453672 \nL 95.370505 172.102595 \nL 95.471994 174.447137 \nL 95.573482 174.05707 \nL 95.776459 177.632388 \nL 96.994319 189.506128 \nL 97.095808 189.686186 \nL 97.400273 192.453608 \nL 97.907715 196.255589 \nL 99.937482 207.434938 \nL 100.647901 210.017505 \nL 101.256831 211.549404 \nL 102.373203 213.537889 \nL 102.880645 214.09906 \nL 103.388087 214.39357 \nL 103.489575 214.346031 \nL 103.591064 214.48644 \nL 103.692552 214.431794 \nL 103.895529 214.542465 \nL 104.199994 214.751485 \nL 105.214878 214.642885 \nL 106.33125 214.117154 \nL 108.563994 212.595461 \nL 109.375901 211.947787 \nL 111.811622 210.447096 \nL 111.913111 210.446411 \nL 112.217576 210.183111 \nL 112.319064 210.227372 \nL 112.927994 209.859293 \nL 113.435436 209.685814 \nL 116.277111 209.439838 \nL 116.683064 209.482567 \nL 118.712832 209.217712 \nL 119.118785 209.262868 \nL 119.626227 209.261362 \nL 120.336646 209.142976 \nL 121.047064 209.319368 \nL 121.148553 209.166712 \nL 121.250041 209.354323 \nL 121.351529 209.12053 \nL 121.453018 209.313449 \nL 121.655995 209.187576 \nL 121.757483 209.29377 \nL 121.858971 209.174845 \nL 122.061948 209.308152 \nL 122.163436 209.254993 \nL 122.264925 209.38812 \nL 122.467902 209.300427 \nL 122.56939 209.360556 \nL 122.772367 209.272758 \nL 123.17832 209.440786 \nL 123.279809 209.425075 \nL 123.482785 209.557621 \nL 123.888739 209.499347 \nL 124.091716 209.582686 \nL 125.106599 209.721667 \nL 125.309576 209.678678 \nL 125.918506 210.024554 \nL 127.440832 210.098778 \nL 127.948274 210.198205 \nL 128.455716 210.340683 \nL 129.978041 210.645142 \nL 130.586972 210.548465 \nL 131.29739 210.75179 \nL 132.312274 210.886949 \nL 132.718227 210.929931 \nL 133.124181 210.802797 \nL 133.428646 210.994171 \nL 133.733111 211.219251 \nL 134.037576 211.139345 \nL 134.139065 211.273755 \nL 134.240553 211.116878 \nL 134.342041 211.418815 \nL 134.44353 211.181168 \nL 134.545018 211.593058 \nL 134.646507 211.365637 \nL 134.849483 211.540607 \nL 134.950972 211.364096 \nL 135.05246 211.533083 \nL 135.255437 211.311652 \nL 135.356925 211.440021 \nL 135.458414 211.287722 \nL 135.66139 211.566792 \nL 135.762879 211.484982 \nL 135.965855 211.635766 \nL 136.067344 211.531483 \nL 136.270321 211.625197 \nL 136.371809 211.565218 \nL 136.574786 211.685549 \nL 136.676274 211.693694 \nL 136.879251 211.800386 \nL 137.589669 211.919438 \nL 137.894135 212.052641 \nL 138.1986 212.061244 \nL 138.706042 212.185769 \nL 139.010507 212.312582 \nL 139.517949 212.437749 \nL 140.532832 212.528439 \nL 140.938786 212.519644 \nL 141.243251 212.563543 \nL 141.953669 212.665911 \nL 142.867065 212.767842 \nL 147.129577 213.062756 \nL 147.637018 213.079079 \nL 148.956367 213.273798 \nL 150.072739 213.297183 \nL 150.884646 213.408457 \nL 151.89953 213.463393 \nL 152.812926 213.586977 \nL 155.248647 213.55122 \nL 156.26353 213.653598 \nL 156.872461 213.659237 \nL 160.0186 213.897829 \nL 160.120089 213.745625 \nL 160.221577 213.903665 \nL 160.323065 213.6902 \nL 160.424554 213.926316 \nL 160.526042 213.694984 \nL 160.62753 213.897515 \nL 160.830507 213.720912 \nL 160.931996 213.801091 \nL 161.033484 213.626053 \nL 161.134972 213.806126 \nL 161.337949 213.820832 \nL 161.439437 213.945246 \nL 161.540926 213.847389 \nL 161.642414 213.957757 \nL 161.845391 213.883312 \nL 162.048368 213.915966 \nL 162.251344 213.992676 \nL 162.55581 213.880726 \nL 162.860275 213.930219 \nL 163.063251 214.027973 \nL 163.672182 213.773045 \nL 163.875158 213.790901 \nL 164.281112 213.608981 \nL 164.687065 213.862786 \nL 165.194507 213.897797 \nL 165.600461 214.040996 \nL 166.412368 214.077736 \nL 166.818321 214.084545 \nL 167.427252 213.997151 \nL 168.7466 213.979325 \nL 169.558507 214.054841 \nL 170.370414 214.030851 \nL 171.994228 213.963491 \nL 177.170136 213.96494 \nL 177.880554 213.917509 \nL 179.808833 213.979834 \nL 181.331159 213.918203 \nL 182.954973 213.919191 \nL 183.665392 214.029805 \nL 183.969857 213.990922 \nL 184.274322 213.917162 \nL 184.578787 213.857197 \nL 191.682973 213.892159 \nL 191.784462 214.095933 \nL 191.88595 213.90319 \nL 191.987438 214.114522 \nL 192.088927 214.040352 \nL 192.291903 214.074934 \nL 192.393392 214.011507 \nL 192.49488 214.118043 \nL 192.596369 214.04571 \nL 192.697857 214.174148 \nL 192.900834 214.107051 \nL 193.306787 214.094487 \nL 193.408276 214.164222 \nL 193.814229 213.950244 \nL 194.017206 214.0115 \nL 195.03209 213.892343 \nL 195.336555 213.961608 \nL 196.960369 214.063483 \nL 197.366322 214.036841 \nL 198.076741 214.038442 \nL 199.091625 214.071961 \nL 199.903532 214.213472 \nL 201.831811 214.140644 \nL 202.339253 214.124889 \nL 203.557113 214.28438 \nL 203.861578 214.171819 \nL 204.064555 214.175453 \nL 204.470508 214.048324 \nL 204.774974 214.051337 \nL 205.079439 214.119466 \nL 205.586881 214.221213 \nL 206.094322 214.248067 \nL 207.819625 214.271544 \nL 208.530043 214.340891 \nL 208.73302 214.394079 \nL 209.037485 214.36403 \nL 209.646416 214.374667 \nL 210.153857 214.356943 \nL 210.661299 214.361396 \nL 211.676183 214.419107 \nL 268.611161 214.42659 \nL 270.031999 214.445561 \nL 270.945394 214.464607 \nL 271.960278 214.457403 \nL 272.975162 214.470912 \nL 274.700464 214.508253 \nL 277.846604 214.522773 \nL 285.052278 214.478672 \nL 286.067162 214.374139 \nL 287.183534 214.436247 \nL 287.386511 214.422958 \nL 287.589487 214.27597 \nL 287.792464 214.477702 \nL 287.995441 214.321329 \nL 288.198418 214.254978 \nL 288.401394 214.420082 \nL 288.908836 214.402925 \nL 289.111813 214.431922 \nL 289.517767 214.388037 \nL 289.92372 214.458216 \nL 291.446046 214.412293 \nL 292.257953 214.496665 \nL 292.765395 214.48106 \nL 295.810046 214.465637 \nL 296.926418 213.981811 \nL 298.448744 211.994214 \nL 299.057674 211.130546 \nL 299.260651 211.59308 \nL 299.362139 211.648159 \nL 300.174046 213.15974 \nL 300.681488 213.690031 \nL 300.782976 213.655205 \nL 301.493395 214.31924 \nL 301.899348 214.291704 \nL 302.508279 214.181799 \nL 303.523162 214.195459 \nL 303.929116 214.072994 \nL 304.030604 214.0708 \nL 304.233581 213.919696 \nL 304.741023 213.748948 \nL 305.248465 212.659742 \nL 305.958883 212.467711 \nL 306.567814 212.390055 \nL 307.481209 212.261091 \nL 307.582697 212.272392 \nL 307.988651 211.947602 \nL 308.090139 211.856555 \nL 308.191628 211.603146 \nL 308.902046 211.972212 \nL 309.003535 211.962752 \nL 309.308 211.661076 \nL 309.612465 211.62149 \nL 309.815442 211.620513 \nL 310.018418 211.546985 \nL 310.221395 211.491948 \nL 311.236279 211.64639 \nL 311.540744 211.564503 \nL 312.048186 211.157006 \nL 312.860093 211.809691 \nL 313.367535 211.435368 \nL 313.672 211.71437 \nL 313.976465 211.793542 \nL 314.686884 211.489172 \nL 314.991349 211.512242 \nL 315.498791 211.358642 \nL 315.803256 211.587243 \nL 316.209209 211.366327 \nL 316.513674 211.65225 \nL 316.818139 211.838614 \nL 317.224093 211.607366 \nL 317.630046 211.835992 \nL 318.036 211.893089 \nL 318.238977 211.813946 \nL 318.441953 211.964298 \nL 318.64493 212.114151 \nL 318.847907 212.015679 \nL 318.949395 211.903519 \nL 319.152372 211.568513 \nL 319.355349 211.703071 \nL 319.558326 211.816121 \nL 319.761302 211.810137 \nL 320.167256 211.57841 \nL 320.674698 211.584187 \nL 320.979163 211.595598 \nL 321.385116 211.821927 \nL 321.689581 211.606624 \nL 321.892558 211.566167 \nL 322.4 211.815532 \nL 322.805954 211.73029 \nL 323.516372 211.904077 \nL 323.719349 211.97426 \nL 324.125302 212.177082 \nL 324.632744 212.134418 \nL 324.835721 211.92869 \nL 325.444651 211.99304 \nL 325.647628 212.132892 \nL 326.053582 211.938628 \nL 326.662512 212.162403 \nL 327.37293 212.312099 \nL 327.778884 212.267729 \nL 328.184837 212.35333 \nL 328.996744 212.350227 \nL 329.30121 212.489926 \nL 329.808651 212.475015 \nL 330.113117 212.485983 \nL 330.417582 212.530829 \nL 330.823535 212.667084 \nL 331.128 212.635798 \nL 331.533954 212.704649 \nL 332.447349 212.79988 \nL 332.751814 212.807778 \nL 333.056279 212.791403 \nL 333.462233 212.889733 \nL 333.766698 212.878755 \nL 334.071163 212.980865 \nL 334.578605 212.972904 \nL 335.593489 212.989949 \nL 335.796466 212.981419 \nL 336.100931 213.041982 \nL 336.405396 213.085143 \nL 336.709861 213.153517 \nL 337.115814 213.09429 \nL 337.724745 213.195157 \nL 338.63814 213.208863 \nL 338.841117 213.295153 \nL 339.145582 213.242453 \nL 339.450047 213.332972 \nL 340.261954 213.254294 \nL 342.190233 213.51311 \nL 343.103629 213.367974 \nL 343.306605 213.351524 \nL 343.61107 213.452802 \nL 344.017024 213.378705 \nL 344.524466 213.465433 \nL 345.031908 213.448013 \nL 349.395908 213.645857 \nL 350.613768 213.697826 \nL 350.918233 213.70628 \nL 351.222699 213.661918 \nL 351.831629 213.735752 \nL 356.398606 213.826403 \nL 357.210513 213.83505 \nL 357.717955 213.807197 \nL 358.123908 213.86057 \nL 360.458141 213.935741 \nL 360.762606 213.875911 \nL 361.371536 213.832578 \nL 363.502792 213.99217 \nL 364.416187 213.933358 \nL 364.822141 213.981515 \nL 365.329583 213.939493 \nL 366.547443 213.970446 \nL 366.547443 213.970446 \n\" style=\"fill:none;stroke:#008080;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 46.965625 224.64 \nL 46.965625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 381.765625 224.64 \nL 381.765625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 46.965625 224.64 \nL 381.765625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 46.965625 7.2 \nL 381.765625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p86f1e96d78\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "class DINN:\n",
    "    def __init__(self, t, S_data, I_data, tao_data, T_star_data, u_data): #, t, S_data, I_data, tao_data, T_star_data, u\n",
    "        self.t = torch.tensor(t, requires_grad=True).float()\n",
    "        self.S = torch.tensor(S_data)\n",
    "        self.I = torch.tensor(I_data)\n",
    "        self.taos = torch.tensor(tao_data)\n",
    "        self.T_stars = torch.tensor(T_star_data)\n",
    "        self.u = torch.tensor(u_data, requires_grad=True).float()\n",
    "\n",
    "        #self.t = t\n",
    "        #self.S = S_data\n",
    "        #self.I = I_data\n",
    "        #self.taos = tao_data\n",
    "        #self.T_stars = T_star_data\n",
    "        #self.u_data = u_data\n",
    "\n",
    "        #learnable parameters\n",
    "        self.alpha1=torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.alpha2=torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.mu=torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "        self.beta=torch.nn.Parameter(torch.rand(1, requires_grad=True))\n",
    "\n",
    "        #NN\n",
    "        self.nx = self.net_x()\n",
    "        self.params = list(self.nx.parameters())\n",
    "        self.params.extend(list([self.alpha1, self.alpha2, self.mu, self.beta]))\n",
    "        \n",
    "    #nets\n",
    "    class net_x(nn.Module): # input = [t, u]\n",
    "        def __init__(self):\n",
    "            super(DINN.net_x, self).__init__()\n",
    "            self.fc1=nn.Linear(1, 200) #takes t\n",
    "            self.fc2=nn.Linear(200, 200)\n",
    "            self.out=nn.Linear(200, 2) #outputs S, I\n",
    "\n",
    "            self.fc11=nn.Linear(1, 200) #takes u\n",
    "            self.fc22=nn.Linear(200, 200)\n",
    "            self.outout=nn.Linear(200, 2) #outputs tao, T*\n",
    "\n",
    "        def forward(self, t, u):\n",
    "            si=F.relu(self.fc1(t))\n",
    "            si=F.relu(self.fc2(si))\n",
    "            si=self.out(si)\n",
    "\n",
    "            taot=F.relu(self.fc1(u))\n",
    "            taot=F.relu(self.fc2(taot))\n",
    "            taot=self.out(taot)\n",
    "            return si, taot    \n",
    "    \n",
    "    \n",
    "    def net_f(self, t, u):\n",
    "        #t_u = torch.tensor([t.float(),u.float()], requires_grad = True)\n",
    "\n",
    "        S, I, tao, T_star = self.nx(t, u) #input = tensor of size 2  \n",
    "        S_t = grad(S, t, retain_graph=True)[0][0] #derivative of S_pred wrt t              \n",
    "        I_t = grad(I, t, retain_graph=True)[0][0] #derivative of I_pred wrt t\n",
    "\n",
    "        f1 = S_t + self.beta * S * I + u * (t > tao) * self.alpha1\n",
    "        f2 = I_t - self.beta * S * I + self.mu * I + u * (t > tao) * self.alpha2\n",
    "\n",
    "        return f1, f2\n",
    "    \n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        print('\\nstarting training...\\n')\n",
    "        losses = []\n",
    "\n",
    "        learning_rate = 0.001\n",
    "        optimizer = optim.Adam(self.params, lr = learning_rate)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            print('\\nself.alpha1', self.alpha1)            \n",
    "            for time_step in range(len(self.t)):\n",
    "                #optimizer.zero_grad()\n",
    "\n",
    "                t_value, u_value = self.t[time_step], self.u[time_step]\n",
    "                t_value = torch.tensor(t_value)\n",
    "                u_value = torch.tensor(u_value)\n",
    "\n",
    "                f1, f2 = self.net_f(t_value, u_value)\n",
    "                t_u = torch.tensor([t_value.float(), u_value.float()], requires_grad = True)\n",
    "                S_pred, I_pred, tao_pred, T_star_pred = self.nx(t_u)\n",
    "                \n",
    "                loss = (torch.mean(torch.square(self.S[time_step]-S_pred))+torch.mean(torch.square(self.I[time_step]-I_pred)) \\\n",
    "                    +torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)) \\\n",
    "                    +torch.mean(torch.square(self.taos[time_step]-tao_pred)) + torch.mean(torch.square(self.T_stars[time_step]-T_star_pred)))/100 \n",
    "                \n",
    "                loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(self.nx.parameters(), 500) #gradient clipping\n",
    "                #optimizer.step()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            print('loss: ' ,loss)\n",
    "            print('self.alpha1 after the training!!!: ', self.alpha1)\n",
    "            \n",
    "        plt.plot(losses, color = 'teal')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "dinn = DINN(tSI_data[0], tSI_data[1], tSI_data[2], tao_data[1], tao_data[2], tao_data[3]) #t, S_data, I_data, tao_data, T_star_data, u\n",
    "dinn.train(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}